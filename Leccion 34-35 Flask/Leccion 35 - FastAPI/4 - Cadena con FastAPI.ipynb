{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a324c608",
   "metadata": {},
   "source": [
    "# 4 - Cadena con FastAPI\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/fastapi.png\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93750b0",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Cadena-de-LangChain\" data-toc-modified-id=\"1---Cadena-de-LangChain-1\">1 - Cadena de LangChain</a></span></li><li><span><a href=\"#2---Cadena-en-una-función\" data-toc-modified-id=\"2---Cadena-en-una-función-2\">2 - Cadena en una función</a></span></li><li><span><a href=\"#3---Código-de-FastAPI\" data-toc-modified-id=\"3---Código-de-FastAPI-3\">3 - Código de FastAPI</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a212be4",
   "metadata": {},
   "source": [
    "## 1 - Cadena de LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907e998",
   "metadata": {},
   "source": [
    "Vamos a usar una cadena de Langchain sencilla para usarla dentro de una API de FastAPI. Vamos a ver otra vez la cadena paso a paso y crear una función con ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9d90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero cargamos la API KEY de OpenAI\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c746765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos el prompt\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    ('system', '''Eres un historiador muy erudito que ofrece respuestas precisas y \n",
    "                  elocuentes a preguntas históricas y que responde en castellano.'''),\n",
    "    \n",
    "    ('human', '{pregunta}')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5652629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el modelo llm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "modelo = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca37ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser de salida, transforma la salida a string\n",
    "\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14e7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la cadena con lcel\n",
    "\n",
    "cadena = prompt | modelo | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8792ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada a la cadena\n",
    "\n",
    "pregunta = '¿Cuales son las 7 maravillas del mundo?'\n",
    "\n",
    "respuesta = cadena.invoke({'pregunta': pregunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0fa1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Las Siete Maravillas del Mundo Antiguo son:\\n\\n1. La Gran Pirámide de Guiza en Egipto.\\n2. Los Jardines Colgantes de Babilonia en Irak.\\n3. La Estatua de Zeus en Olimpia, Grecia.\\n4. El Templo de Artemisa en Éfeso, Turquía.\\n5. El Mausoleo de Halicarnaso en Turquía.\\n6. El Coloso de Rodas en la isla de Rodas, Grecia.\\n7. El Faro de Alejandría en Egipto.\\n\\nEs importante mencionar que estas maravillas fueron seleccionadas en la antigüedad y que actualmente solo queda en pie la Gran Pirámide de Guiza.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97bce6e",
   "metadata": {},
   "source": [
    "## 2 - Cadena en una función"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e83c1d",
   "metadata": {},
   "source": [
    "Ahora que hemos visto esa cadena sencilla paso a paso, vamos a crear una función que reciba la pregunta del usuario y devuelva la respuesta del modelo. Esta función la usaremos después en una aplicación de FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "def cadena_llm(pregunta:str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función es para el uso de un LLM con una cadena sencilla:\n",
    "    \n",
    "    Params:\n",
    "    pregunta: str, inoput del usuario\n",
    "    \n",
    "    Return:\n",
    "    string, devuelve la pregunta del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    global OPENAI_API_KEY\n",
    "    \n",
    "    # preparamos el prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "        ('system', '''Eres un historiador muy erudito que ofrece respuestas precisas y \n",
    "                      elocuentes a preguntas históricas y que responde en castellano.'''),\n",
    "\n",
    "        ('human', '{pregunta}')\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    # iniciamos el modelo llm\n",
    "    modelo = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    # parser de salida, transforma la salida a string\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # creamos la cadena con lcel\n",
    "    cadena = prompt | modelo | parser\n",
    "    \n",
    "    # llamada a la cadena\n",
    "    respuesta = cadena.invoke({'pregunta': pregunta})\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fbb9ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Las Siete Maravillas del Mundo Antiguo son:\\n\\n1. La Gran Pirámide de Guiza en Egipto.\\n2. Los Jardines Colgantes de Babilonia en Irak.\\n3. La Estatua de Zeus en Olimpia, Grecia.\\n4. El Templo de Artemisa en Éfeso, Turquía.\\n5. El Mausoleo de Halicarnaso en Turquía.\\n6. El Coloso de Rodas en la isla de Rodas, Grecia.\\n7. El Faro de Alejandría en Egipto.\\n\\nEs importante mencionar que estas maravillas fueron seleccionadas en la antigüedad y que actualmente solo queda en pie la Gran Pirámide de Guiza.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadena_llm(pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f73b3",
   "metadata": {},
   "source": [
    "## 3 - Código de FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0efac",
   "metadata": {},
   "source": [
    "Ahora vamos a usar esta función para usarla a través de la API. En cuanto la aplicación realice la petición POST y reciba el mensaje del usuario, dicho mensaje se usará para crear la respuesta del modelo. El objetivo es crear un chat que podamos usar desde nuestra API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be1f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from fastapi import FastAPI, Query\n",
    "\n",
    "\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# iniciamos la aplicacion\n",
    "app = FastAPI()\n",
    "\n",
    "# almacén temporal de mensajes, en memoria\n",
    "mensajes = []\n",
    "\n",
    "\n",
    "def cadena_llm(pregunta:str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función es para el uso de un LLM con una cadena sencilla:\n",
    "    \n",
    "    Params:\n",
    "    pregunta: str, inoput del usuario\n",
    "    \n",
    "    Return:\n",
    "    string, devuelve la pregunta del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    global OPENAI_API_KEY\n",
    "    \n",
    "    # preparamos el prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "        ('system', '''Eres un historiador muy erudito que ofrece respuestas precisas y \n",
    "                      elocuentes a preguntas históricas y que responde en castellano.'''),\n",
    "\n",
    "        ('human', '{pregunta}')\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    # iniciamos el modelo llm\n",
    "    modelo = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    # parser de salida, transforma la salida a string\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # creamos la cadena con lcel\n",
    "    cadena = prompt | modelo | parser\n",
    "    \n",
    "    # llamada a la cadena\n",
    "    respuesta = cadena.invoke({'pregunta': pregunta})\n",
    "    \n",
    "    return respuesta\n",
    "\n",
    "\n",
    "\n",
    "# endpoint para obtener todos los mensajes (GET)\n",
    "@app.get('/mensajes')\n",
    "def historial():\n",
    "    \n",
    "    # devuelve el json de mensajes\n",
    "    return mensajes\n",
    "\n",
    "\n",
    "\n",
    "# endpoint para el chat\n",
    "@app.get('/chat')\n",
    "def chat(query: str = Query(...)):\n",
    "    \n",
    "    global cadena_llm\n",
    "    \n",
    "\n",
    "    # respuesta del LLM\n",
    "    respuesta = cadena_llm(query)\n",
    "\n",
    "    # se añade a la lista\n",
    "    mensajes.append({'nombre': 'Tu', 'mensaje': query})\n",
    "    mensajes.append({'nombre': 'Bot', 'mensaje': respuesta})\n",
    "    \n",
    "    return {'nombre': 'Bot', 'mensaje': respuesta}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7024e529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [13739]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50996 - \"GET /chat?query=hola HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:50997 - \"GET /mensajes HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [13739]\n"
     ]
    }
   ],
   "source": [
    "# ejecutar la app\n",
    "\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dfb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
