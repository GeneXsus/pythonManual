{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b50a887",
   "metadata": {},
   "source": [
    "# 5 - Cadena con Flask\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/flask_api.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a583ef8",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Cadena-de-LangChain\" data-toc-modified-id=\"1---Cadena-de-LangChain-1\">1 - Cadena de LangChain</a></span></li><li><span><a href=\"#2---Cadena-en-una-función\" data-toc-modified-id=\"2---Cadena-en-una-función-2\">2 - Cadena en una función</a></span></li><li><span><a href=\"#3---Código-de-Flask\" data-toc-modified-id=\"3---Código-de-Flask-3\">3 - Código de Flask</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a177d",
   "metadata": {},
   "source": [
    "## 1 - Cadena de LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ae376",
   "metadata": {},
   "source": [
    "Vamos a usar una cadena de Langchain sencilla para usarla dentro del ejemplo anterior. Usaremos una cadena que ya hemos visto anteriormente para usarla dentro la aplicación de Flask. Vamos a ver otra vez esa cadena paso a paso y crear una función con ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero cargamos la API KEY de OpenAI\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f87822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos el prompt\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    ('system', '''Eres un historiador muy erudito que ofrece respuestas precisas y \n",
    "                  elocuentes a preguntas históricas y que responde en castellano.'''),\n",
    "    \n",
    "    ('human', '{pregunta}')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el modelo llm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "modelo = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf941ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser de salida, transforma la salida a string\n",
    "\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01357c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la cadena con lcel\n",
    "\n",
    "cadena = prompt | modelo | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada a la cadena\n",
    "\n",
    "pregunta = '¿Cuales son las 7 maravillas del mundo?'\n",
    "\n",
    "respuesta = cadena.invoke({'pregunta': pregunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583073c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dc6e9",
   "metadata": {},
   "source": [
    "## 2 - Cadena en una función"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446e5e0",
   "metadata": {},
   "source": [
    "Ahora que hemos visto esa cadena sencilla paso a paso, vamos a crear una función que reciba la pregunta del usuario y devuelva la respuesta del modelo. Esta función la usaremos después en la aplicación de Flask que creamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4463206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "def cadena_llm(pregunta:str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función es para el uso de un LLM con una cadena sencilla:\n",
    "    \n",
    "    Params:\n",
    "    pregunta: str, inoput del usuario\n",
    "    \n",
    "    Return:\n",
    "    string, devuelve la pregunta del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    global OPENAI_API_KEY\n",
    "    \n",
    "    # preparamos el prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "        ('system', '''Eres un historiador muy erudito que ofrece respuestas precisas y \n",
    "                      elocuentes a preguntas históricas y que responde en castellano.'''),\n",
    "\n",
    "        ('human', '{pregunta}')\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    # iniciamos el modelo llm\n",
    "    modelo = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    # parser de salida, transforma la salida a string\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # creamos la cadena con lcel\n",
    "    cadena = prompt | modelo | parser\n",
    "    \n",
    "    # llamada a la cadena\n",
    "    respuesta = cadena.invoke({'pregunta': pregunta})\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695833c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadena_llm(pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d002aa1",
   "metadata": {},
   "source": [
    "## 3 - Código de Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e9797",
   "metadata": {},
   "source": [
    "Ahora vamos a usar esta función para la página de chat que creamos anteriormente. En cuanto la aplicación realice la petición POST y reciba el mensaje del usuario, dicho mensaje se usará para crear la respuesta del modelo y tanto mensaje como respuesta se devolverán al HTML. El objetivo es crear un chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edd32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerias\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# iniciamos la aplicacion\n",
    "app = Flask(__name__)\n",
    "\n",
    "# almacén temporal de mensajes, en memoria\n",
    "mensajes = []\n",
    "\n",
    "\n",
    "# funcion del modelo\n",
    "def cadena_llm(pregunta:str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función es para el uso de un LLM con una cadena sencilla:\n",
    "    \n",
    "    Params:\n",
    "    pregunta: str, inoput del usuario\n",
    "    \n",
    "    Return:\n",
    "    string, devuelve la pregunta del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    global OPENAI_API_KEY\n",
    "    \n",
    "    # preparamos el prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "        ('system', '''Eres un historiador muy erudito que ofrece respuestas precisas y \n",
    "                      elocuentes a preguntas históricas y que responde en castellano.'''),\n",
    "\n",
    "        ('human', '{pregunta}')\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    # iniciamos el modelo llm\n",
    "    modelo = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    # parser de salida, transforma la salida a string\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # creamos la cadena con lcel\n",
    "    cadena = prompt | modelo | parser\n",
    "    \n",
    "    # llamada a la cadena\n",
    "    respuesta = cadena.invoke({'pregunta': pregunta})\n",
    "    \n",
    "    return respuesta\n",
    "\n",
    "\n",
    "\n",
    "# ruta principal que muestra el chat\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def chat():\n",
    "    \n",
    "    global cadena_llm\n",
    "    \n",
    "    # si el metodo es POST..\n",
    "    if request.method == 'POST':\n",
    "        \n",
    "        # se obtiene el mensaje\n",
    "        mensaje = request.form.get('mensaje')\n",
    "        \n",
    "        # respuesta del LLM\n",
    "        respuesta = cadena_llm(mensaje)\n",
    "        \n",
    "        # se añade a la lista\n",
    "        mensajes.append({'nombre': 'Tu', 'mensaje': mensaje})\n",
    "        mensajes.append({'nombre': 'Bot', 'mensaje': respuesta})\n",
    "        \n",
    "        # redirige para evitar reenviar el formulario\n",
    "        return redirect(url_for('chat'))  \n",
    "    \n",
    "    # renderiza el html\n",
    "    return render_template('chat.html', mensajes=mensajes)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b21af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
