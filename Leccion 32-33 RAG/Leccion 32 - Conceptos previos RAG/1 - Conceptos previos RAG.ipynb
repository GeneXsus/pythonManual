{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e063c10",
   "metadata": {},
   "source": [
    "# 1 - Conceptos previos RAG\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag_2.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8dec7",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---¿Qué-es-un-LLM?\" data-toc-modified-id=\"1---¿Qué-es-un-LLM?-1\">1 - ¿Qué es un LLM?</a></span></li><li><span><a href=\"#2---¿Qué-es-un-RAG?\" data-toc-modified-id=\"2---¿Qué-es-un-RAG?-2\">2 - ¿Qué es un RAG?</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaf4fa",
   "metadata": {},
   "source": [
    "## 1 - ¿Qué es un LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1476e",
   "metadata": {},
   "source": [
    "Un LLM, o \"Large Language Model\" (Gran Modelo de Lenguaje), es un tipo de modelo de inteligencia artificial diseñado para entender, generar y trabajar con lenguaje humano a gran escala. Estos modelos se entrenan utilizando grandes cantidades de datos de texto para aprender patrones, estructuras del lenguaje y relaciones contextuales.\n",
    "\n",
    "Características principales de los LLM:\n",
    "\n",
    "1. Capacidad de Generación de Texto: Los LLMs son capaces de generar texto coherente y contextualmente relevante que puede imitar el estilo y la estructura del lenguaje humano. Esto los hace útiles para tareas como la escritura automática de artículos, generación de respuestas en chatbots, etc...\n",
    "\n",
    "2. Comprensión del Contexto: Gracias a su entrenamiento con grandes cantidades de texto, los LLMs tienen una notable capacidad para entender el contexto de las consultas que reciben, lo que les permite ofrecer respuestas más precisas y relevantes.\n",
    "\n",
    "3. Aplicaciones Multilingües: Algunos LLMs están entrenados en múltiples idiomas, lo que les permite operar en diferentes lenguajes y realizar tareas como traducción automática o asistencia multilingüe.\n",
    "\n",
    "4. Aprendizaje Continuo: Aunque los LLMs se entrenan en un conjunto estático de datos, algunos modelos están diseñados para continuar aprendiendo a partir de nuevas interacciones, lo que mejora su rendimiento y adaptabilidad con el tiempo.\n",
    "\n",
    "5. Interpretación de Sentimiento y Semántica: Pueden analizar y entender sentimientos, opiniones y matices semánticos en el texto, lo que es crucial para aplicaciones como análisis de sentimiento, soporte al cliente y monitorización de redes sociales.\n",
    "\n",
    "Ejemplos de LLM:\n",
    "\n",
    "+ GPT (Generative Pre-trained Transformer) de OpenAI: Es uno de los modelos de lenguaje más conocidos y avanzados, usado ampliamente en aplicaciones comerciales y de investigación por su capacidad para generar texto altamente contextual y creativo.\n",
    "\n",
    "+ BERT (Bidirectional Encoder Representations from Transformers) de Google: Se utiliza principalmente para mejorar la comprensión del lenguaje en buscadores y para mejorar la precisión de las respuestas en aplicaciones de inteligencia artificial.\n",
    "\n",
    "\n",
    "Cuando se habla de los parámetros de un LLM nos estamos refiriendo a los valores internos que el modelo utiliza para realizar predicciones y generar texto basado en el lenguaje humano. Estos parámetros son esenciales para que el modelo funcione correctamente y son ajustados durante el proceso de entrenamiento. Los parámetros son en su mayoría pesos sinápticos. Estos pesos determinan la fuerza de la conexión entre las neuronas en diferentes capas del modelo. Durante el entrenamiento, estos pesos se ajustan para minimizar el error en la predicción del modelo. Junto con los pesos, los biases (sesgos) son otro tipo de parámetros que se añaden a las sumas ponderadas en las neuronas para ayudar al modelo a ajustarse mejor a los datos, son el equivalente a la ordenada en el origen. Los biases permiten que el modelo opere eficazmente cuando todas las entradas son cero.\n",
    "\n",
    "Los LLMs son conocidos por tener un número extremadamente alto de parámetros. Por ejemplo, GPT-3 de OpenAI tiene 175 mil millones de parámetros, lo que lo hace uno de los modelos de lenguaje más grandes y poderosos disponibles. Estos parámetros permiten al modelo capturar una gran cantidad de matices lingüísticos y contextuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57620a",
   "metadata": {},
   "source": [
    "## 2 - ¿Qué es un RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfc872",
   "metadata": {},
   "source": [
    "La generación mejorada por recuperación (Retrieval Augmented Generation - RAG) es el proceso de optimización de un modelo lingüístico de gran tamaño (Large Language Model - LLM), de modo que conozca datos proporcionados por el usuario, que no existan en los datos de entrenamiento del modelo, antes de generar una respuesta. \n",
    "\n",
    "Los LLM se entrenan con grandes volúmenes de datos y usan miles de millones de parámetros para generar resultados originales en tareas como responder preguntas, traducir idiomas y completar frases. Un RAG extiende las ya poderosas capacidades de los LLM a dominios específicos o a la base de conocimientos interna de una organización, todo ello sin la necesidad de volver a entrenar el modelo. Se trata de un método rentable para mejorar los resultados de los LLM de modo que sigan siendo relevantes, precisos y útiles en diversos contextos.\n",
    "\n",
    "Un RAG aporta varios beneficios directos en el desarrollo de una herramienta de inteligencia artificial:\n",
    "\n",
    "+ Rentabilidad de la implementación. El desarrollo de IAs comienza normalmente con un modelo básico. Los modelos fundacionales (Foundational Models - FM) son LLMs accesibles por API entrenados en un amplio espectro de datos generalizados y sin etiquetar. Los costos computacionales y financieros de volver a entrenar a los FM para obtener información específica de la organización o del dominio son muy elevados. Un RAG es un enfoque más rentable para introducir nuevos datos en el LLM.\n",
    "\n",
    "\n",
    "+ Información actualizada. Incluso si el LLM está entrenado con los datos adecuados para las necesidades de la compañía, es complicado mantener la relevancia del modelo. Un RAG permite a los desarrolladores proporcionar las últimas investigaciones, estadísticas o noticias a los modelos generativos. Se puede usar un RAG para conectar el LLM directamente a las redes sociales en vivo, sitios de noticias u otras fuentes de información que se actualizan con frecuencia. De esta manera, un LLM puede proporcionar la información más reciente.\n",
    "\n",
    "\n",
    "+ Confianza. Al darle al LLM datos propios, se conoce perfectamente la fuente de datos además de evitar la alucinación del LLM.\n",
    "\n",
    "\n",
    "+ Mayor control. El RAG permite a los desarrolladores de inteligencia artificial cambiar las fuentes de información para adaptarse a los requisitos cambiantes o a los usos múltiples de la compañía. Además pueden restringir la recuperación de información confidencial a diferentes niveles de autorización y garantizar que el LLM genere las respuestas adecuadas.\n",
    "\n",
    "\n",
    "El esquema básico de un RAG es como sigue:\n",
    "\n",
    "<br>\n",
    "\n",
    "![rag](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Los documentos de la compañía se pasan por un modelo de incrustación (Embedding Model) para ser guardados en forma de vectores en una base de datos.\n",
    "\n",
    "2. La consulta realizada por el usuario pasa por el mismo modelo para convertir dicho texto en vectores.\n",
    "\n",
    "3. Se buscan en la base de datos los vectores más parecidos a la consulta y se extraen los vectores más relevantes.\n",
    "\n",
    "4. Los documentos más relevantes extraídos y la consulta se introducen en el LLM para generar la respuesta más adecuada.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
