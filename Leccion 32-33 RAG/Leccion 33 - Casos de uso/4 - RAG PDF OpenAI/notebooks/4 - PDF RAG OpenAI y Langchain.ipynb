{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88192245",
   "metadata": {},
   "source": [
    "# 4 - PDF RAG OpenAI y Langchain\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag_2.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce8e06",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---OpenAI-API-KEY\" data-toc-modified-id=\"1---OpenAI-API-KEY-1\">1 - OpenAI API KEY</a></span></li><li><span><a href=\"#2---Probando-GPT4-desde-LangChain\" data-toc-modified-id=\"2---Probando-GPT4-desde-LangChain-2\">2 - Probando GPT4 desde LangChain</a></span></li><li><span><a href=\"#3---Cargando-archivo-PDF\" data-toc-modified-id=\"3---Cargando-archivo-PDF-3\">3 - Cargando archivo PDF</a></span></li><li><span><a href=\"#4---Chunks\" data-toc-modified-id=\"4---Chunks-4\">4 - Chunks</a></span></li><li><span><a href=\"#5---Modelo-de-Embedding\" data-toc-modified-id=\"5---Modelo-de-Embedding-5\">5 - Modelo de Embedding</a></span></li><li><span><a href=\"#6---Guardado-en-ChromaDB\" data-toc-modified-id=\"6---Guardado-en-ChromaDB-6\">6 - Guardado en ChromaDB</a></span></li><li><span><a href=\"#7---Carga-desde-Chroma\" data-toc-modified-id=\"7---Carga-desde-Chroma-7\">7 - Carga desde Chroma</a></span></li><li><span><a href=\"#8---Prompt-template\" data-toc-modified-id=\"8---Prompt-template-8\">8 - Prompt template</a></span></li><li><span><a href=\"#9---Cadena\" data-toc-modified-id=\"9---Cadena-9\">9 - Cadena</a></span></li><li><span><a href=\"#11---Más-preguntas\" data-toc-modified-id=\"11---Más-preguntas-10\">11 - Más preguntas</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ff9d9",
   "metadata": {},
   "source": [
    "## 1 - OpenAI API KEY\n",
    "\n",
    "Para llevar a cabo este proyecto, necesitaremos una API KEY de OpenAI para utilizar el modelo GPT-4 Turbo. Esta API KEY se puede obtener en https://platform.openai.com/api-keys. Solo se muestra una vez, por lo que debe guardarse en el momento en que se obtiene. Por supuesto, necesitaremos crear una cuenta para obtenerla.\n",
    "\n",
    "Guardamos la API KEY en un archivo .env para cargarla con la biblioteca dotenv y usarla como una variable de entorno. Este archivo se agrega a .gitignore para asegurar que no pueda verse si subimos el código a GitHub, por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la API KEY\n",
    "\n",
    "import os                           # libreria del sistema operativo\n",
    "from dotenv import load_dotenv      # carga variables de entorno \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4f97a",
   "metadata": {},
   "source": [
    "## 2 - Probando GPT4 desde LangChain\n",
    "\n",
    "Vamos a probar la conexión de LangChain con el modelo GPT-4. Simplemente preguntaremos quién es el CEO de Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba14692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   \n",
    "\n",
    "modelo = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model='gpt-4-turbo')\n",
    "\n",
    "respuesta = modelo.invoke(\"Who is Apple's CEO?\")\n",
    "\n",
    "respuesta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cbcc1",
   "metadata": {},
   "source": [
    "## 3 - Cargando archivo PDF \n",
    "\n",
    "Ahora, cargamos el archivo PDF [Formulario 10-K de Apple de 2023](https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf) previamente descargado. Un 10-K es un informe completo que una empresa que cotiza en bolsa presenta anualmente sobre su desempeño financiero y que es requerido por la Comisión de Bolsa y Valores de EE. UU. (SEC). Este informe contiene mucho más detalle que el informe anual de una empresa, que se envía a sus accionistas antes de la reunión anual para elegir a los directores de la empresa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdf0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ef5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads PDF file page by page\n",
    "\n",
    "loader = PyPDFDirectoryLoader('../pdfs/')\n",
    "\n",
    "paginas = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4aa84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera pagina\n",
    "\n",
    "paginas[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4eba2",
   "metadata": {},
   "source": [
    "## 4 - Chunks\n",
    "\n",
    "El `PyPDFDirectoryLoader` utiliza una instancia de TextSplitter, específicamente el `RecursiveCharacterTextSplitter` por defecto, para manejar la división de documentos. Este enfoque ayuda a descomponer archivos PDF grandes o colecciones de archivos en fragmentos manejables para su posterior procesamiento. El cargador garantiza que cada fragmento sea manejable y conserve la metadatos necesarias, como números de página, que son importantes para hacer referencias y mantener la integridad de los documentos fuente durante el procesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51680d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fc61c",
   "metadata": {},
   "source": [
    "## 5 - Modelo de Embedding\n",
    "\n",
    "Los embeddings transforman los datos, especialmente los datos textuales, en un formato, generalmente un vector de números, que los algoritmos de aprendizaje automático pueden procesar de manera efectiva. Estos embeddings capturan las relaciones contextuales y los significados semánticos de palabras, frases o documentos, lo que permite diversas aplicaciones en IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "vectorizador = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad9d02",
   "metadata": {},
   "source": [
    "## 6 - Guardado en ChromaDB\n",
    "\n",
    "Chroma DB es una base de datos de vectores de código abierto diseñada para almacenar y recuperar embeddings de vectores de manera eficiente. Es especialmente útil para mejorar los LLMs al proporcionar contexto relevante a las consultas de los usuarios. Chroma DB permite almacenar embeddings junto con metadatos, que luego pueden ser utilizados por LLMs o para motores de búsqueda semántica sobre datos textuales.\n",
    "\n",
    "Ahora, almacenamos los fragmentos en la base de datos de vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_db = Chroma.from_documents(chunks, vectorizador, persist_directory='../chroma_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a40500",
   "metadata": {},
   "source": [
    "## 7 - Carga desde Chroma \n",
    "\n",
    "Una vez que los datos están guardados, podemos realizar una búsqueda de los documentos más relevantes según nuestra consulta. Podemos buscar directamente mediante búsqueda por similitud, basada en la similitud de coseno, o podemos instanciar un objeto de recuperación para usar más adelante.\n",
    "\n",
    "Por defecto, el tipo de búsqueda realizada por el recuperador, `search_type`, es por similitud y devuelve los resultados más relevantes según esa similitud. También podemos utilizar la similitud con un umbral para recuperar documentos que superen un cierto nivel de similitud con nuestra consulta. El recuperador también cuenta con un algoritmo llamado MMR (relevancia marginal máxima). El algoritmo de relevancia marginal máxima selecciona documentos en función de una combinación de cuáles son los más similares a las consultas, optimizando también para la diversidad. Lo hace encontrando ejemplos con embeddings que tienen la mayor similitud de coseno con las entradas y luego los agrega de manera iterativa, aplicando una penalización para aquellos que estén demasiado cerca de los ejemplos ya seleccionados.\n",
    "\n",
    "Utilizaremos el algoritmo MMR para devolver 15 documentos. El parámetro `lambda_mult` se refiere a la diversidad de los resultados devueltos por MMR, siendo 1 la mínima diversidad y 0 la máxima. El valor predeterminado es 0.5. Pediremos un poco más de diversidad en su respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = 'What can you tell me about foreign exchange contracts?'\n",
    "\n",
    "chroma_db = Chroma(persist_directory='../chroma_db', embedding_function=vectorizador)\n",
    "\n",
    "docs = chroma_db.similarity_search(consulta, k=10)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b84b10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17383efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recuperador = chroma_db.as_retriever(search_type='mmr', search_kwargs={'k': 15, 'lambda_mult': 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recuperador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d45176",
   "metadata": {},
   "source": [
    "## 8 - Prompt template\n",
    "\n",
    "Los prompt templates son recetas predefinidas para generar instrucciones para modelos de lenguaje.\n",
    "\n",
    "Un template puede incluir instrucciones, contexto y preguntas específicas adecuadas para una tarea determinada. LangChain proporciona herramientas para crear y trabajar con templates de instrucciones y también busca crear templates independientes del modelo, para facilitar la reutilización de templates existentes en diferentes modelos de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dbb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            '''\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7cfac",
   "metadata": {},
   "source": [
    "## 9 - Cadena\n",
    "\n",
    "Una \"cadena\" se refiere a una secuencia de componentes o pasos que están vinculados entre sí para realizar una tarea específica o un conjunto de tareas relacionadas con las operaciones de IA o LLMs. LangChain es una biblioteca diseñada para facilitar la creación y despliegue de aplicaciones de lenguaje, encadenando diferentes componentes, como modelos, bases de datos y lógica personalizada. Cada componente en la cadena maneja una parte específica de la tarea, y la salida de un componente sirve como entrada para el siguiente, creando un flujo de trabajo continuo que aprovecha tanto metodologías de IA como de software tradicional. Una cadena actúa efectivamente como una tubería, donde los datos fluyen a través de cada componente en la cadena, siendo transformados, mejorados o utilizados en cada paso.\n",
    "\n",
    "En LangChain, el `StrOutputParser` analiza la salida del modelo directamente en un formato de cadena. Utilizaremos este parser al crear la secuencia en LangChain; será un enlace adicional en la cadena, permitiéndonos obtener directamente la respuesta del LLM en formato de cadena.\n",
    "\n",
    "`RunnablePassthrough` permite que las entradas pasen sin cambios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadena = {'context': recuperador, 'question': RunnablePassthrough()} | prompt | modelo | parser\n",
    "\n",
    "\n",
    "respuesta = cadena.invoke(consulta)\n",
    "\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b8e89",
   "metadata": {},
   "source": [
    "## 11 - Más preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = 'What can you tell me about foreign exchange contracts?'\n",
    "\n",
    "cadena.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33238e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = 'What are the main products?'\n",
    "\n",
    "cadena.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = 'What can you tell me about legal proceedings?'\n",
    "\n",
    "cadena.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff9478",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = '''\n",
    "        How the company’s business and reputation \n",
    "        are impacted by information technology system failures and network disruptions?\n",
    "        '''\n",
    "\n",
    "\n",
    "cadena.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68db213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "86px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
