{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35836f0a",
   "metadata": {},
   "source": [
    "# 1 - YouTube RAG con HuggingFace y LangChain\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag_1.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04373a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Definiciones-y-dependencias.\" data-toc-modified-id=\"1.-Definiciones-y-dependencias.-0\">1. Definiciones y dependencias.</a></span></li><li><span><a href=\"#2.-Importamos-la-API-KEY-de-HuggingFace.\" data-toc-modified-id=\"2.-Importamos-la-API-KEY-de-HuggingFace.-1\">2. Importamos la API KEY de HuggingFace.</a></span></li><li><span><a href=\"#3.-Llamada-a-HuggingFace-desde-LangChain\" data-toc-modified-id=\"3.-Llamada-a-HuggingFace-desde-LangChain-2\">3. Llamada a HuggingFace desde LangChain</a></span><ul class=\"toc-item\"><li><span><a href=\"#Output-Parser\" data-toc-modified-id=\"Output-Parser-2.1\"><a href=\"https://python.langchain.com/docs/modules/model_io/output_parsers/\" rel=\"nofollow\" target=\"_blank\">Output Parser</a></a></span></li></ul></li><li><span><a href=\"#4.-Creación-de-la-cadena\" data-toc-modified-id=\"4.-Creación-de-la-cadena-3\">4. Creación de la cadena</a></span><ul class=\"toc-item\"><li><span><a href=\"#LangChain-Expression-Language-(LCEL)\" data-toc-modified-id=\"LangChain-Expression-Language-(LCEL)-3.1\"><a href=\"https://python.langchain.com/docs/expression_language/\" rel=\"nofollow\" target=\"_blank\">LangChain Expression Language (LCEL)</a></a></span></li></ul></li><li><span><a href=\"#5.-Plantillas-de-instrucciones-(Prompt-Templates)\" data-toc-modified-id=\"5.-Plantillas-de-instrucciones-(Prompt-Templates)-4\">5. Plantillas de instrucciones (<a href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start/\" rel=\"nofollow\" target=\"_blank\">Prompt Templates</a>)</a></span></li><li><span><a href=\"#6.-Combinando-cadenas\" data-toc-modified-id=\"6.-Combinando-cadenas-5\">6. Combinando cadenas</a></span></li><li><span><a href=\"#7.-Extracción-de-texto-desde-YouTube-con-Whisper\" data-toc-modified-id=\"7.-Extracción-de-texto-desde-YouTube-con-Whisper-6\">7. Extracción de texto desde YouTube con Whisper</a></span></li><li><span><a href=\"#8.-Usando-la-transcripción-como-contexto\" data-toc-modified-id=\"8.-Usando-la-transcripción-como-contexto-7\">8. Usando la transcripción como contexto</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividiendo-en-segmentos-el-texto-completo\" data-toc-modified-id=\"Dividiendo-en-segmentos-el-texto-completo-7.1\">Dividiendo en segmentos el texto completo</a></span></li><li><span><a href=\"#Encontrando-los-trozos-relevantes\" data-toc-modified-id=\"Encontrando-los-trozos-relevantes-7.2\">Encontrando los trozos relevantes</a></span></li></ul></li><li><span><a href=\"#9.-Bases-de-datos-vectoriales\" data-toc-modified-id=\"9.-Bases-de-datos-vectoriales-8\">9. Bases de datos vectoriales</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conectando-la-base-de-datos-a-la-cadena\" data-toc-modified-id=\"Conectando-la-base-de-datos-a-la-cadena-8.1\">Conectando la base de datos a la cadena</a></span></li><li><span><a href=\"#Chroma\" data-toc-modified-id=\"Chroma-8.2\">Chroma</a></span></li></ul></li><li><span><a href=\"#10.-Resumen-paso-a-paso\" data-toc-modified-id=\"10.-Resumen-paso-a-paso-9\">10. Resumen paso a paso</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abbaf5",
   "metadata": {},
   "source": [
    "## 1. Definiciones y dependencias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8a9c2",
   "metadata": {},
   "source": [
    "**Dependencias necesarias para este workshop:**\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "pip install \"langchain[docarray]\"\n",
    "pip install langchain-chroma\n",
    "pip install docarray\n",
    "pip install pydantic==1.10.9\n",
    "pip install openai-whisper\n",
    "pip install pytube\n",
    "pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a644ffb",
   "metadata": {},
   "source": [
    "El propósito de este workshop es crear un RAG (Retrieval Augmented Generation) con [LangChain](https://www.langchain.com/) y un modelo de código abierto desde [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217b916",
   "metadata": {},
   "source": [
    "**¿Qué es un LLM?**\n",
    "\n",
    "\n",
    "Un LLM, o \"Large Language Model\" (Gran Modelo de Lenguaje), es un tipo de modelo de inteligencia artificial diseñado para entender, generar y trabajar con lenguaje humano a gran escala. Estos modelos se entrenan utilizando grandes cantidades de datos de texto para aprender patrones, estructuras del lenguaje y relaciones contextuales.\n",
    "\n",
    "Características principales de los LLM:\n",
    "\n",
    "1. Capacidad de Generación de Texto: Los LLMs son capaces de generar texto coherente y contextualmente relevante que puede imitar el estilo y la estructura del lenguaje humano. Esto los hace útiles para tareas como la escritura automática de artículos, generación de respuestas en chatbots, etc...\n",
    "\n",
    "2. Comprensión del Contexto: Gracias a su entrenamiento con grandes cantidades de texto, los LLMs tienen una notable capacidad para entender el contexto de las consultas que reciben, lo que les permite ofrecer respuestas más precisas y relevantes.\n",
    "\n",
    "3. Aplicaciones Multilingües: Algunos LLMs están entrenados en múltiples idiomas, lo que les permite operar en diferentes lenguajes y realizar tareas como traducción automática o asistencia multilingüe.\n",
    "\n",
    "4. Aprendizaje Continuo: Aunque los LLMs se entrenan en un conjunto estático de datos, algunos modelos están diseñados para continuar aprendiendo a partir de nuevas interacciones, lo que mejora su rendimiento y adaptabilidad con el tiempo.\n",
    "\n",
    "5. Interpretación de Sentimiento y Semántica: Pueden analizar y entender sentimientos, opiniones y matices semánticos en el texto, lo que es crucial para aplicaciones como análisis de sentimiento, soporte al cliente y monitorización de redes sociales.\n",
    "\n",
    "Ejemplos de LLM:\n",
    "\n",
    "+ GPT (Generative Pre-trained Transformer) de OpenAI: Es uno de los modelos de lenguaje más conocidos y avanzados, usado ampliamente en aplicaciones comerciales y de investigación por su capacidad para generar texto altamente contextual y creativo.\n",
    "\n",
    "+ BERT (Bidirectional Encoder Representations from Transformers) de Google: Se utiliza principalmente para mejorar la comprensión del lenguaje en buscadores y para mejorar la precisión de las respuestas en aplicaciones de inteligencia artificial.\n",
    "\n",
    "\n",
    "Cuando se habla de los parámetros de un LLM nos estamos refiriendo a los valores internos que el modelo utiliza para realizar predicciones y generar texto basado en el lenguaje humano. Estos parámetros son esenciales para que el modelo funcione correctamente y son ajustados durante el proceso de entrenamiento. Los parámetros son en su mayoría pesos sinápticos. Estos pesos determinan la fuerza de la conexión entre las neuronas en diferentes capas del modelo. Durante el entrenamiento, estos pesos se ajustan para minimizar el error en la predicción del modelo. Junto con los pesos, los biases (sesgos) son otro tipo de parámetros que se añaden a las sumas ponderadas en las neuronas para ayudar al modelo a ajustarse mejor a los datos, son el equivalente a la ordenada en el origen. Los biases permiten que el modelo opere eficazmente cuando todas las entradas son cero.\n",
    "\n",
    "Los LLMs son conocidos por tener un número extremadamente alto de parámetros. Por ejemplo, GPT-3 de OpenAI tiene 175 mil millones de parámetros, lo que lo hace uno de los modelos de lenguaje más grandes y poderosos disponibles. Estos parámetros permiten al modelo capturar una gran cantidad de matices lingüísticos y contextuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d17c2",
   "metadata": {},
   "source": [
    "**¿Qué es un RAG?**\n",
    "\n",
    "La generación mejorada por recuperación (Retrieval Augmented Generation - RAG) es el proceso de optimización de un modelo lingüístico de gran tamaño (Large Language Model - LLM), de modo que conozca datos proporcionados por el usuario, que no existan en los datos de entrenamiento del modelo, antes de generar una respuesta. \n",
    "\n",
    "Los LLM se entrenan con grandes volúmenes de datos y usan miles de millones de parámetros para generar resultados originales en tareas como responder preguntas, traducir idiomas y completar frases. Un RAG extiende las ya poderosas capacidades de los LLM a dominios específicos o a la base de conocimientos interna de una organización, todo ello sin la necesidad de volver a entrenar el modelo. Se trata de un método rentable para mejorar los resultados de los LLM de modo que sigan siendo relevantes, precisos y útiles en diversos contextos.\n",
    "\n",
    "Un RAG aporta varios beneficios directos en el desarrollo de una herramienta de inteligencia artificial:\n",
    "\n",
    "+ Rentabilidad de la implementación. El desarrollo de IAs comienza normalmente con un modelo básico. Los modelos fundacionales (Foundational Models - FM) son LLMs accesibles por API entrenados en un amplio espectro de datos generalizados y sin etiquetar. Los costos computacionales y financieros de volver a entrenar a los FM para obtener información específica de la organización o del dominio son muy elevados. Un RAG es un enfoque más rentable para introducir nuevos datos en el LLM.\n",
    "\n",
    "\n",
    "+ Información actualizada. Incluso si el LLM está entrenado con los datos adecuados para las necesidades de la compañía, es complicado mantener la relevancia del modelo. Un RAG permite a los desarrolladores proporcionar las últimas investigaciones, estadísticas o noticias a los modelos generativos. Se puede usar un RAG para conectar el LLM directamente a las redes sociales en vivo, sitios de noticias u otras fuentes de información que se actualizan con frecuencia. De esta manera, un LLM puede proporcionar la información más reciente.\n",
    "\n",
    "\n",
    "+ Confianza. Al darle al LLM datos propios, se conoce perfectamente la fuente de datos además de evitar la alucinación del LLM.\n",
    "\n",
    "\n",
    "+ Mayor control. El RAG permite a los desarrolladores de inteligencia artificial cambiar las fuentes de información para adaptarse a los requisitos cambiantes o a los usos múltiples de la compañía. Además pueden restringir la recuperación de información confidencial a diferentes niveles de autorización y garantizar que el LLM genere las respuestas adecuadas.\n",
    "\n",
    "\n",
    "El esquema básico de un RAG es como sigue:\n",
    "\n",
    "<br>\n",
    "\n",
    "![rag](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Los documentos de la compañía se pasan por un modelo de incrustación (Embedding Model) para ser guardados en forma de vectores en una base de datos.\n",
    "\n",
    "2. La consulta realizada por el usuario pasa por el mismo modelo para convertir dicho texto en vectores.\n",
    "\n",
    "3. Se buscan en la base de datos los vectores más parecidos a la consulta y se extraen los vectores más relevantes.\n",
    "\n",
    "4. Los documentos más relevantes extraídos y la consulta se introducen en el LLM para generar la respuesta más adecuada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bc874",
   "metadata": {},
   "source": [
    "![langchain](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg)\n",
    "\n",
    "**¿Qué es LangChain?**\n",
    "\n",
    "\n",
    "LangChain es un framework de código abierto para crear aplicaciones basadas en LLM. LangChain proporciona herramientas y abstracciones para mejorar la personalización, precisión y relevancia de la información que generan los modelos. Por ejemplo, los desarrolladores pueden usar los componentes de LangChain para crear nuevas cadenas de peticiones o personalizar las plantillas existentes. LangChain también incluye componentes que permiten a los LLM acceder a nuevos conjuntos de datos sin necesidad de repetir el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ef96c",
   "metadata": {},
   "source": [
    "![huggingface](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/huggingface.png)\n",
    "\n",
    "**¿Qué es HuggingFace?**\n",
    "\n",
    "HuggingFace es una empresa de tecnología que se especializa en inteligencia artificial (IA), particularmente en el área de procesamiento del lenguaje natural (NLP). Es conocida por desarrollar y mantener la biblioteca \"Transformers\", que proporciona modelos de IA pre-entrenados y herramientas para facilitar la construcción de aplicaciones relacionadas con el lenguaje, como la traducción automática, el análisis de sentimientos y la generación de texto. La compañía también contribuye a la investigación en IA y fomenta una comunidad activa de desarrolladores y investigadores que colaboran en proyectos de código abierto. Además, HuggingFace opera una plataforma en línea donde los usuarios pueden experimentar con diferentes modelos de IA, compartir sus propios modelos y colaborar en proyectos de IA. Esta plataforma también facilita la implementación y el uso de modelos de IA en diversas aplicaciones prácticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f769bd",
   "metadata": {},
   "source": [
    "## 2. Importamos la API KEY de HuggingFace. \n",
    "\n",
    "Para usar los modelos de [HuggingFace](https://huggingface.co/models), vamos a necesitar el [token](https://huggingface.co/settings/tokens) que nos proporciona la plataforma. Por supuesto, necesitamos crearnos una cuenta en HuggingFace. El token lo guardamos en un archivo `.env` para cargarlo con la librería dotenv y usarlo como variable de entorno. Dicho archivo se añade al `.gitignore` para que nadie pueda verlo si subimos el código a GitHub por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la API KEY\n",
    "\n",
    "import os                           # libreria del sistema operativo\n",
    "from dotenv import load_dotenv      # carga variables de entorno \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# importamos el token\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce409b23",
   "metadata": {},
   "source": [
    "## 3. Llamada a HuggingFace desde LangChain\n",
    "\n",
    "![modelo](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/modelo.png)\n",
    "\n",
    "Vamos a usar el modelo [zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), una versión del modelo mistralai/Mistral-7B-v0.1 que fue entrenado con una mezcla de conjuntos de datos sintéticos y públicamente disponibles utilizando optimización de preferencias directas (Direct Preference Optimization - [DPO](https://arxiv.org/abs/2305.18290)). Zephyr es un modelo de generación de texto con aproximadamente 7 mil millones de parámetros similar en rendimiento a GPT-3.5-turbo. \n",
    "\n",
    "Cargaremos el modelo desde el hub de HuggingFace. El parámetro `max_new_tokens` especifica el número máximo de tokens nuevos que el modelo puede generar en una sola invocación de su función de predicción o generación de texto. Usaremos 512 tokens máximos. Cuando un modelo genera texto, selecciona el siguiente token basándose en una distribución de probabilidad de todos los tokens posibles. El parámetro `top_k` restringe esta selección a los tokens más probables, donde k es un número entero definido por el usuario. Por ejemplo, si top_k está configurado en 30, el modelo solo considerará los 30 tokens más probables como candidatos para ser el siguiente token en la secuencia. Esto limita la elección a un subconjunto de posibilidades más probable y puede ayudar a que el texto generado sea más coherente y menos propenso a respuestas aleatorias o incoherencias. El parámetro `temperature` en un modelo es una configuración que afecta la aleatoriedad de las respuestas generadas por el modelo. Este parámetro ayuda a controlar cómo de conservador o aventurero será el modelo al seleccionar palabras durante la generación de texto. Una temperatura baja, 0.5 o menos, hace que el modelo sea más determinista, tendiendo a seleccionar los tokens más probables. Esto resulta en respuestas más coherentes y predecibles, pero potencialmente menos creativas o variadas. Una temperatura alta, 1.0 o más, hace que el modelo sea menos determinista y más propenso a tomar riesgos, seleccionando tokens menos probables. Esto puede generar respuestas más diversas y creativas, pero también puede aumentar la probabilidad de errores o respuestas incoherentes. Usaremos una temperatura de 0.1. El parámetro `repetition_penalty` en un modelo se utiliza para desalentar la repetición de palabras o frases en la generación de texto. Este ajuste ayuda a aumentar la variedad y la coherencia en las respuestas generadas, haciéndolas más agradables y naturales. Si $repetitionpenalty > 1$, la probabilidad de tokens repetidos disminuye, lo que desincentiva la repetición. Cuanto mayor sea el valor, más fuerte será el efecto en desalentar la repetición. Si $repetitionpenalty < 1$, la probabilidad de tokens repetidos aumenta, lo que podría no ser deseable pero puede ser útil en contextos específicos donde la repetición es preferida. Usaremos 1.03 como repetition_penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55bff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                     task=\"text-generation\",\n",
    "                     huggingfacehub_api_token=HUGGINGFACE_TOKEN,\n",
    "                     \n",
    "                     model_kwargs={'max_new_tokens': 512,\n",
    "                                   'top_k': 30,\n",
    "                                   'temperature': 0.1,\n",
    "                                   'repetition_penalty': 1.03})\n",
    "\n",
    "\n",
    "modelo = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocamos al modelo para que genere la respuesta\n",
    "\n",
    "respuesta = modelo.invoke('¿Quién ganó la liga española en 2020?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta del modelo\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipo de dato de la respuesta\n",
    "\n",
    "type(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c25a56",
   "metadata": {},
   "source": [
    "Como veis, la respuesta del modelo es un objeto de LangChain AIMessage. Si queremos extraer la string de la respuesta podemos hacerlo directamente con el atributo `content` de la respuesta. Probémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a637bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta.content.split('<|assistant|>')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bcdcae",
   "metadata": {},
   "source": [
    "### [Output Parser](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "\n",
    "El propio LangChain tiene un parser para la salida de string, `StrOutputParser`. Parse se refiere a cambiar el tipo de formato, básicamente es traducir. LangChain tiene muchos parsers incorporados, desde strings hasta csv pasando por JSON o incluso Pandas DataFrames. \n",
    "\n",
    "Desde el parser `StrOutputParser` de LangChain el modelo devuelve directamente la respuesta en formato string. Este parser lo usaremos a la hora de crear la cadena de LangChain, será un eslabón más de la misma y así obtendremos directamente la respuesta del LLM en formato string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca93a0",
   "metadata": {},
   "source": [
    "## 4. Creación de la cadena\n",
    "\n",
    "Una cadena de LangChain es una secuencia configurada de componentes o pasos que se encadenan para realizar tareas complejas de procesamiento del lenguaje. LangChain es una herramienta diseñada para facilitar la construcción y el despliegue de aplicaciones de lenguaje, integrando modelos de lenguaje avanzados y otras funcionalidades en flujos de trabajo coherentes y efectivos. El propósito de una cadena de LangChain es permitir que cada paso del proceso contribuya de manera efectiva al resultado final. Una cadena de LangChain típicamente incluirá varios componentes clave:\n",
    "\n",
    "+ Recuperadores (Retrievers): Componentes que buscan y recuperan información relevante de una base de datos o de un conjunto de documentos. Estos son esenciales para proporcionar contexto o datos específicos que el modelo de lenguaje necesita para generar respuestas informadas.\n",
    "\n",
    "+ Modelos de Lenguaje: El núcleo de una cadena de LangChain, donde se utilizan modelos avanzados como GPT-3 para generar texto, resolver preguntas, o realizar otras tareas de NLP basadas en la información y contextos proporcionados por otros componentes de la cadena.\n",
    "\n",
    "+ Post-procesadores: Componentes que toman la salida del modelo de lenguaje y la refinan, por ejemplo, corrigiendo errores, formateando la respuesta, o aplicando filtros adicionales de contenido.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ahora crearemos la cadena que tiene básicamente dos eslabones: el modelo y el parser a string. El flujo del proceso sería:\n",
    "\n",
    "1. Generamos una consulta con una instrucción en texto (prompt).\n",
    "2. Le pasamos la consulta al modelo que hemos inicializado.\n",
    "3. El modelo devuelve una respuesta.\n",
    "4. La respuesta del modelo entra en el parser.\n",
    "5. El parser devuelve la respuesta en formato string.\n",
    "\n",
    "![chain1](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la cadena\n",
    "\n",
    "cadena = modelo | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocamos a la cadena dándole la consulta para que realice todo el flujo\n",
    "\n",
    "cadena.invoke('¿Quién ganó la liga española en 2020?').split('<|assistant|>')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7b1f6",
   "metadata": {},
   "source": [
    "### [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)\n",
    "\n",
    "Fijaros en esta expresión:\n",
    "\n",
    "```python\n",
    "chain = model | parser\n",
    "```\n",
    "\n",
    "Esta expresión no es un [bitwise operator](https://realpython.com/python-bitwise-operators/) de Python, sino que usa para crear la cadena dentro de LangChain y concatenar las funciones dentro del diccionario del agente. Funcionaría como un grafo directo, haciendo que se ejecuten de una en una las tareas de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64182ffd",
   "metadata": {},
   "source": [
    "## 5. Plantillas de instrucciones ([Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/))\n",
    "\n",
    "Las plantillas de instrucciones son recetas predefinidas para generar instrucciones para modelos de lenguaje.\n",
    "\n",
    "Una plantilla puede incluir instrucciones, contexto y preguntas específicas apropiadas para una tarea dada. LangChain proporciona herramientas para crear y trabajar con plantillas de instrucciones y además se esfuerza por crear plantillas agnósticas al modelo para facilitar la reutilización de plantillas existentes en diferentes modelos de lenguaje.\n",
    "\n",
    "![chain2](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain2.png)\n",
    "\n",
    "Típicamente, los modelos de lenguaje esperan que la instrucción sea una cadena de texto o una lista de mensajes de chat. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ded55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6be4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt3.5 funciona mejor en inglés, hagamos un ejemplo en esa lengua\n",
    "\n",
    "\n",
    "# plantilla de texto con un contexto y una pregunta\n",
    "plantilla = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            '''\n",
    "\n",
    "# carga de la plantilla en el prompt\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "\n",
    "# formato de salida del prompt\n",
    "prompt.format(context='The building where Pepe lives is green.', \n",
    "              question=\"What color is Pepe's house?\").split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b27cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la cadena añadiendo la plantilla del prompt en el primer paso\n",
    "\n",
    "cadena = prompt | modelo | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocamos a la cadena\n",
    "\n",
    "cadena.invoke({'context': 'The building where Pepe lives is green.',\n",
    "               'question': \"What color is Pepe's house?\"}).split('<|assistant|>')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50fbd6",
   "metadata": {},
   "source": [
    "Fijaros que ahora en la invocación de la cadena no le pasamos una string, sino que le damos un diccionario que contiene tanto el contexto como la pregunta. Esto es necesario, puesto que el primer paso de la cadena es la creación del prompt y necesita de ambos términos para su creación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3747fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probemos directamente la cadena en castellano, a ver que ocurre\n",
    "\n",
    "cadena.invoke({'context': 'Tengo un coche blanco',\n",
    "               'question': '¿De qué color es mi coche?'}).split('<|assistant|>')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otro ejemplo\n",
    "\n",
    "contexto = 'Los números naturales son: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10'\n",
    "\n",
    "\n",
    "pregunta = '''¿Cuales son los números naturales pares? \n",
    "              Segun la respuesta, súmalos.\n",
    "              Devuelve solamente el resultado numérico.'''\n",
    "\n",
    "\n",
    "cadena.invoke({'context': contexto,\n",
    "               'question': pregunta}).split('<|assistant|>')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b02e7",
   "metadata": {},
   "source": [
    "## 6. Combinando cadenas\n",
    "\n",
    "\n",
    "Acabamos de ver que el modelo de lenguaje funciona también en castellano. Pero podemos crear cadenas para flujos de trabajo más complejos, como por ejemplo, traducir lo que hace la primera cadena a otro lenguaje. \n",
    "\n",
    "![chain3](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain3.png)\n",
    "\n",
    "Usaremos otra plantilla de instrucciones para combinarla con la cadena que ya hemos construido, de tal manera que traduzca al castellano la respuesta del modelo. Veámoslo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a usar esta funcion como último eslabón para limpiar la salida del parser\n",
    "\n",
    "def clean_parser(response: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Funcion para limpiar la salida del parser\n",
    "    \"\"\"\n",
    "    \n",
    "    return response.split('<|assistant|>')[-1].split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc44189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la plantilla de traducción con la respuesta de la cadena y el lenguaje de salida\n",
    "\n",
    "prompt_traductor = ChatPromptTemplate.from_template('Translate {answer} to {language}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b223fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la nueva cadena basada en la anterior a la cual le damos el lenguaje al que queremos traducir\n",
    "\n",
    "cadena_traducida = (\n",
    "    {'answer': cadena, 'language': lambda x: x['language']} | prompt_traductor | modelo | parser | clean_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53689d",
   "metadata": {},
   "source": [
    "Fijaros que el primer paso de la cadena traducida es un diccionario con la respuesta de la cadena original por un lado y el lenguaje por el otro. La razón de la función `lambda` es que cuando invoquemos a esta cadena compuesta le pasaremos también un diccionario cuyos valores son ejecutables, como hicimos antes, con el contexto, la pregunta y el lenguaje al que queremos traducir la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocamos a la nueva cadena, dándole el contexto, la pregunta y el lenguaje de salida\n",
    "\n",
    "cadena_traducida.invoke({'context': 'The building where Pepe lives is green.',\n",
    "                         'question': \"What color is Pepe's house?\",\n",
    "                         'language': 'Spanish',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traducimos a otro idioma para probar la cadena\n",
    "\n",
    "cadena_traducida.invoke({'context': 'The building where Pepe lives is green.',\n",
    "                         'question': \"What color is Pepe's house?\",\n",
    "                         'language': 'French',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otro ejemplo\n",
    "\n",
    "cadena_traducida.invoke({'context': 'The building where Pepe lives is green.',\n",
    "                         'question': \"What color is Pepe's house?\",\n",
    "                         'language': 'German',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23af0f6",
   "metadata": {},
   "source": [
    "Hasta aquí, tenemos una cadena a la cual le damos un contexto, una pregunta y un lenguaje y nos devuelve una respuesta contextualizada en el idioma seleccionado. Nos hemos propuesto transcribir un video de YouTube para que la cadena lo use como contexto y podamos realizar consultas a dicha transcripción. Para ello lo primero será extraer el texto desde un video. Vamos a ello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590d36a",
   "metadata": {},
   "source": [
    "## 7. Extracción de texto desde YouTube con Whisper\n",
    "\n",
    "Primero tenemos que importar las librerías que vamos a usar. Usaremos [pytube](https://pytube.io/en/latest/), para acceder al video y luego [whisper](https://openai.com/research/whisper), el modelo de speech-to-text de OpenAI. También tendremos que instalar [ffmpeg](https://ffmpeg.org/) según nuestro sistema operativo para que nuestra máquina sea capaz de analizar el audio que viene desde el video de YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e388fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la url del video: Planned Chaos - by Ludwig von Mises - (Full Audiobook) (3:09:32)\n",
    "\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=7EnHeZXLzTc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870719c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos pytube para extraer el video\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora extraemos el audio desde el video\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddceadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se carga el modelo base de Whisper en local, 139M\n",
    "\n",
    "modelo_whisper = whisper.load_model('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb39e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descripción del modelo whisper\n",
    "\n",
    "modelo_whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e11814",
   "metadata": {},
   "source": [
    "**¿Cómo funciona whisper?**\n",
    "\n",
    "Como veis, Whisper está básicamente dividido en dos partes: un AudioEncoder y un TextDecoder. Esto es lo que se conoce como AutoEncoder. El AudioEncoder reconoce el audio y lo vectoriza y desde esos vectores el TextDecoder saca el texto. Por daros un paso a paso sería algo así:\n",
    " \n",
    "\n",
    "1. Preprocesamiento del Audio: Whisper comienza con el preprocesamiento del audio de entrada, donde se ajusta la calidad del audio para mejorar la precisión del reconocimiento. Esto puede incluir normalizar el volumen, filtrar ruido de fondo y segmentar el audio en trozos más manejables.\n",
    "\n",
    "2. Extracción de Características: El modelo extrae características del audio procesado. Esto implica convertir las señales de audio en una forma que el modelo pueda entender, como espectrogramas o características de Mel-frequency cepstral coefficients (MFCC), que son representaciones de la frecuencia y amplitud del sonido a lo largo del tiempo.\n",
    "\n",
    "3. Modelo de Aprendizaje: Whisper utiliza un modelo de red neuronal, entrenado en una gran cantidad de datos de audio y transcripciones correspondientes. Este modelo aprende a identificar patrones y correlaciones entre el audio y las transcripciones textuales.\n",
    "\n",
    "4. Reconocimiento y Traducción: Durante la fase de reconocimiento, Whisper convierte el audio en texto utilizando su red neuronal. Además, puede traducir el texto reconocido a otros idiomas, gracias a su entrenamiento en múltiples idiomas.\n",
    "\n",
    "5. Post-procesamiento: Finalmente, el texto generado pasa por un proceso de post-procesamiento para corregir errores comunes, ajustar la puntuación y mejorar la coherencia del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc59b9",
   "metadata": {},
   "source": [
    "Usaremos ahora Whisper para transcribir el audio a un archivo de texto. Primero crearemos un directorio temporal donde se guardará el audio y luego guardaremos la salida de Whisper en el archivo `transcripcion.txt`. Si ya existe el archivo, no nos interesa convertirlo otra vez, puesto que Whisper tarda unos minutos en trancribir todo el audio, asi que solo ejecutamos el modelo si el archivo de texto no existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da14c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile    # para manejo de archivos temporales\n",
    "\n",
    "\n",
    "# ruta de guardado del archivo de texto\n",
    "\n",
    "RUTA_TXT = 'txt/transcripcion.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2004ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# si el archivo de texto no existe...\n",
    "if not os.path.exists(RUTA_TXT):\n",
    "    \n",
    "    # abrimos el directorio temporal...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # descargamos el audio de YouTube...\n",
    "        archivo_audio = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # y Whisper transcribe el audio a texto, en 32 bits (fp16=False)\n",
    "        transcripcion = modelo_whisper.transcribe(archivo_audio, fp16=False)['text'].strip()\n",
    "        \n",
    "        \n",
    "        # se guarda el archivo de texto\n",
    "        with open(RUTA_TXT, 'w') as archivo_texto:\n",
    "            archivo_texto.write(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9186507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el archivo de texto para usarlo en la cadena\n",
    "\n",
    "with open(RUTA_TXT, 'r') as archivo_texto:\n",
    "    \n",
    "    transcripcion = archivo_texto.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 primeros caracteres del texto\n",
    "    \n",
    "transcripcion[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966944f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº de caracteres del texto\n",
    "\n",
    "len(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº de palabras del texto\n",
    "\n",
    "len(transcripcion.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 últimos caracteres\n",
    "\n",
    "transcripcion[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334470d5",
   "metadata": {},
   "source": [
    "## 8. Usando la transcripción como contexto\n",
    "\n",
    "La idea fundamental del RAG es que el LLM tenga nuestra transcripción como contexto a la hora de generar respuestas, es decir, que conozca el texto para poder respondernos las preguntas que tengamos al respecto del mismo. Vamos a ver que ocurre cuando lo hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cadena.invoke({\n",
    "        'context': transcripcion,\n",
    "        'question': 'What are the key points of this book? Put them in a bullet point list.'})\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e2865",
   "metadata": {},
   "source": [
    "Si nos fijamos en el mensaje del error, nos está diciendo que la longitud máxima del contexto del modelo zephyr es de 17432 tokens pero nuestro contexto tiene 33960 tokens. Un token no es un caracter ni una palabra, recordad que nuestro texto tiene 159723 y 25880 respectivamente. Un token es una unidad de texto que el modelo utiliza para procesar y generar lenguaje. Los tokens son las piezas fundamentales de información con las que el modelo trabaja, y pueden variar en gran medida dependiendo de cómo está diseñado el modelo y cómo se ha entrenado. Un token puede ser una palabra completa, una parte de una palabra (como prefijos, raíces o sufijos), o incluso puntuación y espacios. La forma en que se dividen los textos en tokens depende del \"tokenizador\" utilizado por el modelo. La tokenización es uno de los procesos esenciales para el funcionamiento de los LLM, actuando como los bloques constructivos básicos para todas las tareas de procesamiento de lenguaje que realiza el modelo.\n",
    "\n",
    "El problema aquí es que el texto es demasiado grande, de hecho el mensaje de error nos recomienda reducir la longitud del texto. Pero existe otra solución mejor a este problema que reducir el contenido del texto. Además, nosotros queremos que toda la transcripción sea el contexto que el LLM utilice para darnos una respuesta. Veamos esa solución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b82917",
   "metadata": {},
   "source": [
    "### Dividiendo en segmentos el texto completo\n",
    "\n",
    "![chain4](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain4.png)\n",
    "\n",
    "Dado que el texto completo es demasiado grande para ser usado como contexto para el LLM, la solución obvia es dividir el texto en varios trozos para darle al modelo textos más pequeños como contexto. Para dividir el texto, primero usaremos el TextLoader de LangChain para cargar toda la transcripción del video como un objeto Document de LangChain. Un Document es un fragmento de texto con metadatos asociados. \n",
    "\n",
    "El cargador de texto proporciona un método \"load\" para cargar datos como Document desde una fuente como un txt o un md, por ejemplo. Opcionalmente, implementan también una \"lazy load\" para cargar los datos en la memoria. \"Lazy\" se refiere a una estrategia de carga o inicialización que pospone la ejecución de una operación hasta que es estrictamente necesaria. Esta técnica es comúnmente usada para mejorar el rendimiento y optimizar el uso de los recursos del sistema, especialmente la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5110bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(RUTA_TXT)\n",
    "\n",
    "documento_texto = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documento_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documento_texto[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fab5e",
   "metadata": {},
   "source": [
    "Una vez cargado el texto completo como un objeto de LangChain, lo dividimos con un [text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/). Existen distintos tipos de splitters según el tipo de archivo que queramos dividir, ya sea texto, HTML, markdown o código. Vamos a un usar un splitter recursivo. Dividir el texto de manera recursiva tiene el propósito de intentar mantener las piezas de texto relacionadas una con la siguiente. Es la manera recomendada de comenzar a dividir texto.\n",
    "\n",
    "Lo que haremos será dividir la transcripción en trozos (chunks) de 1000 caracteres con una superposición de 30 caracteres, es decir, cada trozo tendrá 1000 caracteres y los 30 últimos caracteres de uno serán los mismos que los 30 primeros caracteres del siguiente trozo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff03944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
    "\n",
    "documentos = splitter.split_documents(documento_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de trozos de texto desde la transcripción completa\n",
    "\n",
    "len(documentos)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos los dos primeros\n",
    "\n",
    "documentos[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a625d",
   "metadata": {},
   "source": [
    "### Encontrando los trozos relevantes\n",
    "\n",
    "Una vez troceado el texto completo la pregunta surge naturalmente, ¿qué trozos deberíamos de darle al LLM como contexto?. No usaremos cualquiera de los 165 trozos de texto que hemos generado, sino que tenemos que usar aquellos que sean los más relevantes para la consulta que estamos haciendo. Es aquí donde entra el concepto de embeddings. Los embeddings son representaciones vectoriales de palabras, frases, u otros elementos en un espacio vectorial continuo de baja dimensión. Son una herramienta fundamental en el campo del procesamiento del lenguaje, ya que permiten que los modelos trabajen con información textual de una manera matemática. Recordemos que las máquinas solo entienden de números.\n",
    "\n",
    "\n",
    "![chain5](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain5.png)\n",
    "\n",
    "\n",
    "Usaremos el modelo de embeddings de RoBERTa desde HuggingFace que ya viene incorporado en LangChain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0eaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# inicializamos el modelo de embedding con Roberta\n",
    "\n",
    "vectorizador = HuggingFaceEmbeddings(model_name='sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded27922",
   "metadata": {},
   "source": [
    "Hagamos un ejemplo sencillo para entender el proceso antes de vectorizar todos los trozos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizamos una frase\n",
    "\n",
    "consulta = vectorizador.embed_query('Hola que tal, esto es una clase de IA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longitud del vector\n",
    "\n",
    "print(f'Longitud del vector consulta: {len(consulta)}\\n')\n",
    "\n",
    "\n",
    "# primero 5 elementos del vector\n",
    "\n",
    "print(consulta[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad989c0",
   "metadata": {},
   "source": [
    "Como véis, los embeddings tienen 1024 dimensiones. Esta dimensión de los embeddings está relacionada con la capacidad del modelo para capturar y representar la información lingüística de manera más rica y detallada. Entrando un poco más en detalle:\n",
    "\n",
    "1. Representación Más Rica: Un mayor número de dimensiones en los embeddings permite que el modelo capture una gama más amplia de matices en los datos de texto. Con 1024 dimensiones, RoBERTa puede representar características lingüísticas complejas y relaciones entre palabras de una manera más efectiva que con menos dimensiones.\n",
    "\n",
    "2. Mejora en el Desempeño de Tareas NLP: La profundidad adicional en los embeddings ayuda al modelo a desempeñarse mejor en una variedad de tareas de procesamiento del lenguaje, incluyendo la comprensión, la inferencia textual\n",
    "\n",
    "3. Uso de Memoria y Cómputo: Aunque aumentar las dimensiones de los embeddings mejora la capacidad del modelo, también requiere más memoria y poder de cómputo. Los diseñadores de RoBERTa decidieron que 1024 dimensiones eran un buen equilibrio entre el rendimiento mejorado y los requisitos de recursos.\n",
    "\n",
    "4. Evidencia Empírica: Las configuraciones de modelos como BERT y RoBERTa se basan en pruebas extensivas y experimentación. Los investigadores y desarrolladores prueban diferentes configuraciones de dimensiones para encontrar aquella que ofrezca el mejor rendimiento en las métricas relevantes, como la precisión de la tarea y la eficiencia del entrenamiento.\n",
    "\n",
    "5. Comparación con Otras Configuraciones: RoBERTa está disponible en varias versiones, donde la cantidad de dimensiones varía (por ejemplo, base vs. large). Esto permite que diferentes usuarios elijan la versión que mejor se adapte a sus necesidades y recursos. La versión large con 1024 dimensiones está orientada a aquellos que necesitan el máximo rendimiento y tienen los recursos para soportarlo. y la respuesta a preguntas. Esto se debe a que puede diferenciar mejor entre los contextos y significados más sutiles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058697b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora vectoricemos dos frase más \n",
    "\n",
    "frase1 = vectorizador.embed_query('¿Hoy estuviste estudiando o no?')\n",
    "\n",
    "frase2 = vectorizador.embed_query('Estamos en clase, hay que ponerse a estudiar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f3dec",
   "metadata": {},
   "source": [
    "Ahora que tenemos dos frases vectorizadas, queremos saber cual se parece más a la consulta, es decir, cual de las dos frases es la más relevante. El método más habitual para hacer esto es la similitud del coseno dado que estamos trabajando con vectores. La similitud del coseno mide el coseno del ángulo entre dos vectores en un espacio vectorial. El valor de esta similitud varía entre -1 y 1, donde 1 indica que los dos vectores son idénticos en orientación (ángulo de 0 grados), 0 sugiere que los vectores son ortogonales (ángulo de 90 grados) y no comparten ninguna información, -1 indica que los vectores son diametralmente opuestos (ángulo de 180 grados). De esta manera obtenemos una métrica de similitud entre los vectores. Desde la librería `scikit-learn` podemos usar la similitud del coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similitud_frase1 = cosine_similarity([consulta], [frase1])[0][0]\n",
    "\n",
    "similitud_frase2 = cosine_similarity([consulta], [frase2])[0][0]\n",
    "\n",
    "similitud_frase1, similitud_frase2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d44a8",
   "metadata": {},
   "source": [
    "Como vemos, la frase más parecida a nuestra consulta es la frase 2. Con esta estrategia podemos saber que trozos de texto son los más relevantes para la consulta específica que hagamos al LLM. El proceso será vectorizar todos los trozos de texto y luego calcular la similitud de los mismos con nuestra consulta para obtener los textos más relevantes y usarlos como contexto en el LLM. Para ello, lo adecuado es guardar los vectores obtenidos por el modelo de embedding en una base de datos y luego realizar una búsqueda de los vectores más relevantes.\n",
    "\n",
    "\n",
    "![chain6](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77ed2b",
   "metadata": {},
   "source": [
    "## 9. Bases de datos vectoriales\n",
    "\n",
    "\n",
    "LangChain provee de una buena cantidad de [bases de datos vectoriales](https://python.langchain.com/docs/integrations/vectorstores/) para el manejo de los embeddings. Vamos probar primero con una base de datos local en memoria, DocArrayInMemorySearch, que es un índice de documentos proporcionado por Docarray que almacena documentos en memoria. Es un excelente punto de partida para conjuntos de datos pequeños, donde es posible que no desees iniciar un servidor de base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "\n",
    "# a docarray le pasamos los texto y el modelo de embedding\n",
    "\n",
    "local_db = DocArrayInMemorySearch.from_documents(documentos, vectorizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos extraer los k vectores más relevates con su similitud, donde k es el nº de trozos que queremos \n",
    "\n",
    "consulta = 'What are the key points of this book? Put them in a bullet point list.'\n",
    "\n",
    "local_db.similarity_search_with_score(query=consulta, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f01465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# también podemos extraer los k vectores más relevates sin similitud\n",
    "\n",
    "local_db.similarity_search(query=consulta, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6257598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o definir un recuperador de la base de datos para invocarlo que por defecto devuelve los 4 más relevantes\n",
    "\n",
    "recuperador = local_db.as_retriever()\n",
    "\n",
    "recuperador.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71382c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recuperador.invoke(consulta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aunque podemos definirlo para extraer los que queramos\n",
    "\n",
    "# o definir un recuperador de la base de datos para invocarlo que por defecto devuelve los 4 más parecidos\n",
    "\n",
    "recuperador = local_db.as_retriever(search_kwargs={'k': 10})\n",
    "\n",
    "len(recuperador.invoke(consulta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283794d",
   "metadata": {},
   "source": [
    "### Conectando la base de datos a la cadena\n",
    "\n",
    "\n",
    "![chain7](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain7.png)\n",
    "\n",
    "\n",
    "Ahora que tenemos la base de datos disponible en memoria, podemos conectarla a la cadena de LangChain, junto con la plantilla del prompt, el modelo LLM y el parser a string que habiamos definido anteriormente. Para ello vamos a usar dos objetos que nos proporciona LangChain.\n",
    "\n",
    "RunnableParallel es esencialmente un diccionario cuyos valores son ejecutables, u objetos que pueden convertirse en ejecutables, como funciones. Ejecuta todos sus valores en paralelo, y cada valor se llama con la entrada general del RunnableParallel. El valor de retorno final es un diccionario con los resultados de cada valor bajo su clave correspondiente. Es útil para paralelizar operaciones, pero también puede ser útil para manipular la salida de un Runnable (protocolo ejecutable) para que coincida con el formato de entrada del siguiente Runnable en una secuencia. Se espera que la entrada al prompt sea un diccionario con las claves \"contexto\" y \"pregunta\". La entrada del usuario es solo la pregunta. Por lo tanto, necesitamos obtener el contexto usando nuestro recuperador y pasar la entrada del usuario.\n",
    "\n",
    "RunnablePassthrough por sí solo permite pasar las entradas sin cambios. Esto típicamente se usa en conjunto con RunnableParallel para pasar datos a una nueva clave en el diccionario que entra en la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "\n",
    "# ejecutamos el RunnableParallel para obtener el contexto\n",
    "\n",
    "contexto_recuperado = RunnableParallel(context=recuperador, question=RunnablePassthrough())\n",
    "\n",
    "contexto_recuperado.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91adfa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creamos la cadena con el contexto recuperado desde la bade de datos\n",
    "\n",
    "cadena = contexto_recuperado | prompt | modelo | parser | clean_parser\n",
    "\n",
    "cadena.invoke(consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372323e",
   "metadata": {},
   "source": [
    "Como hemos comentado, Docarray almacena documentos en memoria local. Pero para una aplicación real no nos sirve. Si queremos hacer un despliegue del modelo para una aplicación en producción necesitaremos una base de datos que podamos alojar en un servidor. Veamos un par de opciones open source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac9913",
   "metadata": {},
   "source": [
    "###  Chroma \n",
    "\n",
    "![chroma](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chroma.jpeg)\n",
    "\n",
    "\n",
    "Otra opción que tenomos es [ChromaDB](https://www.trychroma.com/), que es una base de datos especializada en el almacenamiento y recuperación eficiente de información lingüística, incluyendo datos de texto, anotaciones semánticas y sintácticas. ChromaDB es particularmente útil para el almacenamiento y la gestión de grandes cantidades de datos de lenguaje natural, lo que permite a los desarrolladores aprovechar al máximo los avances en algoritmos de aprendizaje automático y análisis de texto. Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado en disco\n",
    "\n",
    "chroma_db = Chroma.from_documents(documentos, vectorizador, persist_directory='save_db')\n",
    "\n",
    "docs = chroma_db.similarity_search(consulta, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430df1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga desde disco\n",
    "\n",
    "chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)\n",
    "\n",
    "docs = chroma_db.similarity_search(consulta)\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cadena con traducción y con la recuperacion de ChromaDB con 5 documentos\n",
    "\n",
    "cadena =  prompt | modelo | parser\n",
    "\n",
    "cadena_traducida = (\n",
    "    {'answer': cadena, 'language': lambda x: x['language']} \n",
    "    | prompt_traductor \n",
    "    | modelo \n",
    "    | parser \n",
    "    | clean_parser\n",
    ")\n",
    "\n",
    "cadena_traducida.invoke({'context': chroma_db.similarity_search(consulta, k=10), \n",
    "                         'question': consulta,\n",
    "                         'language': 'Spanish'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d2cae",
   "metadata": {},
   "source": [
    "Desde aquí, ya podríamos hacer un despligue de nuestra cadena para implementarla en una aplicación en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c5881",
   "metadata": {},
   "source": [
    "## 10. Resumen paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ced09e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# librerias\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from pytube import YouTube\n",
    "import whisper\n",
    "import tempfile\n",
    "\n",
    "import os                           \n",
    "from dotenv import load_dotenv      \n",
    "\n",
    "\n",
    "# vamos a usar esta funcion como último eslabón para limpiar la salida del parser\n",
    "def clean_parser(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Funcion para limpiar la salida del parser\n",
    "    \"\"\"\n",
    "    return response.split('<|assistant|>')[-1].split('\\n')\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# importamos el token\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN')\n",
    "\n",
    "\n",
    "# iniciamos modelo \n",
    "llm = HuggingFaceHub(repo_id='HuggingFaceH4/zephyr-7b-beta',\n",
    "                     task='text-generation',\n",
    "                     huggingfacehub_api_token=HUGGINGFACE_TOKEN,\n",
    "                     \n",
    "                     model_kwargs={'max_new_tokens': 512,\n",
    "                                   'top_k': 30,\n",
    "                                   'temperature': 0.1,\n",
    "                                   'repetition_penalty': 1.03})\n",
    "\n",
    "\n",
    "modelo = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "# plantilla de texto con un contexto y una pregunta\n",
    "plantilla = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            '''\n",
    "\n",
    "# carga de la plantilla en el prompt\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "\n",
    "# traduccion\n",
    "prompt_traductor = ChatPromptTemplate.from_template('Translate {answer} to {language}')\n",
    "\n",
    "\n",
    "# extraccion de datos\n",
    "# definimos la url del video\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=7EnHeZXLzTc'\n",
    "\n",
    "# usamos pytube para extraer el video\n",
    "youtube = YouTube(VIDEO_URL)\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "# se carga el modelo base de Whisper \n",
    "modelo_whisper = whisper.load_model('base')\n",
    "\n",
    "\n",
    "# ruta de guardado del archivo de texto\n",
    "RUTA_TXT = 'txt/transcripcion.txt'\n",
    "\n",
    "# si el archivo de texto no existe...\n",
    "if not os.path.exists(RUTA_TXT):\n",
    "    \n",
    "    # abrimos el directorio temporal...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # descargamos el audio de YouTube...\n",
    "        archivo_audio = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # y Whiper transcribe el audio a texto.\n",
    "        transcripcion = modelo_whisper.transcribe(archivo_audio, fp16=False)['text'].strip()\n",
    "        \n",
    "        \n",
    "        # se guarda el archivo de texto\n",
    "        with open(RUTA_TXT, 'w') as archivo_texto:\n",
    "            archivo_texto.write(transcripcion)\n",
    "\n",
    "\n",
    "# cargamos el archivo de texto para usarlo en la cadena\n",
    "with open(RUTA_TXT, 'r') as archivo_texto:\n",
    "    transcripcion = archivo_texto.read()\n",
    "\n",
    "\n",
    "# transformacion del dato\n",
    "loader = TextLoader(RUTA_TXT)\n",
    "documento_texto = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
    "documentos = splitter.split_documents(documento_texto)\n",
    "\n",
    "\n",
    "# inicializamos el modelo de embedding con Roberta\n",
    "vectorizador = HuggingFaceEmbeddings(model_name='sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "\n",
    "\n",
    "# guardado en disco, no sería necesario, hacer aparte\n",
    "chroma_db = Chroma.from_documents(documentos, vectorizador, persist_directory='save_db')\n",
    "\n",
    "\n",
    "# carga desde disco\n",
    "chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# consulta\n",
    "consulta = 'What are the key points of this book? Put them in a bullet point list.'\n",
    "\n",
    "\n",
    "# parser a string\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cadena con traducción y con la recuperacion de ChromaDB con 15 documentos\n",
    "cadena =  prompt | modelo | parser\n",
    "\n",
    "\n",
    "cadena_traducida = (\n",
    "    {'answer': cadena, 'language': lambda x: x['language']} \n",
    "    | prompt_traductor \n",
    "    | modelo \n",
    "    | parser \n",
    "    | clean_parser\n",
    ")\n",
    "\n",
    "cadena_traducida.invoke({'context': chroma_db.similarity_search(consulta, k=15), \n",
    "                         'question': consulta,\n",
    "                         'language': 'Spanish'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb153143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {
    "height": "379px",
    "width": "533px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "48px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
