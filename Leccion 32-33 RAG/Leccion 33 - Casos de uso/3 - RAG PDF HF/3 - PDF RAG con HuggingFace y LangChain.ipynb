{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93ae013",
   "metadata": {},
   "source": [
    "# 3 - PDF RAG con HuggingFace y LangChain\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag_1.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9510e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Definiciones-y-dependencias.\" data-toc-modified-id=\"1.-Definiciones-y-dependencias.-0\">1. Definiciones y dependencias.</a></span></li><li><span><a href=\"#2.-Carga-de-archivos-PDF.\" data-toc-modified-id=\"2.-Carga-de-archivos-PDF.-1\">2. Carga de archivos PDF.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Document-Loaders-de-LangChain\" data-toc-modified-id=\"Document-Loaders-de-LangChain-1.1\"><a href=\"https://python.langchain.com/docs/modules/data_connection/document_loaders/\" rel=\"nofollow\" target=\"_blank\">Document Loaders de LangChain</a></a></span></li></ul></li><li><span><a href=\"#3.-Troceando-los-documentos-(Chunks)\" data-toc-modified-id=\"3.-Troceando-los-documentos-(Chunks)-2\">3. Troceando los documentos (Chunks)</a></span></li><li><span><a href=\"#4.-Preceso-de-Embedding\" data-toc-modified-id=\"4.-Preceso-de-Embedding-3\">4. Preceso de Embedding</a></span></li><li><span><a href=\"#5.-Guardado-de-embeddings-en-Chroma-DB\" data-toc-modified-id=\"5.-Guardado-de-embeddings-en-Chroma-DB-4\">5. Guardado de embeddings en Chroma DB</a></span></li><li><span><a href=\"#6.-Búsqueda-de-documentos-relevantes-en-Chroma\" data-toc-modified-id=\"6.-Búsqueda-de-documentos-relevantes-en-Chroma-5\">6. Búsqueda de documentos relevantes en Chroma</a></span></li><li><span><a href=\"#7.-Plantillas-de-instrucciones-(Prompt-Templates)\" data-toc-modified-id=\"7.-Plantillas-de-instrucciones-(Prompt-Templates)-6\">7. Plantillas de instrucciones (<a href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start/\" rel=\"nofollow\" target=\"_blank\">Prompt Templates</a>)</a></span></li><li><span><a href=\"#8.-Modelo-LLM\" data-toc-modified-id=\"8.-Modelo-LLM-7\">8. Modelo LLM</a></span></li><li><span><a href=\"#9.-Construcción-de-la-cadena\" data-toc-modified-id=\"9.-Construcción-de-la-cadena-8\">9. Construcción de la cadena</a></span></li><li><span><a href=\"#10.-Resumen-paso-a-paso\" data-toc-modified-id=\"10.-Resumen-paso-a-paso-9\">10. Resumen paso a paso</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c05b75",
   "metadata": {},
   "source": [
    "## 1. Definiciones y dependencias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903002c",
   "metadata": {},
   "source": [
    "**Dependencias necesarias para este workshop:**\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "pip install langchain-chroma\n",
    "pip install pypdf\n",
    "pip install transformers\n",
    "pip install sentence-transformers\n",
    "pip install transformers[sentencepiece]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f9aeb",
   "metadata": {},
   "source": [
    "El propósito de este workshop es crear un RAG (Retrieval Augmented Generation) con [LangChain](https://www.langchain.com/) y un modelo de código abierto desde [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8331a",
   "metadata": {},
   "source": [
    "**¿Qué es un LLM?**\n",
    "\n",
    "\n",
    "Un LLM, o \"Large Language Model\" (Gran Modelo de Lenguaje), es un tipo de modelo de inteligencia artificial diseñado para entender, generar y trabajar con lenguaje humano a gran escala. Estos modelos se entrenan utilizando grandes cantidades de datos de texto para aprender patrones, estructuras del lenguaje y relaciones contextuales.\n",
    "\n",
    "Características principales de los LLM:\n",
    "\n",
    "1. Capacidad de Generación de Texto: Los LLMs son capaces de generar texto coherente y contextualmente relevante que puede imitar el estilo y la estructura del lenguaje humano. Esto los hace útiles para tareas como la escritura automática de artículos, generación de respuestas en chatbots, etc...\n",
    "\n",
    "2. Comprensión del Contexto: Gracias a su entrenamiento con grandes cantidades de texto, los LLMs tienen una notable capacidad para entender el contexto de las consultas que reciben, lo que les permite ofrecer respuestas más precisas y relevantes.\n",
    "\n",
    "3. Aplicaciones Multilingües: Algunos LLMs están entrenados en múltiples idiomas, lo que les permite operar en diferentes lenguajes y realizar tareas como traducción automática o asistencia multilingüe.\n",
    "\n",
    "4. Aprendizaje Continuo: Aunque los LLMs se entrenan en un conjunto estático de datos, algunos modelos están diseñados para continuar aprendiendo a partir de nuevas interacciones, lo que mejora su rendimiento y adaptabilidad con el tiempo.\n",
    "\n",
    "5. Interpretación de Sentimiento y Semántica: Pueden analizar y entender sentimientos, opiniones y matices semánticos en el texto, lo que es crucial para aplicaciones como análisis de sentimiento, soporte al cliente y monitorización de redes sociales.\n",
    "\n",
    "Ejemplos de LLM:\n",
    "\n",
    "+ GPT (Generative Pre-trained Transformer) de OpenAI: Es uno de los modelos de lenguaje más conocidos y avanzados, usado ampliamente en aplicaciones comerciales y de investigación por su capacidad para generar texto altamente contextual y creativo.\n",
    "\n",
    "+ BERT (Bidirectional Encoder Representations from Transformers) de Google: Se utiliza principalmente para mejorar la comprensión del lenguaje en buscadores y para mejorar la precisión de las respuestas en aplicaciones de inteligencia artificial.\n",
    "\n",
    "\n",
    "Cuando se habla de los parámetros de un LLM nos estamos refiriendo a los valores internos que el modelo utiliza para realizar predicciones y generar texto basado en el lenguaje humano. Estos parámetros son esenciales para que el modelo funcione correctamente y son ajustados durante el proceso de entrenamiento. Los parámetros son en su mayoría pesos sinápticos. Estos pesos determinan la fuerza de la conexión entre las neuronas en diferentes capas del modelo. Durante el entrenamiento, estos pesos se ajustan para minimizar el error en la predicción del modelo. Junto con los pesos, los biases (sesgos) son otro tipo de parámetros que se añaden a las sumas ponderadas en las neuronas para ayudar al modelo a ajustarse mejor a los datos, son el equivalente a la ordenada en el origen. Los biases permiten que el modelo opere eficazmente cuando todas las entradas son cero.\n",
    "\n",
    "Los LLMs son conocidos por tener un número extremadamente alto de parámetros. Por ejemplo, GPT-3 de OpenAI tiene 175 mil millones de parámetros, lo que lo hace uno de los modelos de lenguaje más grandes y poderosos disponibles. Estos parámetros permiten al modelo capturar una gran cantidad de matices lingüísticos y contextuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c092514",
   "metadata": {},
   "source": [
    "**¿Qué es un RAG?**\n",
    "\n",
    "La generación mejorada por recuperación (Retrieval Augmented Generation - RAG) es el proceso de optimización de un modelo lingüístico de gran tamaño (Large Language Model - LLM), de modo que conozca datos proporcionados por el usuario, que no existan en los datos de entrenamiento del modelo, antes de generar una respuesta. \n",
    "\n",
    "Los LLM se entrenan con grandes volúmenes de datos y usan miles de millones de parámetros para generar resultados originales en tareas como responder preguntas, traducir idiomas y completar frases. Un RAG extiende las ya poderosas capacidades de los LLM a dominios específicos o a la base de conocimientos interna de una organización, todo ello sin la necesidad de volver a entrenar el modelo. Se trata de un método rentable para mejorar los resultados de los LLM de modo que sigan siendo relevantes, precisos y útiles en diversos contextos.\n",
    "\n",
    "Un RAG aporta varios beneficios directos en el desarrollo de una herramienta de inteligencia artificial:\n",
    "\n",
    "+ Rentabilidad de la implementación. El desarrollo de IAs comienza normalmente con un modelo básico. Los modelos fundacionales (Foundational Models - FM) son LLMs accesibles por API entrenados en un amplio espectro de datos generalizados y sin etiquetar. Los costos computacionales y financieros de volver a entrenar a los FM para obtener información específica de la organización o del dominio son muy elevados. Un RAG es un enfoque más rentable para introducir nuevos datos en el LLM.\n",
    "\n",
    "\n",
    "+ Información actualizada. Incluso si el LLM está entrenado con los datos adecuados para las necesidades de la compañía, es complicado mantener la relevancia del modelo. Un RAG permite a los desarrolladores proporcionar las últimas investigaciones, estadísticas o noticias a los modelos generativos. Se puede usar un RAG para conectar el LLM directamente a las redes sociales en vivo, sitios de noticias u otras fuentes de información que se actualizan con frecuencia. De esta manera, un LLM puede proporcionar la información más reciente.\n",
    "\n",
    "\n",
    "+ Confianza. Al darle al LLM datos propios, se conoce perfectamente la fuente de datos además de evitar la alucinación del LLM.\n",
    "\n",
    "\n",
    "+ Mayor control. El RAG permite a los desarrolladores de inteligencia artificial cambiar las fuentes de información para adaptarse a los requisitos cambiantes o a los usos múltiples de la compañía. Además pueden restringir la recuperación de información confidencial a diferentes niveles de autorización y garantizar que el LLM genere las respuestas adecuadas.\n",
    "\n",
    "\n",
    "El esquema básico de un RAG es como sigue:\n",
    "\n",
    "<br>\n",
    "\n",
    "![rag](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Los documentos de la compañía se pasan por un modelo de incrustación (Embedding Model) para ser guardados en forma de vectores en una base de datos.\n",
    "\n",
    "2. La consulta realizada por el usuario pasa por el mismo modelo para convertir dicho texto en vectores.\n",
    "\n",
    "3. Se buscan en la base de datos los vectores más parecidos a la consulta y se extraen los vectores más relevantes.\n",
    "\n",
    "4. Los documentos más relevantes extraídos y la consulta se introducen en el LLM para generar la respuesta más adecuada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930da92",
   "metadata": {},
   "source": [
    "![langchain](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg)\n",
    "\n",
    "**¿Qué es LangChain?**\n",
    "\n",
    "\n",
    "LangChain es un framework de código abierto para crear aplicaciones basadas en LLM. LangChain proporciona herramientas y abstracciones para mejorar la personalización, precisión y relevancia de la información que generan los modelos. Por ejemplo, los desarrolladores pueden usar los componentes de LangChain para crear nuevas cadenas de peticiones o personalizar las plantillas existentes. LangChain también incluye componentes que permiten a los LLM acceder a nuevos conjuntos de datos sin necesidad de repetir el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd545383",
   "metadata": {},
   "source": [
    "![huggingface](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/huggingface.png)\n",
    "\n",
    "**¿Qué es HuggingFace?**\n",
    "\n",
    "HuggingFace es una empresa de tecnología que se especializa en inteligencia artificial (IA), particularmente en el área de procesamiento del lenguaje natural (NLP). Es conocida por desarrollar y mantener la biblioteca \"Transformers\", que proporciona modelos de IA pre-entrenados y herramientas para facilitar la construcción de aplicaciones relacionadas con el lenguaje, como la traducción automática, el análisis de sentimientos y la generación de texto. La compañía también contribuye a la investigación en IA y fomenta una comunidad activa de desarrolladores y investigadores que colaboran en proyectos de código abierto. Además, HuggingFace opera una plataforma en línea donde los usuarios pueden experimentar con diferentes modelos de IA, compartir sus propios modelos y colaborar en proyectos de IA. Esta plataforma también facilita la implementación y el uso de modelos de IA en diversas aplicaciones prácticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a83abb",
   "metadata": {},
   "source": [
    "## 2. Carga de archivos PDF.\n",
    "\n",
    "\n",
    "![pdf](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/pdf.png)\n",
    "\n",
    "Formato de Documento Portátil (Portable Document Format - PDF), estandarizado como ISO 32000, es un formato de archivo desarrollado por Adobe en 1992 para presentar documentos, incluyendo el formato de texto e imágenes, de manera independiente del software de aplicación, hardware y sistemas operativos. El objetivo va a ser cargar archivos PDF para poder hacer preguntas a un modelo de lenguage (Large Language Model - LLM) al respecto de ellos. A esto se lo conoce como modelos de respuesta a preguntas (question answering models). Los modelos QA son sistemas de inteligencia artificial diseñados para responder preguntas formuladas en lenguaje natural. Estos modelos son una parte fundamental del procesamiento del lenguaje natural y se utilizan en una amplia gama de aplicaciones, desde asistentes virtuales hasta herramientas de búsqueda y análisis de información. El proceso de estos modelos es como sigue:\n",
    "\n",
    "1. Comprensión de texto: Estos modelos generalmente están entrenados para entender y procesar texto escrito en un lenguaje humano. Pueden analizar grandes volúmenes de texto y extraer información relevante.\n",
    "\n",
    "2. Búsqueda de información: Identifican partes del texto que son relevantes para la pregunta. Esto puede implicar la extracción directa de la respuesta de un texto específico o la inferencia de una respuesta basada en múltiples fragmentos de texto.\n",
    "\n",
    "3. Generación de respuestas: Una vez que se identifica la información relevante, el modelo genera una respuesta en lenguaje natural.\n",
    "\n",
    "Existen dos tipos de modelos QA:\n",
    "\n",
    "+ Basados en recuperación (retrieval-based): Estos modelos buscan en una base de datos predefinida o en un conjunto de documentos para encontrar la respuesta, generalmente devolviendo el fragmento de texto más relevante. Este será el tipo de modelo QA que vamos a utilizar.\n",
    "\n",
    "+ Basados en generación (generative-based): Estos modelos pueden generar respuestas nuevas que no están explícitamente presentes en los textos de entrada, basándose en el conocimiento adquirido durante su entrenamiento.\n",
    "\n",
    "\n",
    "Los datos los hemos descargado de [Indra](https://www.indracompany.com/es/accionistas/memoria-cuentas-anuales/), donde están reflejados los resultados de la compañía anualmente de manera pública, y los hemos guardado en la carpeta `pdfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27382330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # libreria del sistema operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de archivos en la carpeta pdfs\n",
    "\n",
    "os.listdir('pdfs')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38466366",
   "metadata": {},
   "source": [
    "### [Document Loaders de LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "\n",
    "Los \"document loaders\" de LangChain son componentes diseñados para cargar y procesar documentos dentro de la plataforma LangChain. \"Los document loaders\" en LangChain se utilizan para:\n",
    "\n",
    "1. Cargar documentos: Pueden cargar documentos desde diversas fuentes, como archivos locales, bases de datos o servicios en la nube.\n",
    "\n",
    "2. Procesar y preparar documentos: Facilitan la extracción y preparación de texto de los documentos para su posterior análisis y procesamiento por parte de modelos de lenguaje.\n",
    "\n",
    "3. Indexación: Ayudan en la indexación de los documentos para facilitar la búsqueda y recuperación rápida de información relevante, lo que es crucial para tareas como la respuesta a preguntas basada en documentos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efeafb",
   "metadata": {},
   "source": [
    "Usaremos el `PyPDFLoader` de LangChain, el cual nos permite cargar y extraer texto de archivos PDF para que puedan ser procesados por modelos de lenguaje o utilizados en aplicaciones de respuesta a preguntas. Nos permite extraer texto de documentos PDF, lo cual es esencial para procesar este tipo de documentos que son comunes en muchos contextos profesionales y académicos. Esto incluye la capacidad de manejar múltiples páginas y extraer el texto de manera eficiente. Además incluye funciones para preprocesar el texto extraído. Esto puede implicar la eliminación de encabezados y pies de página, corrección de errores de OCR, y normalización del texto para prepararlo para análisis posteriores. Puede configurarse para trabajar con diferentes rutas de documentos y ajustar su comportamiento según las necesidades del usuario, como especificar rangos de páginas o seleccionar modos de extracción de texto específicos. Para poder usar este \"loader\", es necesario instalar la librería `pypdf`, comentado en las depedencias del workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga de archivo PDF página a página\n",
    "\n",
    "loader = PyPDFLoader('pdfs/memoria_consolidada_2022.pdf')\n",
    "\n",
    "paginas = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93df585",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginas[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2837d",
   "metadata": {},
   "source": [
    "Existen más \"loaders\" de documentos PDF en LangChain. En particular existen dos que son capaces de reconocer la estructura tabular dentro del PDF. El primero es `UnstructuredPDFLoader`, basado en la librería [unstructured](https://github.com/Unstructured-IO/unstructured), sin embargo aún está en desarrollo y falla con frecuencia. El segundo es `AmazonTextractPDFLoader`, basado en el servicio de AWS Textract, un sistema intregado de Amazon para el reconocimiento de archivos de texto o PDFs y va más allá del simple reconocimiento óptico de caracteres (OCR). Para usarlo es necesario cuenta de desarrollador en AWS y el uso de `S3`, pudiendo conectarnos a través de la librería `boto3` de Python,\n",
    "\n",
    "Veíamos que teniamos varios documentos PDF en nuestra carpeta, uno correspondiente al año 2022 y otro al año 2023. Con `PyPDFLoader` podemos cargar toda la carpeta a la vez. Veamos como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84859a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga de todos los archivos PDF página a página\n",
    "\n",
    "loader = PyPDFDirectoryLoader('pdfs/')\n",
    "\n",
    "paginas = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8514b",
   "metadata": {},
   "source": [
    "## 3. Troceando los documentos (Chunks)\n",
    "\n",
    "Ahora tenemos 828 páginas de ambos documentos PDF. Para un uso óptimo del contexto del LLM que vayamos a usar, vamos a trocear (chunk) las páginas del PDF. Podríamos usar cualquiera de los [text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/) que tiene LangChain, dado que nuestros documentos PDF ahora están en formato texto. Pero el propio `PyPDFLoader` tiene un método para hacer esto directamente, en vez de usar el método `load()` se usa el método `load_and_split()`.\n",
    "\n",
    "![chunks](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chunks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# carga de todos los archivos PDF y realiza los chunks\n",
    "\n",
    "loader = PyPDFDirectoryLoader('pdfs/')\n",
    "\n",
    "documentos = loader.load_and_split()\n",
    "\n",
    "len(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e22178",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735508bb",
   "metadata": {},
   "source": [
    "## 4. Preceso de Embedding\n",
    "\n",
    "Ahora tenemos 1048 trozos (chunks) listos para el embedding. Los embeddings permiten que los modelos de aprendizaje automático trabajen con datos complejos, como texto, de una manera más efectiva al transformarlos en formatos que son más fáciles de analizar y procesar. Se trata de vectorizar el texto de tal manera que podamos extraer los textos más relevantes al respecto de nuestra pregunta. Usaremos los modelos de embedding desde HuggingFace, además LangChain ya tiene incorporado la conexión a dichos modelos.\n",
    "\n",
    "![embedding](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/embedding.png)\n",
    "\n",
    "\n",
    "Desde `sentence-transformers` usaremos el modelo [all-roberta-large-v1](https://huggingface.co/sentence-transformers/all-roberta-large-v1), el cual convierte frases y párrafos a vectores densos de 1024 elementos. Vector denso se refiere a que no existen elementos con valor 0, sino que todos los 1024 elementos son distintos de 0. Dichos vectores serán después usados para la búsqueda semántica dentro del RAG, es decir, los usaremos para extraer esos textos más relevantes respectos a la consulta que hagamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa0297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inicializamos el modelo de embedding con Roberta\n",
    "\n",
    "vectorizador = HuggingFaceEmbeddings(model_name='sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd379c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizamos una prueba para comprobar que funciona correctamente\n",
    "\n",
    "vector = vectorizador.embed_query(documentos[10].page_content)\n",
    "\n",
    "vector[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a45c0",
   "metadata": {},
   "source": [
    "## 5. Guardado de embeddings en Chroma DB\n",
    "\n",
    "![chroma](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chroma.jpeg)\n",
    "\n",
    "Chroma es una base de datos vectorial. Como tal, su objetivo es que seamos capaces de guardar vectores, los embeddings que creamos desde el texto, para después dotar de dicha información al LLM o simplemente para tenerlos guardados.\n",
    "\n",
    "A nivel general, la forma de usar Chroma es la siguiente:\n",
    "\n",
    "1. Crear nuestra colección, lo que es el equivalente a una tabla en una base de datos relacional. En este proceso, deberemos indicar qué modelo debe usar Chroma para convertir los textos en embeddings. En nuestro caso usaremos el modelo de embedding que hemos cargado anteriormente.\n",
    "\n",
    "2. Enviar a Chroma un texto que queramos que guarde, junto con los metadatos que queramos para el filtrado del texto. Cuando Chroma reciba el texto se encargará de convertirlo a embedding con el modelo que le damos.\n",
    "\n",
    "3. Consultar Chroma enviando un texto o un embedding, recibiremos los k documentos más parecidos, siendo k un parámetro de la consulta. Además, podremos filtrar la consulta en base a metadatos para que únicamente se ejecute sobre los documentos que cumplan una serie de criterios.\n",
    "\n",
    "\n",
    "Todo este proceso se realiza desde el `vectorstores` que tiene LangChain, simplemente dándole los documentos, el modelo de embedding y la ruta de guardado de la base de datos al objeto `Chroma` incorporado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90abd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3181933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# guardado en disco\n",
    "\n",
    "chroma_db = Chroma.from_documents(documentos,                    # documentos de texto\n",
    "                                  vectorizador,                  # modelo de embedding\n",
    "                                  persist_directory='save_db'    # ruta de guardado\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad28611",
   "metadata": {},
   "source": [
    "## 6. Búsqueda de documentos relevantes en Chroma\n",
    "\n",
    "Vamos a cargar la base de datos de Chroma desde los archivos locales y realizar una búsqueda para recuperar los documentos más relevantes según la consulta realizada. Iniciamos la carga de la base de datos con la ruta a los archivos que hemos guardado anteriormente y la función de embedding, el vectorizador que ya hemos definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005881ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga desde disco\n",
    "\n",
    "chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04286016",
   "metadata": {},
   "source": [
    "Podemos recuperar los documentos a través de una búsqueda por similitud con el método `similarity_search` de Chroma, pasándole como argumentos la consulta que hacemos y el número de documentos relevantes que queremos extraer de la base de datos para ser usados después como contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a83ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# búsqueda por similitud con el retorno de los 5 documentos más relevantes\n",
    "\n",
    "documentos = chroma_db.similarity_search('Derivados de activos no corrientes 2022', k=5)\n",
    "\n",
    "documentos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60223577",
   "metadata": {},
   "source": [
    "También podemos crear un objeto recuperador para luego usarlo en la cadena de LangChain. Por defecto, el tipo de búsqueda que realiza el recuperador, `search_type`, es por similitud y devuelve los más relevantes según ella. Podemos usar también la similitud con un umbral para recuperar los documentos que sobrepasen cierto nivel de similitud con nuestra consulta. El recuperador dispone además de un algoritmo llamado `MMR (maximal marginal relevance)`. El algoritmo de relevancia marginal máxima selecciona documentos basándose en una combinación de qué documentos son más similares a las consultas, al mismo tiempo que optimiza la diversidad. Lo hace encontrando los ejemplos con embeddings que tienen la mayor similitud coseno con las entradas, y luego los va añadiendo iterativamente mientras les aplica una penalización por cercanía a los ejemplos ya seleccionados.\n",
    "\n",
    "Usaremos el algoritmo MMR para que devuelva 20 documentos. El parámetro `lambda_mult` se refiere a la diversidad de los resultados devueltos por MMR siendo 1 para diversidad mínima y 0 para máxima. Por defecto es 0.5. Le pediremos un poco más de diversidad en su respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recuperador = chroma_db.as_retriever(search_type='mmr', \n",
    "                                     search_kwargs={'k': 20, 'lambda_mult': 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50559f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "recuperador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278db97d",
   "metadata": {},
   "source": [
    "## 7. Plantillas de instrucciones ([Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/))\n",
    "\n",
    "![prompts](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/prompts.png)\n",
    "\n",
    "Las plantillas de instrucciones son recetas predefinidas para generar instrucciones para modelos de lenguaje. Una plantilla puede incluir instrucciones, contexto y preguntas específicas apropiadas para una tarea dada. LangChain proporciona herramientas para crear y trabajar con plantillas de instrucciones y además se esfuerza por crear plantillas agnósticas al modelo para facilitar la reutilización de plantillas existentes en diferentes modelos de lenguaje. Esta clase permite a los desarrolladores estructurar y manipular cómo se formulan las solicitudes (o \"prompts\") al modelo de lenguaje. Esto facilita una interacción más efectiva y controlada con el modelo, optimizando la generación de texto para casos de uso específicos.\n",
    "\n",
    "Las funcionalidades pricipales son estas: \n",
    "\n",
    "1. Estructuración de Prompts: un template ayuda a organizar y estructurar los prompts que se envían a los modelos de lenguaje. Esto incluye la incorporación de datos específicos en una plantilla predefinida, lo que asegura que la información relevante se presente al modelo de manera coherente cada vez.\n",
    "\n",
    "2. Reutilización: permite la reutilización de patrones de prompt comunes en diferentes partes de una aplicación, asegurando consistencia y reduciendo la duplicidad de código.\n",
    "\n",
    "3. Personalización: se pueden definir múltiples plantillas para distintos tipos de tareas o estilos de interacción, lo que permite una personalización flexible según el contexto o las necesidades del usuario.\n",
    "\n",
    "4. Integración con Datos: un template se utiliza para integrar dinámicamente datos en tiempo real dentro de los prompts, lo que es crucial para aplicaciones que dependen de información actualizada o específica del contexto para generar respuestas adecuadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# plantilla de texto con un contexto y una pregunta\n",
    "plantilla = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \n",
    "            Don´t response with the prompt. Translate the answer to Spanish.\n",
    "            '''\n",
    "\n",
    "\n",
    "# carga de la plantilla en el prompt\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df60d0a",
   "metadata": {},
   "source": [
    "## 8. Modelo LLM \n",
    "\n",
    "Para usar los modelos de [HuggingFace](https://huggingface.co/models), vamos a necesitar el [token](https://huggingface.co/settings/tokens) que nos proporciona la plataforma. Por supuesto, necesitamos crearnos una cuenta en HuggingFace. El token lo guardamos en un archivo `.env` para cargarlo con la librería dotenv y usarlo como variable de entorno. Dicho archivo se añade al `.gitignore` para que nadie pueda verlo si subimos el código a GitHub por ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ef5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv      # carga variables de entorno \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# importamos el token\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc6863",
   "metadata": {},
   "source": [
    "![modelo](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/modelo.png)\n",
    "\n",
    "Vamos a usar el modelo [zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), una versión del modelo mistralai/Mistral-7B-v0.1 que fue entrenado con una mezcla de conjuntos de datos sintéticos y públicamente disponibles utilizando optimización de preferencias directas (Direct Preference Optimization - [DPO](https://arxiv.org/abs/2305.18290)). Zephyr es un modelo de generación de texto con aproximadamente 7 mil millones de parámetros similar en rendimiento a GPT-3.5-turbo. \n",
    "\n",
    "Cargaremos el modelo desde el hub de HuggingFace. El parámetro `max_new_tokens` especifica el número máximo de tokens nuevos que el modelo puede generar en una sola invocación de su función de predicción o generación de texto. Usaremos 512 tokens máximos. Cuando un modelo genera texto, selecciona el siguiente token basándose en una distribución de probabilidad de todos los tokens posibles. El parámetro `top_k` restringe esta selección a los tokens más probables, donde k es un número entero definido por el usuario. Por ejemplo, si top_k está configurado en 30, el modelo solo considerará los 30 tokens más probables como candidatos para ser el siguiente token en la secuencia. Esto limita la elección a un subconjunto de posibilidades más probable y puede ayudar a que el texto generado sea más coherente y menos propenso a respuestas aleatorias o incoherencias. El parámetro `temperature` en un modelo es una configuración que afecta la aleatoriedad de las respuestas generadas por el modelo. Este parámetro ayuda a controlar cómo de conservador o aventurero será el modelo al seleccionar palabras durante la generación de texto. Una temperatura baja, 0.5 o menos, hace que el modelo sea más determinista, tendiendo a seleccionar los tokens más probables. Esto resulta en respuestas más coherentes y predecibles, pero potencialmente menos creativas o variadas. Una temperatura alta, 1.0 o más, hace que el modelo sea menos determinista y más propenso a tomar riesgos, seleccionando tokens menos probables. Esto puede generar respuestas más diversas y creativas, pero también puede aumentar la probabilidad de errores o respuestas incoherentes. Usaremos una temperatura de 0.1. El parámetro `repetition_penalty` en un modelo se utiliza para desalentar la repetición de palabras o frases en la generación de texto. Este ajuste ayuda a aumentar la variedad y la coherencia en las respuestas generadas, haciéndolas más agradables y naturales. Si $repetitionpenalty > 1$, la probabilidad de tokens repetidos disminuye, lo que desincentiva la repetición. Cuanto mayor sea el valor, más fuerte será el efecto en desalentar la repetición. Si $repetitionpenalty < 1$, la probabilidad de tokens repetidos aumenta, lo que podría no ser deseable pero puede ser útil en contextos específicos donde la repetición es preferida. Usaremos 1.03 como repetition_penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec11c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(repo_id='HuggingFaceH4/zephyr-7b-beta',\n",
    "                     task='text-generation',\n",
    "                     huggingfacehub_api_token=HUGGINGFACE_TOKEN,\n",
    "                     \n",
    "                     model_kwargs={'max_new_tokens': 512,\n",
    "                                   'top_k': 30,\n",
    "                                   'temperature': 0.1,\n",
    "                                   'repetition_penalty': 1.03})\n",
    "\n",
    "\n",
    "modelo = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizamos una prueba del modelo para ver que funciona\n",
    "\n",
    "modelo.invoke('Capital de España')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e2076",
   "metadata": {},
   "source": [
    "## 9. Construcción de la cadena\n",
    "\n",
    "Una cadena de LangChain es una secuencia configurada de componentes o pasos que se encadenan para realizar tareas complejas de procesamiento del lenguaje. LangChain es una herramienta diseñada para facilitar la construcción y el despliegue de aplicaciones de lenguaje, integrando modelos de lenguaje avanzados y otras funcionalidades en flujos de trabajo coherentes y efectivos. El propósito de una cadena de LangChain es permitir que cada paso del proceso contribuya de manera efectiva al resultado final. Una cadena de LangChain típicamente incluirá varios componentes clave:\n",
    "\n",
    "+ Recuperadores (Retrievers): Componentes que buscan y recuperan información relevante de una base de datos o de un conjunto de documentos. Estos son esenciales para proporcionar contexto o datos específicos que el modelo de lenguaje necesita para generar respuestas informadas.\n",
    "\n",
    "+ Modelos de Lenguaje: El núcleo de una cadena de LangChain, donde se utilizan modelos avanzados como GPT-3 para generar texto, resolver preguntas, o realizar otras tareas de NLP basadas en la información y contextos proporcionados por otros componentes de la cadena.\n",
    "\n",
    "+ Post-procesadores: Componentes que toman la salida del modelo de lenguaje y la refinan, por ejemplo, corrigiendo errores, formateando la respuesta, o aplicando filtros adicionales de contenido.\n",
    "\n",
    "\n",
    "\n",
    "Vamos a construir la cadena de LangChain. El proceso será:\n",
    "\n",
    "1. Escribir la consulta.\n",
    "2. Recuperar los documentos relevantes de la base de datos para generar el contexto.\n",
    "3. Crear el prompt desde la plantilla con la consulta y el contexto.\n",
    "4. Introducir al modelo el prompt.\n",
    "5. Transformar la respuesta del modelo.\n",
    "\n",
    "\n",
    "![chain](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain.png)\n",
    "\n",
    "Vamos a construir primero una cadena simple con la recuperación por similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f689a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = '¿Cuál es el Balance de Situación Financiera Consolidado al 31 de diciembre de 2022?'\n",
    "\n",
    "\n",
    "# cadena con la plantilla de prompt y el modelo, con LangChain Expression Language (LCEL)\n",
    "cadena = prompt | modelo\n",
    "\n",
    "\n",
    "# invocamos a la cadena con el contexto traido de la base de datos y la consulta \n",
    "respuesta = cadena.invoke({'context': chroma_db.similarity_search(consulta), \n",
    "                           'question': consulta})\n",
    "\n",
    "\n",
    "# transformamos la respuesta del modelo \n",
    "respuesta.content.split('<|assistant|>')[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acaf9f",
   "metadata": {},
   "source": [
    "RunnablePassthrough permite pasar las entradas sin cambios. De esta manera le pasamos la consulta tanto al recuperador como al modelo. Al recuperador de la base de datos le vamos a pedir que nos devuelva los 5 documentos más relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdcc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = '¿Cuál es el Balance de Situación Financiera Consolidado al 31 de diciembre de 2022?'\n",
    "\n",
    "\n",
    "# recuperamos los 5 documentos más relevantes de la base de datos\n",
    "recuperador = chroma_db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'lambda_mult': 0.25})\n",
    "\n",
    "\n",
    "# cadena con el recuperador, la plantilla de prompt y el modelo\n",
    "cadena = {'context': recuperador, 'question': RunnablePassthrough()} | prompt | modelo\n",
    "\n",
    "\n",
    "# respuesta de la cadena\n",
    "respuesta = cadena.invoke(consulta)\n",
    "\n",
    "\n",
    "# transformamos la respuesta del modelo\n",
    "respuesta.content.split('<|assistant|>')[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c357e",
   "metadata": {},
   "source": [
    "Así, ya tenemos disponible la recuperación de datos desde los PDFs según la consulta realizada al modelo LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1a053",
   "metadata": {},
   "source": [
    "## 10. Resumen paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# librerias\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# importamos el token de huggingface\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN')\n",
    "\n",
    "\n",
    "\n",
    "# carga de todos los archivos PDF y realiza los chunks\n",
    "loader = PyPDFDirectoryLoader('pdfs/')\n",
    "documentos = loader.load_and_split()\n",
    "\n",
    "\n",
    "# modelo embedding\n",
    "vectorizador = HuggingFaceEmbeddings(model_name='sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "\n",
    "# guardado en disco, no sería necesario, debería de hacerse aparte\n",
    "chroma_db = Chroma.from_documents(documentos,                    # documentos de texto\n",
    "                                  vectorizador,                  # modelo de embedding\n",
    "                                  persist_directory=\"save_db\"    # ruta de guardado\n",
    "                                 )\n",
    "\n",
    "\n",
    "# carga desde disco\n",
    "chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)\n",
    "\n",
    "\n",
    "\n",
    "# plantilla de texto con un contexto y una pregunta\n",
    "plantilla = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \n",
    "            Don´t response with the prompt. Translate the answer to Spanish.\n",
    "            '''\n",
    "\n",
    "\n",
    "# carga de la plantilla en el prompt\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "\n",
    "\n",
    "# modelo de huggingface\n",
    "llm = HuggingFaceHub(repo_id='HuggingFaceH4/zephyr-7b-beta',\n",
    "                     task='text-generation',\n",
    "                     huggingfacehub_api_token=HUGGINGFACE_TOKEN,\n",
    "                     \n",
    "                     model_kwargs={'max_new_tokens': 512,\n",
    "                                   'top_k': 30,\n",
    "                                   'temperature': 0.1,\n",
    "                                   'repetition_penalty': 1.03})\n",
    "\n",
    "\n",
    "modelo = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "# consulta que hacemos\n",
    "consulta = '¿Cuál es el Balance de Situación Financiera Consolidado al 31 de diciembre de 2022?'\n",
    "\n",
    "\n",
    "# recuperamos los 5 documentos más relevantes de la base de datos\n",
    "recuperador = chroma_db.as_retriever(search_type='mmr', search_kwargs={'k': 5, 'lambda_mult': 0.25})\n",
    "\n",
    "\n",
    "# cadena con el recuperador, la plantilla de prompt y el modelo\n",
    "cadena = {'context': recuperador, 'question': RunnablePassthrough()} | prompt | modelo\n",
    "\n",
    "\n",
    "# respuesta de la cadena\n",
    "respuesta = cadena.invoke(consulta)\n",
    "\n",
    "\n",
    "# transformamos la respuesta del modelo\n",
    "respuesta.content.split('<|assistant|>')[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072c990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "33px",
    "top": "111.141px",
    "width": "194.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
