{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35836f0a",
   "metadata": {},
   "source": [
    "# 2 - YouTube RAG con GPT3.5 Turbo y LangChain\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag_2.webp\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04373a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Definiciones-y-dependencias.\" data-toc-modified-id=\"1.-Definiciones-y-dependencias.-0\">1. Definiciones y dependencias.</a></span></li><li><span><a href=\"#2.-Importamos-la-API-KEY-de-OpenAI.\" data-toc-modified-id=\"2.-Importamos-la-API-KEY-de-OpenAI.-1\">2. Importamos la API KEY de OpenAI.</a></span></li><li><span><a href=\"#3.-Llamada-a-GPT3.5-desde-LangChain\" data-toc-modified-id=\"3.-Llamada-a-GPT3.5-desde-LangChain-2\">3. Llamada a GPT3.5 desde LangChain</a></span><ul class=\"toc-item\"><li><span><a href=\"#Output-Parser\" data-toc-modified-id=\"Output-Parser-2.1\"><a href=\"https://python.langchain.com/docs/modules/model_io/output_parsers/\" rel=\"nofollow\" target=\"_blank\">Output Parser</a></a></span></li></ul></li><li><span><a href=\"#4.-Creación-de-la-cadena\" data-toc-modified-id=\"4.-Creación-de-la-cadena-3\">4. Creación de la cadena</a></span><ul class=\"toc-item\"><li><span><a href=\"#LangChain-Expression-Language-(LCEL)\" data-toc-modified-id=\"LangChain-Expression-Language-(LCEL)-3.1\"><a href=\"https://python.langchain.com/docs/expression_language/\" rel=\"nofollow\" target=\"_blank\">LangChain Expression Language (LCEL)</a></a></span></li></ul></li><li><span><a href=\"#5.-Plantillas-de-instrucciones-(Prompt-Templates)\" data-toc-modified-id=\"5.-Plantillas-de-instrucciones-(Prompt-Templates)-4\">5. Plantillas de instrucciones (<a href=\"https://python.langchain.com/docs/modules/model_io/prompts/quick_start/\" rel=\"nofollow\" target=\"_blank\">Prompt Templates</a>)</a></span></li><li><span><a href=\"#6.-Combinando-cadenas\" data-toc-modified-id=\"6.-Combinando-cadenas-5\">6. Combinando cadenas</a></span></li><li><span><a href=\"#7.-Extracción-de-texto-desde-YouTube-con-Whisper\" data-toc-modified-id=\"7.-Extracción-de-texto-desde-YouTube-con-Whisper-6\">7. Extracción de texto desde YouTube con Whisper</a></span></li><li><span><a href=\"#8.-Usando-la-transcripción-como-contexto\" data-toc-modified-id=\"8.-Usando-la-transcripción-como-contexto-7\">8. Usando la transcripción como contexto</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividiendo-en-segmentos-el-texto-completo\" data-toc-modified-id=\"Dividiendo-en-segmentos-el-texto-completo-7.1\">Dividiendo en segmentos el texto completo</a></span></li><li><span><a href=\"#Encontrando-los-trozos-relevantes\" data-toc-modified-id=\"Encontrando-los-trozos-relevantes-7.2\">Encontrando los trozos relevantes</a></span></li></ul></li><li><span><a href=\"#9.-Bases-de-datos-vectoriales\" data-toc-modified-id=\"9.-Bases-de-datos-vectoriales-8\">9. Bases de datos vectoriales</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conectando-la-base-de-datos-a-la-cadena\" data-toc-modified-id=\"Conectando-la-base-de-datos-a-la-cadena-8.1\">Conectando la base de datos a la cadena</a></span></li><li><span><a href=\"#Chroma\" data-toc-modified-id=\"Chroma-8.2\">Chroma</a></span></li></ul></li><li><span><a href=\"#10.-Resumen-paso-a-paso\" data-toc-modified-id=\"10.-Resumen-paso-a-paso-9\">10. Resumen paso a paso</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24422a41",
   "metadata": {},
   "source": [
    "## 1. Definiciones y dependencias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8a9c2",
   "metadata": {},
   "source": [
    "**Dependencias necesarias para este workshop:**\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "pip install langchain-openai\n",
    "pip install \"langchain[docarray]\"\n",
    "pip install langchain-chroma\n",
    "pip install docarray\n",
    "pip install pydantic==1.10.9\n",
    "pip install openai-whisper\n",
    "pip install pytube\n",
    "pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d2171",
   "metadata": {},
   "source": [
    "El propósito de este workshop es crear un RAG (Retrieval Augmented Generation) con [LangChain](https://www.langchain.com/) y el modelo GPT3.5 Turbo de [OpenAI](https://openai.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b949527",
   "metadata": {},
   "source": [
    "**¿Qué es un LLM?**\n",
    "\n",
    "\n",
    "Un LLM, o \"Large Language Model\" (Gran Modelo de Lenguaje), es un tipo de modelo de inteligencia artificial diseñado para entender, generar y trabajar con lenguaje humano a gran escala. Estos modelos se entrenan utilizando grandes cantidades de datos de texto para aprender patrones, estructuras del lenguaje y relaciones contextuales.\n",
    "\n",
    "Características principales de los LLM:\n",
    "\n",
    "1. Capacidad de Generación de Texto: Los LLMs son capaces de generar texto coherente y contextualmente relevante que puede imitar el estilo y la estructura del lenguaje humano. Esto los hace útiles para tareas como la escritura automática de artículos, generación de respuestas en chatbots, etc...\n",
    "\n",
    "2. Comprensión del Contexto: Gracias a su entrenamiento con grandes cantidades de texto, los LLMs tienen una notable capacidad para entender el contexto de las consultas que reciben, lo que les permite ofrecer respuestas más precisas y relevantes.\n",
    "\n",
    "3. Aplicaciones Multilingües: Algunos LLMs están entrenados en múltiples idiomas, lo que les permite operar en diferentes lenguajes y realizar tareas como traducción automática o asistencia multilingüe.\n",
    "\n",
    "4. Aprendizaje Continuo: Aunque los LLMs se entrenan en un conjunto estático de datos, algunos modelos están diseñados para continuar aprendiendo a partir de nuevas interacciones, lo que mejora su rendimiento y adaptabilidad con el tiempo.\n",
    "\n",
    "5. Interpretación de Sentimiento y Semántica: Pueden analizar y entender sentimientos, opiniones y matices semánticos en el texto, lo que es crucial para aplicaciones como análisis de sentimiento, soporte al cliente y monitorización de redes sociales.\n",
    "\n",
    "Ejemplos de LLM:\n",
    "\n",
    "+ GPT (Generative Pre-trained Transformer) de OpenAI: Es uno de los modelos de lenguaje más conocidos y avanzados, usado ampliamente en aplicaciones comerciales y de investigación por su capacidad para generar texto altamente contextual y creativo.\n",
    "\n",
    "+ BERT (Bidirectional Encoder Representations from Transformers) de Google: Se utiliza principalmente para mejorar la comprensión del lenguaje en buscadores y para mejorar la precisión de las respuestas en aplicaciones de inteligencia artificial.\n",
    "\n",
    "\n",
    "Cuando se habla de los parámetros de un LLM nos estamos refiriendo a los valores internos que el modelo utiliza para realizar predicciones y generar texto basado en el lenguaje humano. Estos parámetros son esenciales para que el modelo funcione correctamente y son ajustados durante el proceso de entrenamiento. Los parámetros son en su mayoría pesos sinápticos. Estos pesos determinan la fuerza de la conexión entre las neuronas en diferentes capas del modelo. Durante el entrenamiento, estos pesos se ajustan para minimizar el error en la predicción del modelo. Junto con los pesos, los biases (sesgos) son otro tipo de parámetros que se añaden a las sumas ponderadas en las neuronas para ayudar al modelo a ajustarse mejor a los datos, son el equivalente a la ordenada en el origen. Los biases permiten que el modelo opere eficazmente cuando todas las entradas son cero.\n",
    "\n",
    "Los LLMs son conocidos por tener un número extremadamente alto de parámetros. Por ejemplo, GPT-3 de OpenAI tiene 175 mil millones de parámetros, lo que lo hace uno de los modelos de lenguaje más grandes y poderosos disponibles. Estos parámetros permiten al modelo capturar una gran cantidad de matices lingüísticos y contextuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d17c2",
   "metadata": {},
   "source": [
    "**¿Qué es un RAG?**\n",
    "\n",
    "La generación mejorada por recuperación (Retrieval Augmented Generation - RAG) es el proceso de optimización de un modelo lingüístico de gran tamaño (Large Language Model - LLM), de modo que conozca datos proporcionados por el usuario, que no existan en los datos de entrenamiento del modelo, antes de generar una respuesta. \n",
    "\n",
    "Los LLM se entrenan con grandes volúmenes de datos y usan miles de millones de parámetros para generar resultados originales en tareas como responder preguntas, traducir idiomas y completar frases. Un RAG extiende las ya poderosas capacidades de los LLM a dominios específicos o a la base de conocimientos interna de una organización, todo ello sin la necesidad de volver a entrenar el modelo. Se trata de un método rentable para mejorar los resultados de los LLM de modo que sigan siendo relevantes, precisos y útiles en diversos contextos.\n",
    "\n",
    "Un RAG aporta varios beneficios directos en el desarrollo de una herramienta de inteligencia artificial:\n",
    "\n",
    "+ Rentabilidad de la implementación. El desarrollo de IAs comienza normalmente con un modelo básico. Los modelos fundacionales (Foundational Models - FM) son LLMs accesibles por API entrenados en un amplio espectro de datos generalizados y sin etiquetar. Los costos computacionales y financieros de volver a entrenar a los FM para obtener información específica de la organización o del dominio son muy elevados. Un RAG es un enfoque más rentable para introducir nuevos datos en el LLM.\n",
    "\n",
    "\n",
    "+ Información actualizada. Incluso si el LLM está entrenado con los datos adecuados para las necesidades de la compañía, es complicado mantener la relevancia del modelo. Un RAG permite a los desarrolladores proporcionar las últimas investigaciones, estadísticas o noticias a los modelos generativos. Se puede usar un RAG para conectar el LLM directamente a las redes sociales en vivo, sitios de noticias u otras fuentes de información que se actualizan con frecuencia. De esta manera, un LLM puede proporcionar la información más reciente.\n",
    "\n",
    "\n",
    "+ Confianza. Al darle al LLM datos propios, se conoce perfectamente la fuente de datos además de evitar la alucinación del LLM.\n",
    "\n",
    "\n",
    "+ Mayor control. El RAG permite a los desarrolladores de inteligencia artificial cambiar las fuentes de información para adaptarse a los requisitos cambiantes o a los usos múltiples de la compañía. Además pueden restringir la recuperación de información confidencial a diferentes niveles de autorización y garantizar que el LLM genere las respuestas adecuadas.\n",
    "\n",
    "\n",
    "El esquema básico de un RAG es como sigue:\n",
    "\n",
    "<br>\n",
    "\n",
    "![rag](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/rag.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Los documentos de la compañía se pasan por un modelo de incrustación (Embedding Model) para ser guardados en forma de vectores en una base de datos.\n",
    "\n",
    "2. La consulta realizada por el usuario pasa por el mismo modelo para convertir dicho texto en vectores.\n",
    "\n",
    "3. Se buscan en la base de datos los vectores más parecidos a la consulta y se extraen los vectores más relevantes.\n",
    "\n",
    "4. Los documentos más relevantes extraídos y la consulta se introducen en el LLM para generar la respuesta más adecuada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bc874",
   "metadata": {},
   "source": [
    "![langchain](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg)\n",
    "\n",
    "**¿Qué es LangChain?**\n",
    "\n",
    "\n",
    "LangChain es un framework de código abierto para crear aplicaciones basadas en LLM. LangChain proporciona herramientas y abstracciones para mejorar la personalización, precisión y relevancia de la información que generan los modelos. Por ejemplo, los desarrolladores pueden usar los componentes de LangChain para crear nuevas cadenas de peticiones o personalizar las plantillas existentes. LangChain también incluye componentes que permiten a los LLM acceder a nuevos conjuntos de datos sin necesidad de repetir el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ef96c",
   "metadata": {},
   "source": [
    "![openai](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/openai.png)\n",
    "\n",
    "**¿Qué es OpenAI?**\n",
    "\n",
    "OpenAI es una organización dedicada a la investigación y aplicación de inteligencia artificial. Se centra en diversas áreas como modelos generativos, seguridad, robótica y procesamiento de lenguaje natural. Además, OpenAI proporciona una plataforma de API que permite a los usuarios crear y utilizar sus propios modelos de IA personalizados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f769bd",
   "metadata": {},
   "source": [
    "## 2. Importamos la API KEY de OpenAI. \n",
    "\n",
    "Para realizar este proyecto necesitaremos una API KEY de OpenAI para poder usar el modelo GPT3.5 Turbo. Dicha API KEY se obtiene en https://platform.openai.com/api-keys. Solamente se muestra una vez, por lo que hay que guardarla en el mismo momento en que se obtiene. Por supuesto, tendremos que crear una cuenta para obtenerla.\n",
    "\n",
    "La API KEY la guardamos en un archivo `.env` para cargarla con la librería dotenv y usarla como variable de entorno. Dicho archivo se añade al `.gitignore` para que nadie pueda verla si subimos el código a GitHub por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83b7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la API KEY\n",
    "\n",
    "import os                           # libreria del sistema operativo\n",
    "from dotenv import load_dotenv      # carga variables de entorno \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce409b23",
   "metadata": {},
   "source": [
    "## 3. Llamada a GPT3.5 desde LangChain\n",
    "\n",
    "A continuación, importamos el LLM de OpenAI desde LangChain. Aquí es donde usaremos nuestra API KEY para cargar el modelo GPT3.5 Turbo. Vamos a preguntarle quién ganó la liga española en 2020, por ejemplo, para comprobar que efectivamente funciona y que estamos correctamente conectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55bff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   # conexión de LangChain a los modelos de OpenAI\n",
    "\n",
    "modelo = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b796cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocamos al modelo para que genere la respuesta\n",
    "\n",
    "respuesta = modelo.invoke('¿Quién ganó la liga española en 2020?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0643bb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='El Real Madrid ganó la liga española en 2020.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 22, 'total_tokens': 37, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-75f073ab-d1c2-4b83-bf56-693429627050-0', usage_metadata={'input_tokens': 22, 'output_tokens': 15, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# respuesta del modelo\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4090b130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipo de dato de la respuesta\n",
    "\n",
    "type(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c25a56",
   "metadata": {},
   "source": [
    "Como veis, la respuesta del modelo es un objeto de LangChain AIMessage. Si queremos extraer la string de la respuesta podemos hacerlo directamente con el atributo `content` de la respuesta. Probémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a637bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El Real Madrid ganó la liga española en 2020.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bcdcae",
   "metadata": {},
   "source": [
    "### [Output Parser](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "\n",
    "El propio LangChain tiene un parser para la salida de string, `StrOutputParser`. Parse se refiere a cambiar el tipo de formato, básicamente es traducir. LangChain tiene muchos parsers incorporados, desde strings hasta csv pasando por JSON o incluso Pandas DataFrames. \n",
    "\n",
    "Desde el parser `StrOutputParser` de LangChain el modelo devuelve directamente la respuesta en formato string. Este parser lo usaremos a la hora de crear la cadena de LangChain, será un eslabón más de la misma y así obtendremos directamente la respuesta del LLM en formato string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63dbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca93a0",
   "metadata": {},
   "source": [
    "## 4. Creación de la cadena\n",
    "\n",
    "Una cadena de LangChain es una secuencia configurada de componentes o pasos que se encadenan para realizar tareas complejas de procesamiento del lenguaje. LangChain es una herramienta diseñada para facilitar la construcción y el despliegue de aplicaciones de lenguaje, integrando modelos de lenguaje avanzados y otras funcionalidades en flujos de trabajo coherentes y efectivos. El propósito de una cadena de LangChain es permitir que cada paso del proceso contribuya de manera efectiva al resultado final. Una cadena de LangChain típicamente incluirá varios componentes clave:\n",
    "\n",
    "+ Recuperadores (Retrievers): Componentes que buscan y recuperan información relevante de una base de datos o de un conjunto de documentos. Estos son esenciales para proporcionar contexto o datos específicos que el modelo de lenguaje necesita para generar respuestas informadas.\n",
    "\n",
    "+ Modelos de Lenguaje: El núcleo de una cadena de LangChain, donde se utilizan modelos avanzados como GPT-3 para generar texto, resolver preguntas, o realizar otras tareas de NLP basadas en la información y contextos proporcionados por otros componentes de la cadena.\n",
    "\n",
    "+ Post-procesadores: Componentes que toman la salida del modelo de lenguaje y la refinan, por ejemplo, corrigiendo errores, formateando la respuesta, o aplicando filtros adicionales de contenido.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ahora crearemos la cadena que tiene básicamente dos eslabones: el modelo y el parser a string. El flujo del proceso sería:\n",
    "\n",
    "1. Generamos una consulta con una instrucción en texto (prompt).\n",
    "2. Le pasamos la consulta al modelo que hemos inicializado.\n",
    "3. El modelo devuelve una respuesta.\n",
    "4. La respuesta del modelo entra en el parser.\n",
    "5. El parser devuelve la respuesta en formato string.\n",
    "\n",
    "![chain1](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8b1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la cadena\n",
    "\n",
    "cadena = modelo | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff80a50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El equipo que ganó la liga española en 2020 fue el Real Madrid.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invocamos a la cadena dándole la consulta para que realice todo el flujo\n",
    "\n",
    "cadena.invoke('¿Quién ganó la liga española en 2020?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7b1f6",
   "metadata": {},
   "source": [
    "### [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)\n",
    "\n",
    "Fijaros en esta expresión:\n",
    "\n",
    "```python\n",
    "chain = model | parser\n",
    "```\n",
    "\n",
    "Esta expresión no es un [bitwise operator](https://realpython.com/python-bitwise-operators/) de Python, sino que usa para crear la cadena dentro de LangChain y concatenar las funciones dentro del diccionario del agente. Funcionaría como un grafo directo, haciendo que se ejecuten de una en una las tareas de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64182ffd",
   "metadata": {},
   "source": [
    "## 5. Plantillas de instrucciones ([Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/))\n",
    "\n",
    "Las plantillas de instrucciones son recetas predefinidas para generar instrucciones para modelos de lenguaje.\n",
    "\n",
    "Una plantilla puede incluir instrucciones, contexto y preguntas específicas apropiadas para una tarea dada. LangChain proporciona herramientas para crear y trabajar con plantillas de instrucciones y además se esfuerza por crear plantillas agnósticas al modelo para facilitar la reutilización de plantillas existentes en diferentes modelos de lenguaje.\n",
    "\n",
    "![chain2](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain2.png)\n",
    "\n",
    "Típicamente, los modelos de lenguaje esperan que la instrucción sea una cadena de texto o una lista de mensajes de chat. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ded55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6be4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human: ',\n",
       " \"            Answer the question based on the context below. If you can't \",\n",
       " '            answer the question, reply \"I don\\'t know\".',\n",
       " '',\n",
       " '            Context: The building where Pepe lives is green.',\n",
       " '',\n",
       " \"            Question: What color is Pepe's house?\",\n",
       " '            ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt3.5 funciona mejor en inglés, hagamos un ejemplo en esa lengua\n",
    "\n",
    "\n",
    "# plantilla de texto con un contexto y una pregunta\n",
    "plantilla = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            '''\n",
    "\n",
    "# carga de la plantilla en el prompt\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "\n",
    "# formato de salida del prompt\n",
    "prompt.format(context='The building where Pepe lives is green.', \n",
    "              question=\"What color is Pepe's house?\").split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b27cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la cadena añadiendo la plantilla del prompt en el primer paso\n",
    "\n",
    "cadena = prompt | modelo | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4a5943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The color of Pepe's house is green.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invocamos a la cadena\n",
    "\n",
    "cadena.invoke({'context': 'The building where Pepe lives is green.',\n",
    "               'question': \"What color is Pepe's house?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50fbd6",
   "metadata": {},
   "source": [
    "Fijaros que ahora en la invocación de la cadena no le pasamos una string, sino que le damos un diccionario que contiene tanto el contexto como la pregunta. Esto es necesario, puesto que el primer paso de la cadena es la creación del prompt y necesita de ambos términos para su creación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3747fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'De color blanco.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probemos directamente la cadena en castellano, a ver que ocurre\n",
    "\n",
    "cadena.invoke({'context': 'Tengo un coche blanco',\n",
    "               'question': '¿De qué color es mi coche?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b02e7",
   "metadata": {},
   "source": [
    "## 6. Combinando cadenas\n",
    "\n",
    "\n",
    "Acabamos de ver que el modelo de lenguaje funciona también en castellano. Pero podemos crear cadenas para flujos de trabajo más complejos, como por ejemplo, traducir lo que hace la primera cadena a otro lenguaje. \n",
    "\n",
    "![chain3](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain3.png)\n",
    "\n",
    "Usaremos otra plantilla de instrucciones para combinarla con la cadena que ya hemos construido, de tal manera que traduzca al castellano la respuesta del modelo. Veámoslo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc44189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la plantilla de traducción con la respuesta de la cadena y el lenguaje de salida\n",
    "\n",
    "prompt_traductor = ChatPromptTemplate.from_template('Translate {answer} to {language}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b223fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la nueva cadena basada en la anterior a la cual le damos el lenguaje al que queremos traducir\n",
    "\n",
    "cadena_traducida = (\n",
    "    {'answer': cadena, 'language': lambda x: x['language']} | prompt_traductor | modelo | parser \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53689d",
   "metadata": {},
   "source": [
    "Fijaros que el primer paso de la cadena traducida es un diccionario con la respuesta de la cadena original por un lado y el lenguaje por el otro. La razón de la función `lambda` es que cuando invoquemos a esta cadena compuesta le pasaremos también un diccionario cuyos valores son ejecutables, como hicimos antes, con el contexto, la pregunta y el lenguaje al que queremos traducir la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86f8b596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La casa de Pepe es verde.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invocamos a la nueva cadena, dándole el contexto, la pregunta y el lenguaje de salida\n",
    "\n",
    "cadena_traducida.invoke({'context': 'The building where Pepe lives is green.',\n",
    "                         'question': \"What color is Pepe's house?\",\n",
    "                         'language': 'Spanish',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5288d72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vert'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traducimos a otro idioma para probar la cadena\n",
    "\n",
    "cadena_traducida.invoke({'context': 'The building where Pepe lives is green.',\n",
    "                         'question': \"What color is Pepe's house?\",\n",
    "                         'language': 'French',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e0008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pepe's Haus ist grün.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otro ejemplo\n",
    "\n",
    "cadena_traducida.invoke({'context': 'The building where Pepe lives is green.',\n",
    "                         'question': \"What color is Pepe's house?\",\n",
    "                         'language': 'German',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23af0f6",
   "metadata": {},
   "source": [
    "Hasta aquí, tenemos una cadena a la cual le damos un contexto, una pregunta y un lenguaje y nos devuelve una respuesta contextualizada en el idioma seleccionado. Nos hemos propuesto transcribir un video de YouTube para que la cadena lo use como contexto y podamos realizar consultas a dicha transcripción. Para ello lo primero será extraer el texto desde un video. Vamos a ello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590d36a",
   "metadata": {},
   "source": [
    "## 7. Extracción de texto desde YouTube con Whisper\n",
    "\n",
    "Primero tenemos que importar las librerías que vamos a usar. Usaremos [pytube](https://pytube.io/en/latest/), para acceder al video y luego [whisper](https://openai.com/research/whisper), el modelo de speech-to-text de OpenAI. También tendremos que instalar [ffmpeg](https://ffmpeg.org/) según nuestro sistema operativo para que nuestra máquina sea capaz de analizar el audio que viene desde el video de YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e388fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1f054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la url del video: Planned Chaos - by Ludwig von Mises - (Full Audiobook) (3:09:32)\n",
    "\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=7EnHeZXLzTc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "870719c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos pytube para extraer el video\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8b3c0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ahora extraemos el audio desde el video\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddceadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se carga el modelo base de Whisper en local, 139M\n",
    "\n",
    "modelo_whisper = whisper.load_model('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb39e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descripción del modelo whisper\n",
    "\n",
    "modelo_whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e11814",
   "metadata": {},
   "source": [
    "**¿Cómo funciona whisper?**\n",
    "\n",
    "Como veis, Whisper está básicamente dividido en dos partes: un AudioEncoder y un TextDecoder. Esto es lo que se conoce como AutoEncoder. El AudioEncoder reconoce el audio y lo vectoriza y desde esos vectores el TextDecoder saca el texto. Por daros un paso a paso sería algo así:\n",
    " \n",
    "\n",
    "1. Preprocesamiento del Audio: Whisper comienza con el preprocesamiento del audio de entrada, donde se ajusta la calidad del audio para mejorar la precisión del reconocimiento. Esto puede incluir normalizar el volumen, filtrar ruido de fondo y segmentar el audio en trozos más manejables.\n",
    "\n",
    "2. Extracción de Características: El modelo extrae características del audio procesado. Esto implica convertir las señales de audio en una forma que el modelo pueda entender, como espectrogramas o características de Mel-frequency cepstral coefficients (MFCC), que son representaciones de la frecuencia y amplitud del sonido a lo largo del tiempo.\n",
    "\n",
    "3. Modelo de Aprendizaje: Whisper utiliza un modelo de red neuronal, entrenado en una gran cantidad de datos de audio y transcripciones correspondientes. Este modelo aprende a identificar patrones y correlaciones entre el audio y las transcripciones textuales.\n",
    "\n",
    "4. Reconocimiento y Traducción: Durante la fase de reconocimiento, Whisper convierte el audio en texto utilizando su red neuronal. Además, puede traducir el texto reconocido a otros idiomas, gracias a su entrenamiento en múltiples idiomas.\n",
    "\n",
    "5. Post-procesamiento: Finalmente, el texto generado pasa por un proceso de post-procesamiento para corregir errores comunes, ajustar la puntuación y mejorar la coherencia del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc59b9",
   "metadata": {},
   "source": [
    "Usaremos ahora Whisper para transcribir el audio a un archivo de texto. Primero crearemos un directorio temporal donde se guardará el audio y luego guardaremos la salida de Whisper en el archivo `transcripcion.txt`. Si ya existe el archivo, no nos interesa convertirlo otra vez, puesto que Whisper tarda unos minutos en trancribir todo el audio, asi que solo ejecutamos el modelo si el archivo de texto no existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0da14c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile    # para manejo de archivos temporales\n",
    "\n",
    "\n",
    "# ruta de guardado del archivo de texto\n",
    "\n",
    "RUTA_TXT = 'txt/transcripcion.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2004ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90 µs, sys: 64 µs, total: 154 µs\n",
      "Wall time: 131 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# si el archivo de texto no existe...\n",
    "if not os.path.exists(RUTA_TXT):\n",
    "    \n",
    "    # abrimos el directorio temporal...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # descargamos el audio de YouTube...\n",
    "        archivo_audio = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # y Whisper transcribe el audio a texto, en 32 bits (fp16=False)\n",
    "        transcripcion = modelo_whisper.transcribe(archivo_audio, fp16=False)['text'].strip()\n",
    "        \n",
    "        \n",
    "        # se guarda el archivo de texto\n",
    "        with open(RUTA_TXT, 'w') as archivo_texto:\n",
    "            archivo_texto.write(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9186507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el archivo de texto para usarlo en la cadena\n",
    "\n",
    "with open(RUTA_TXT, 'r') as archivo_texto:\n",
    "    \n",
    "    transcripcion = archivo_texto.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "524c7025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Planned Chaos by Ludwig von Meises Introductory remarks The characteristic mark of this age of dictators, wars and revolutions is its anti-capitalistic bias. Most governments and political parties are eager to restrict the sphere of private initiative and free enterprise. It is an almost unchallenged dogma that capitalism is done for, and that the coming of all-round regimentation of economic activities is both inescapable and highly desirable. Nonetheless, capitalism is still very vigorous in the Western Hemisphere. Capitalist production has made very remarkable progress even in these last years. Methods of production were greatly improved. Consumers have been supplied with better and cheaper goods and with many new articles unheard of a short time ago. Many countries have expanded the size and improved the quality of their manufacturing. In spite of the anti-capitalistic policies of all governments and of almost all political parties, the capitalist mode of production is in many coun'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1000 primeros caracteres del texto\n",
    "    \n",
    "transcripcion[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "966944f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159723"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nº de caracteres del texto\n",
    "\n",
    "len(transcripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "044c77ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25880"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nº de palabras del texto\n",
    "\n",
    "len(transcripcion.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7665230b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n of a proletarian has contributed to the elaboration of the interventionist and socialist programmes. Their authors were all of bourgeois background. The esoteric writings of dialectical materialism, of Hegel, the father, both of Marxism and of German aggressive nationalism, the books of George's Sorrel, of Gentile and Spengler, are not read by the average man. They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1000 últimos caracteres\n",
    "\n",
    "transcripcion[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334470d5",
   "metadata": {},
   "source": [
    "## 8. Usando la transcripción como contexto\n",
    "\n",
    "La idea fundamental del RAG es que el LLM tenga nuestra transcripción como contexto a la hora de generar respuestas, es decir, que conozca el texto para poder respondernos las preguntas que tengamos al respecto del mismo. Vamos a ver que ocurre cuando lo hacemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6594c256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 30815 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cadena.invoke({\n",
    "        'context': transcripcion,\n",
    "        'question': 'What are the key points of this book? Put them in a bullet point list.'})\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e2865",
   "metadata": {},
   "source": [
    "Si nos fijamos en el mensaje del error, nos está diciendo que la longitud máxima del contexto de GPT3.5 es de 16385 tokens pero nuestro contexto tiene 30815 tokens. Un token no es un caracter ni una palabra, recordad que nuestro texto tiene 159723 y 25880 respectivamente. Un token es una unidad de texto que el modelo utiliza para procesar y generar lenguaje. Los tokens son las piezas fundamentales de información con las que el modelo trabaja, y pueden variar en gran medida dependiendo de cómo está diseñado el modelo y cómo se ha entrenado. Un token puede ser una palabra completa, una parte de una palabra (como prefijos, raíces o sufijos), o incluso puntuación y espacios. La forma en que se dividen los textos en tokens depende del \"tokenizador\" utilizado por el modelo. La tokenización es uno de los procesos esenciales para el funcionamiento de los LLM, actuando como los bloques constructivos básicos para todas las tareas de procesamiento de lenguaje que realiza el modelo.\n",
    "\n",
    "El problema aquí es que el texto es demasiado grande, de hecho el mensaje de error nos recomienda reducir la longitud del texto. Pero existe otra solución mejor a este problema que reducir el contenido del texto. Además, nosotros queremos que toda la transcripción sea el contexto que el LLM utilice para darnos una respuesta. Veamos esa solución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b82917",
   "metadata": {},
   "source": [
    "### Dividiendo en segmentos el texto completo\n",
    "\n",
    "![chain4](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain4.png)\n",
    "\n",
    "Dado que el texto completo es demasiado grande para ser usado como contexto para el LLM, la solución obvia es dividir el texto en varios trozos para darle al modelo textos más pequeños como contexto. Para dividir el texto, primero usaremos el TextLoader de LangChain para cargar toda la transcripción del video como un objeto Document de LangChain. Un Document es un fragmento de texto con metadatos asociados. \n",
    "\n",
    "El cargador de texto proporciona un método \"load\" para cargar datos como Document desde una fuente como un txt o un md, por ejemplo. Opcionalmente, implementan también una \"lazy load\" para cargar los datos en la memoria. \"Lazy\" se refiere a una estrategia de carga o inicialización que pospone la ejecución de una operación hasta que es estrictamente necesaria. Esta técnica es comúnmente usada para mejorar el rendimiento y optimizar el uso de los recursos del sistema, especialmente la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5110bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(RUTA_TXT)\n",
    "\n",
    "documento_texto = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9312bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documento_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddea0e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documento_texto[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fab5e",
   "metadata": {},
   "source": [
    "Una vez cargado el texto completo como un objeto de LangChain, lo dividimos con un [text splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/). Existen distintos tipos de splitters según el tipo de archivo que queramos dividir, ya sea texto, HTML, markdown o código. Vamos a un usar un splitter recursivo. Dividir el texto de manera recursiva tiene el propósito de intentar mantener las piezas de texto relacionadas una con la siguiente. Es la manera recomendada de comenzar a dividir texto.\n",
    "\n",
    "Lo que haremos será dividir la transcripción en trozos (chunks) de 1000 caracteres con una superposición de 30 caracteres, es decir, cada trozo tendrá 1000 caracteres y los 30 últimos caracteres de uno serán los mismos que los 30 primeros caracteres del siguiente trozo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fff03944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
    "\n",
    "documentos = splitter.split_documents(documento_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "232c07e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# número de trozos de texto desde la transcripción completa\n",
    "\n",
    "len(documentos)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f591a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='Planned Chaos by Ludwig von Meises Introductory remarks The characteristic mark of this age of dictators, wars and revolutions is its anti-capitalistic bias. Most governments and political parties are eager to restrict the sphere of private initiative and free enterprise. It is an almost unchallenged dogma that capitalism is done for, and that the coming of all-round regimentation of economic activities is both inescapable and highly desirable. Nonetheless, capitalism is still very vigorous in the Western Hemisphere. Capitalist production has made very remarkable progress even in these last years. Methods of production were greatly improved. Consumers have been supplied with better and cheaper goods and with many new articles unheard of a short time ago. Many countries have expanded the size and improved the quality of their manufacturing. In spite of the anti-capitalistic policies of all governments and of almost all political parties, the capitalist mode of production is in many'),\n",
       " Document(metadata={'source': 'txt/transcripcion.txt'}, page_content=\"mode of production is in many countries still fulfilling its social function in supplying the consumers with more, better and cheaper goods. It is certainly not a merit of governments, politicians and labour union officers, that the standard of living is improving in the country's committed to the principle of private ownership of the means of production. Not offices and bureaucrats, but big business deserves the credit for the fact that most of the families in the United States own a motorcar and a radio set. The increase in per capita consumption in America, as compared with conditions a quarter of a century ago, is not an achievement of laws and executive orders. It is an accomplishment of businessmen who enlarge the size of their factories or built new ones. One must stress this point because our contemporaries are inclined to ignore it. Intangled in the superstitions of statism and government omnipotence, they are exclusively preoccupied with governmental measures. They expect\")]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veamos los dos primeros\n",
    "\n",
    "documentos[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a625d",
   "metadata": {},
   "source": [
    "### Encontrando los trozos relevantes\n",
    "\n",
    "Una vez troceado el texto completo la pregunta surge naturalmente, ¿qué trozos deberíamos de darle al LLM como contexto?. No usaremos cualquiera de los 165 trozos de texto que hemos generado, sino que tenemos que usar aquellos que sean los más relevantes para la consulta que estamos haciendo. Es aquí donde entra el concepto de embeddings. Los embeddings son representaciones vectoriales de palabras, frases, u otros elementos en un espacio vectorial continuo de baja dimensión. Son una herramienta fundamental en el campo del procesamiento del lenguaje, ya que permiten que los modelos trabajen con información textual de una manera matemática. Recordemos que las máquinas solo entienden de números.\n",
    "\n",
    "\n",
    "![chain5](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain5.png)\n",
    "\n",
    "\n",
    "Usaremos el modelo de embeddings de OpenAI que ya viene incorporado en LangChain. Aquí también tendremos que usar la API KEY de OpenAI para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae0eaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "vectorizador = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded27922",
   "metadata": {},
   "source": [
    "Hagamos un ejemplo sencillo para entender el proceso antes de vectorizar todos los trozos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd7d3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizamos una frase\n",
    "\n",
    "consulta = vectorizador.embed_query('Hola que tal, esto es una clase de IA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6916ca42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vector consulta: 1536\n",
      "\n",
      "[-0.009476827457547188, -0.005400040186941624, -0.009671423584222794, -0.017513643950223923, 0.0057730162516236305]\n"
     ]
    }
   ],
   "source": [
    "# longitud del vector\n",
    "\n",
    "print(f'Longitud del vector consulta: {len(consulta)}\\n')\n",
    "\n",
    "\n",
    "# primero 5 elementos del vector\n",
    "\n",
    "print(consulta[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad989c0",
   "metadata": {},
   "source": [
    "La elección de 1536 como longitud de los embeddings en algunos modelos no es arbitraria, sino que se basa en una serie de consideraciones técnicas y de rendimiento. \n",
    "\n",
    "1. Capacidad de Representación: Un tamaño de vector de 1536 permite una rica representación de los datos. Con más dimensiones, el modelo puede capturar más detalles y matices en los datos. Esto es particularmente útil donde las sutilezas del lenguaje y el contexto pueden ser críticos. Un tamaño mayor puede ayudar a capturar relaciones más complejas y patrones más finos.\n",
    "\n",
    "2. Rendimiento del Modelo: Aumentar el tamaño del embedding puede mejorar la capacidad del modelo para aprender y generalizar, hasta cierto punto. Modelos como BERT y sus variantes utilizan grandes vectores de embeddings para capturar un contexto lingüístico amplio y complejo, lo que les permite realizar bien en una variedad de tareas.\n",
    "\n",
    "3. Equilibrio entre Rendimiento y Recursos: Elegir el tamaño del embedding también es un balance entre el rendimiento del modelo y el consumo de recursos computacionales, como memoria y tiempo de procesamiento. Un tamaño de 1536 puede ser un buen compromiso para obtener buenos resultados mientras se mantienen los requisitos de recursos en niveles manejables.\n",
    "\n",
    "4. Compatibilidad y Estandarización: En algunos casos, el tamaño de los embeddings puede ser elegido para ser compatible con arquitecturas de modelo existentes o para estandarizar el entrenamiento y la implementación de modelos en diferentes entornos y aplicaciones. Esto facilita el uso de técnicas como la transferencia de aprendizaje y la implementación de modelos en producción.\n",
    "\n",
    "5. Resultados Empíricos: La decisión final sobre el tamaño del vector de embeddings a menudo se basa en pruebas y experiencias empíricas. Los investigadores y desarrolladores prueban diferentes tamaños y seleccionan el que proporciona el mejor rendimiento en las tareas específicas de interés, basándose en métricas como la precisión, la recuperación, y el F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "058697b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora vectoricemos dos frase más \n",
    "\n",
    "frase1 = vectorizador.embed_query('¿Hoy estuviste estudiando o no?')\n",
    "\n",
    "frase2 = vectorizador.embed_query('Estamos en clase, hay que ponerse a estudiar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f3dec",
   "metadata": {},
   "source": [
    "Ahora que tenemos dos frases vectorizadas, queremos saber cual se parece más a la consulta, es decir, cual de las dos frases es la más relevante. El método más habitual para hacer esto es la similitud del coseno dado que estamos trabajando con vectores. La similitud del coseno mide el coseno del ángulo entre dos vectores en un espacio vectorial. El valor de esta similitud varía entre -1 y 1, donde 1 indica que los dos vectores son idénticos en orientación (ángulo de 0 grados), 0 sugiere que los vectores son ortogonales (ángulo de 90 grados) y no comparten ninguna información, -1 indica que los vectores son diametralmente opuestos (ángulo de 180 grados). De esta manera obtenemos una métrica de similitud entre los vectores. Desde la librería `scikit-learn` podemos usar la similitud del coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08d9b811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7787063171053135, 0.8269535169772084)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similitud_frase1 = cosine_similarity([consulta], [frase1])[0][0]\n",
    "\n",
    "similitud_frase2 = cosine_similarity([consulta], [frase2])[0][0]\n",
    "\n",
    "similitud_frase1, similitud_frase2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d44a8",
   "metadata": {},
   "source": [
    "Como vemos, la frase más parecida a nuestra consulta es la frase 2. Con esta estrategia podemos saber que trozos de texto son los más relevantes para la consulta específica que hagamos al LLM. El proceso será vectorizar todos los trozos de texto y luego calcular la similitud de los mismos con nuestra consulta para obtener los textos más relevantes y usarlos como contexto en el LLM. Para ello, lo adecuado es guardar los vectores obtenidos por el modelo de embedding en una base de datos y luego realizar una búsqueda de los vectores más relevantes.\n",
    "\n",
    "\n",
    "![chain6](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77ed2b",
   "metadata": {},
   "source": [
    "## 9. Bases de datos vectoriales\n",
    "\n",
    "\n",
    "LangChain provee de una buena cantidad de [bases de datos vectoriales](https://python.langchain.com/docs/integrations/vectorstores/) para el manejo de los embeddings. Vamos probar primero con una base de datos local en memoria, DocArrayInMemorySearch, que es un índice de documentos proporcionado por Docarray que almacena documentos en memoria. Es un excelente punto de partida para conjuntos de datos pequeños, donde es posible que no desees iniciar un servidor de base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49fbda4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "\n",
    "# a docarray le pasamos los texto y el modelo de embedding\n",
    "\n",
    "local_db = DocArrayInMemorySearch.from_documents(documentos, vectorizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbaa3ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.'),\n",
       "  0.7531981736270884)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podemos extraer los k vectores más relevates con su similitud, donde k es el nº de trozos que queremos \n",
    "\n",
    "consulta = 'What are the key points of this book? Put them in a bullet point list.'\n",
    "\n",
    "local_db.similarity_search_with_score(query=consulta, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78f01465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# también podemos extraer los k vectores más relevates sin similitud\n",
    "\n",
    "local_db.similarity_search(query=consulta, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6257598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.'),\n",
       " Document(metadata={'source': 'txt/transcripcion.txt'}, page_content=\"a time-consuming procedure. They suggested a bold stroke. A small group of fanatics should be organised as the vanguard of the revolution. Strict discipline and unconditional obedience to the chief should make these professional revolutionists fit for a sudden attack. They should supplant the Tsarist government and then rule the country according to the traditional methods of the Tsar's police. The terms used to signify these two groups, Bolsheviks' majority for the latter and Mensheviks' minority for the former, refer to a vote taken in 1903 at a meeting held for the discussion of these tactical issues. The only difference dividing the two groups from one another was the matter of tactical methods. They both agreed with regard to the ultimate end, socialism. Both sex tried to justify their respective points of view by quoting passages from Marx's and Engels writings. This is, of course, the Marxian custom. And each sect was in a position to discover in these sacred books, Dicta\"),\n",
       " Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='Every single plant, shop or farm, stands in the same relation to the superior central organisation, as does a post office to the office of the postmaster general. The whole nation forms one single labour army with compulsory service. The commander of this army is the chief of state. The second pattern we may call it the German or Sfangsvitz shaft system differs from the first one in that it seemingly and nominally maintains private ownership of the means of production, entrepreneurship and market exchange. So-called entrepreneurs do the buying and selling, pay the workers, contract debts and pay interest and amortisation. But they are no longer entrepreneurs. In Nazi Germany they were called shop managers or betriebsfuerre. The government tells these seeming entrepreneurs what and how to produce, at what prices and from whom to buy, at what prices and to whom to sell. The government decrees at what wages labourers should work, and to whom and under what terms the capitalists should'),\n",
       " Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='Planned Chaos by Ludwig von Meises Introductory remarks The characteristic mark of this age of dictators, wars and revolutions is its anti-capitalistic bias. Most governments and political parties are eager to restrict the sphere of private initiative and free enterprise. It is an almost unchallenged dogma that capitalism is done for, and that the coming of all-round regimentation of economic activities is both inescapable and highly desirable. Nonetheless, capitalism is still very vigorous in the Western Hemisphere. Capitalist production has made very remarkable progress even in these last years. Methods of production were greatly improved. Consumers have been supplied with better and cheaper goods and with many new articles unheard of a short time ago. Many countries have expanded the size and improved the quality of their manufacturing. In spite of the anti-capitalistic policies of all governments and of almost all political parties, the capitalist mode of production is in many')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o definir un recuperador de la base de datos para invocarlo que por defecto devuelve los 4 más relevantes\n",
    "\n",
    "recuperador = local_db.as_retriever()\n",
    "\n",
    "recuperador.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f71382c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recuperador.invoke(consulta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a95e9eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aunque podemos definirlo para extraer los que queramos\n",
    "\n",
    "# o definir un recuperador de la base de datos para invocarlo que por defecto devuelve los 4 más parecidos\n",
    "\n",
    "recuperador = local_db.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "len(recuperador.invoke(consulta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283794d",
   "metadata": {},
   "source": [
    "### Conectando la base de datos a la cadena\n",
    "\n",
    "\n",
    "![chain7](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chain7.png)\n",
    "\n",
    "\n",
    "Ahora que tenemos la base de datos disponible en memoria, podemos conectarla a la cadena de LangChain, junto con la plantilla del prompt, el modelo LLM y el parser a string que habiamos definido anteriormente. Para ello vamos a usar dos objetos que nos proporciona LangChain.\n",
    "\n",
    "RunnableParallel es esencialmente un diccionario cuyos valores son ejecutables, u objetos que pueden convertirse en ejecutables, como funciones. Ejecuta todos sus valores en paralelo, y cada valor se llama con la entrada general del RunnableParallel. El valor de retorno final es un diccionario con los resultados de cada valor bajo su clave correspondiente. Es útil para paralelizar operaciones, pero también puede ser útil para manipular la salida de un Runnable (protocolo ejecutable) para que coincida con el formato de entrada del siguiente Runnable en una secuencia. Se espera que la entrada al prompt sea un diccionario con las claves \"contexto\" y \"pregunta\". La entrada del usuario es solo la pregunta. Por lo tanto, necesitamos obtener el contexto usando nuestro recuperador y pasar la entrada del usuario.\n",
    "\n",
    "RunnablePassthrough por sí solo permite pasar las entradas sin cambios. Esto típicamente se usa en conjunto con RunnableParallel para pasar datos a una nueva clave en el diccionario que entra en la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5a9c9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.'),\n",
       "  Document(metadata={'source': 'txt/transcripcion.txt'}, page_content=\"a time-consuming procedure. They suggested a bold stroke. A small group of fanatics should be organised as the vanguard of the revolution. Strict discipline and unconditional obedience to the chief should make these professional revolutionists fit for a sudden attack. They should supplant the Tsarist government and then rule the country according to the traditional methods of the Tsar's police. The terms used to signify these two groups, Bolsheviks' majority for the latter and Mensheviks' minority for the former, refer to a vote taken in 1903 at a meeting held for the discussion of these tactical issues. The only difference dividing the two groups from one another was the matter of tactical methods. They both agreed with regard to the ultimate end, socialism. Both sex tried to justify their respective points of view by quoting passages from Marx's and Engels writings. This is, of course, the Marxian custom. And each sect was in a position to discover in these sacred books, Dicta\"),\n",
       "  Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='Every single plant, shop or farm, stands in the same relation to the superior central organisation, as does a post office to the office of the postmaster general. The whole nation forms one single labour army with compulsory service. The commander of this army is the chief of state. The second pattern we may call it the German or Sfangsvitz shaft system differs from the first one in that it seemingly and nominally maintains private ownership of the means of production, entrepreneurship and market exchange. So-called entrepreneurs do the buying and selling, pay the workers, contract debts and pay interest and amortisation. But they are no longer entrepreneurs. In Nazi Germany they were called shop managers or betriebsfuerre. The government tells these seeming entrepreneurs what and how to produce, at what prices and from whom to buy, at what prices and to whom to sell. The government decrees at what wages labourers should work, and to whom and under what terms the capitalists should'),\n",
       "  Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='Planned Chaos by Ludwig von Meises Introductory remarks The characteristic mark of this age of dictators, wars and revolutions is its anti-capitalistic bias. Most governments and political parties are eager to restrict the sphere of private initiative and free enterprise. It is an almost unchallenged dogma that capitalism is done for, and that the coming of all-round regimentation of economic activities is both inescapable and highly desirable. Nonetheless, capitalism is still very vigorous in the Western Hemisphere. Capitalist production has made very remarkable progress even in these last years. Methods of production were greatly improved. Consumers have been supplied with better and cheaper goods and with many new articles unheard of a short time ago. Many countries have expanded the size and improved the quality of their manufacturing. In spite of the anti-capitalistic policies of all governments and of almost all political parties, the capitalist mode of production is in many'),\n",
       "  Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='principles. There was some minor descent with regard to tactical methods, but in all the central matters Stalin and Trotsky were in agreement. He had lived, before 1917, many years in foreign countries, and was to some degree familiar with the main languages of the Western peoples. He posed as an expert in international affairs. Actually, he did not know anything about Western civilization, political ideas, and economic conditions. As a wandering exile, he had moved almost exclusively in the circles of his fellow exiles. The only foreigners whom he had met occasionally in coffeehouses and club rooms of Western and Central Europe were radical doctrineaires, by their Marxian prepositions precluded from reality. His main reading was Marxian books and periodicals. He scorned all other writings as bourgeois literature. He was absolutely unfitted to see events from any other angle than that of Marxism. Like Marx, he was ready to interpret every great strike and every small riot as the sign')],\n",
       " 'question': 'What are the key points of this book? Put them in a bullet point list.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "\n",
    "# ejecutamos el RunnableParallel para obtener el contexto\n",
    "\n",
    "contexto_recuperado = RunnableParallel(context=recuperador, question=RunnablePassthrough())\n",
    "\n",
    "contexto_recuperado.invoke(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b91adfa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Intellectual leaders popularized fallacies that are destroying liberty and western civilization\\n- Intellectuals are responsible for mass slaughters in the 20th century\\n- Reason and ideas, not material forces, determine human affairs\\n- Common sense and moral courage are needed to stop the trend towards socialism and despotism\\n- The book discusses the Bolsheviks and Mensheviks, who both aimed for socialism but differed in tactical methods\\n- The book criticizes the central organization of the economy in Nazi Germany and the illusion of private ownership\\n- The book challenges the anti-capitalistic bias and argues that capitalism is still vigorous in the Western Hemisphere'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creamos la cadena con el contexto recuperado desde la bade de datos\n",
    "\n",
    "cadena = contexto_recuperado | prompt | modelo | parser\n",
    "\n",
    "cadena.invoke(consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372323e",
   "metadata": {},
   "source": [
    "Como hemos comentado, Docarray almacena documentos en memoria local. Pero para una aplicación real no nos sirve. Si queremos hacer un despliegue del modelo para una aplicación en producción necesitaremos una base de datos que podamos alojar en un servidor. Veamos un par de opciones open source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac9913",
   "metadata": {},
   "source": [
    "###  Chroma \n",
    "\n",
    "![chroma](https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/chroma.jpeg)\n",
    "\n",
    "\n",
    "Otra opción que tenomos es [ChromaDB](https://www.trychroma.com/), que es una base de datos especializada en el almacenamiento y recuperación eficiente de información lingüística, incluyendo datos de texto, anotaciones semánticas y sintácticas. ChromaDB es particularmente útil para el almacenamiento y la gestión de grandes cantidades de datos de lenguaje natural, lo que permite a los desarrolladores aprovechar al máximo los avances en algoritmos de aprendizaje automático y análisis de texto. Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "706c97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85e2500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado en disco\n",
    "\n",
    "chroma_db = Chroma.from_documents(documentos, vectorizador, persist_directory='save_db')\n",
    "\n",
    "docs = chroma_db.similarity_search(consulta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea10d40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "430df1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_66181/680532756.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'txt/transcripcion.txt'}, page_content='They did not move the masses directly. It was the intellectuals who popularised them. The intellectual leaders of the peoples have produced and propagated the fallacies which are on the point of destroying liberty and western civilisation. The intellectuals alone are responsible for the mass slaughters which are the characteristic mark of our century. They alone can reverse the trend and pave the way for a resurrection of freedom. Not mythical material productive forces, but reason and ideas determine the course of human affairs. What is needed to stop the trend towards socialism and despotism is common sense and moral courage.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carga desde disco\n",
    "\n",
    "chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)\n",
    "\n",
    "docs = chroma_db.similarity_search(consulta)\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95cfca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Los líderes intelectuales son responsables de popularizar falacias que están destruyendo la libertad y la civilización occidental. Los intelectuales también son responsables de matanzas en masa en el siglo XX. La necesidad de revertir la tendencia hacia el socialismo y el despotismo con sentido común y coraje moral. Comparación entre dos patrones organizativos: uno en el que el gobierno controla todos los aspectos de la producción y otro en el que la propiedad privada se mantiene nominalmente pero es controlada por el gobierno al estilo español.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cadena con traducción y con la recuperacion de ChromaDB con 5 documentos\n",
    "\n",
    "cadena =  prompt | modelo | parser\n",
    "\n",
    "cadena_traducida = (\n",
    "    {'answer': cadena, 'language': lambda x: x['language']} | prompt_traductor | modelo | parser\n",
    ")\n",
    "\n",
    "cadena_traducida.invoke({'context': chroma_db.similarity_search(consulta, k=5), \n",
    "                         'question': consulta,\n",
    "                         'language': 'Spanish'}).split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d2cae",
   "metadata": {},
   "source": [
    "Desde aquí, ya podríamos hacer un despligue de nuestra cadena para implementarla en una aplicación en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c5881",
   "metadata": {},
   "source": [
    "## 10. Resumen paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67ced09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 s, sys: 188 ms, total: 1.55 s\n",
      "Wall time: 11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['- Los intelectuales han popularizado falacias que están destruyendo la libertad y la civilización occidental',\n",
       " '- Los líderes intelectuales son responsables de las matanzas en masa en el siglo XX',\n",
       " '- La razón y las ideas, no las fuerzas productivas materiales, determinan los asuntos humanos',\n",
       " '- Se necesita sentido común y valor moral para detener la tendencia hacia el socialismo y el despotismo',\n",
       " '- La era de los dictadores, las guerras y las revoluciones tiene un sesgo anticapitalista',\n",
       " '- Muchos gobiernos y partidos políticos están ansiosos por restringir la iniciativa privada y la libre empresa',\n",
       " '- El capitalismo sigue siendo vigoroso en el Hemisferio Occidental a pesar de las políticas anticapitalistas',\n",
       " '- La producción capitalista ha hecho un progreso notable con métodos mejorados y suministro de bienes a España.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# librerias\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from pytube import YouTube\n",
    "import whisper\n",
    "import tempfile\n",
    "\n",
    "import os                           \n",
    "from dotenv import load_dotenv      \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# importamos la api key de OpenAI\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# iniciamos modelo gpt3.5 turbo\n",
    "modelo = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')\n",
    "\n",
    "\n",
    "\n",
    "# plantilla de texto con un contexto y una pregunta\n",
    "plantilla = '''\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            '''\n",
    "\n",
    "# carga de la plantilla en el prompt\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "\n",
    "# traduccion\n",
    "prompt_traductor = ChatPromptTemplate.from_template('Translate {answer} to {language}')\n",
    "\n",
    "\n",
    "# extraccion de datos\n",
    "# definimos la url del video\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=7EnHeZXLzTc'\n",
    "\n",
    "# usamos pytube para extraer el video\n",
    "youtube = YouTube(VIDEO_URL)\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "# se carga el modelo base de Whisper \n",
    "modelo_whisper = whisper.load_model('base')\n",
    "\n",
    "\n",
    "# ruta de guardado del archivo de texto\n",
    "RUTA_TXT = 'txt/transcripcion.txt'\n",
    "\n",
    "# si el archivo de texto no existe...\n",
    "if not os.path.exists(RUTA_TXT):\n",
    "    \n",
    "    # abrimos el directorio temporal...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # descargamos el audio de YouTube...\n",
    "        archivo_audio = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # y Whiper transcribe el audio a texto.\n",
    "        transcripcion = modelo_whisper.transcribe(archivo_audio, fp16=False)['text'].strip()\n",
    "        \n",
    "        \n",
    "        # se guarda el archivo de texto\n",
    "        with open(RUTA_TXT, 'w') as archivo_texto:\n",
    "            archivo_texto.write(transcripcion)\n",
    "\n",
    "\n",
    "# cargamos el archivo de texto para usarlo en la cadena\n",
    "with open(RUTA_TXT, 'r') as archivo_texto:\n",
    "    transcripcion = archivo_texto.read()\n",
    "\n",
    "\n",
    "# transformacion del dato\n",
    "loader = TextLoader(RUTA_TXT)\n",
    "documento_texto = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
    "documentos = splitter.split_documents(documento_texto)\n",
    "\n",
    "\n",
    "vectorizador = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "\n",
    "# guardado en disco, no sería necesario, hacer aparte\n",
    "chroma_db = Chroma.from_documents(documentos, vectorizador, persist_directory='save_db')\n",
    "\n",
    "\n",
    "# carga desde disco\n",
    "chroma_db = Chroma(persist_directory='save_db', embedding_function=vectorizador)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# consulta\n",
    "consulta = 'What are the key points of this book? Put them in a bullet point list.'\n",
    "\n",
    "\n",
    "# parser a string\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cadena con traducción y con la recuperacion de ChromaDB con 5 documentos\n",
    "cadena =  prompt | modelo | parser\n",
    "\n",
    "\n",
    "cadena_traducida = (\n",
    "    {'answer': cadena, 'language': lambda x: x['language']} | prompt_traductor | modelo | parser\n",
    ")\n",
    "\n",
    "cadena_traducida.invoke({'context': chroma_db.similarity_search(consulta, k=5), \n",
    "                         'question': consulta,\n",
    "                         'language': 'Spanish'}).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3446a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {
    "height": "379px",
    "width": "533px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "599px",
    "left": "63px",
    "top": "111.141px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
