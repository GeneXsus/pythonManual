{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a807d1",
   "metadata": {},
   "source": [
    "# 2 - Model I/O\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428e963",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---I/O\" data-toc-modified-id=\"1---I/O-1\">1 - I/O</a></span></li><li><span><a href=\"#2---LLMs\" data-toc-modified-id=\"2---LLMs-2\">2 - LLMs</a></span></li><li><span><a href=\"#3---Chat-models\" data-toc-modified-id=\"3---Chat-models-3\">3 - Chat models</a></span></li><li><span><a href=\"#4---Prompts\" data-toc-modified-id=\"4---Prompts-4\">4 - Prompts</a></span></li><li><span><a href=\"#5---Parsers-de-Salida\" data-toc-modified-id=\"5---Parsers-de-Salida-5\">5 - Parsers de Salida</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1---SimpleJsonOutputParser\" data-toc-modified-id=\"5.1---SimpleJsonOutputParser-5.1\">5.1 - SimpleJsonOutputParser</a></span></li><li><span><a href=\"#5.2---CommaSeparatedListOutputParser\" data-toc-modified-id=\"5.2---CommaSeparatedListOutputParser-5.2\">5.2 - CommaSeparatedListOutputParser</a></span></li><li><span><a href=\"#5.3---DatetimeOutputParser\" data-toc-modified-id=\"5.3---DatetimeOutputParser-5.3\">5.3 - DatetimeOutputParser</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b12eb",
   "metadata": {},
   "source": [
    "## 1 - I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48606c51",
   "metadata": {},
   "source": [
    "En LangChain, el elemento central de cualquier aplicación gira en torno al modelo de lenguaje. Este módulo proporciona los componentes esenciales para interactuar de manera efectiva con cualquier modelo de lenguaje, asegurando una integración y comunicación fluidas. Los componentes clave del Model I/O son:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **LLMs y Modelos de Chat (usados indistintamente)**:\n",
    "\n",
    "    + **LLMs**: modelos de completado de texto puro, reciben una cadena de texto como entrada y devuelven una cadena de texto como salida.\n",
    "\n",
    "\n",
    "    + **Modelos de Chat**:  modelos que utilizan un modelo de lenguaje como base, pero difieren en los formatos de entrada y salida. Aceptan una lista de mensajes de chat como entrada y devuelven un mensaje de chat como salida.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Prompts**: permiten la creación de plantillas, la selección dinámica y la gestión de las entradas del modelo. Esto permite generar prompts flexibles y específicos al contexto que guían las respuestas del modelo de lenguaje.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Parsers de Salida**: extraen y formatean la información de las salidas del modelo. Son útiles para convertir la salida en bruto de los modelos de lenguaje en datos estructurados o formatos específicos necesarios para la aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c446d5c",
   "metadata": {},
   "source": [
    "## 2 - LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903dc8a8",
   "metadata": {},
   "source": [
    "La integración de LangChain con los modelos de lenguaje (LLMs) como OpenAI o HuggingFace es un aspecto fundamental de su funcionalidad. LangChain no aloja LLMs por sí mismo, pero ofrece una interfaz uniforme para interactuar con varios LLMs.\n",
    "\n",
    "Esta sección proporciona una visión general sobre el uso del envoltorio (wrapper) de LLM de OpenAI en LangChain, aplicable también a otros tipos de LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdcf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero cargamos la API KEY de OpenAI\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610661f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos llm desde LangChain\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69718819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el llm, por defecto gpt-3.5-turbo-instruct\n",
    "\n",
    "llm = OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a01fb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSoy un programa de ordenador, no tengo emociones ni capacidad de sentir, por lo que no puedo estar bien o mal. ¿En qué puedo ayudarte?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso del llm con el metodo invoke\n",
    "\n",
    "llm.invoke('hola como estas?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9253d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otro ejemplo\n",
    "\n",
    "respuesta = llm.invoke('¿cuales son las 7 maravillas del mundo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4265a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. La Gran Pirámide de Guiza (Egipto)\n",
      "2. Jardines Colgantes de Babilonia (actualmente, Iraq)\n",
      "3. Estatua de Zeus en Olimpia (Grecia)\n",
      "4. Templo de Artemisa en Éfeso (actualmente, Turquía)\n",
      "5. Mausoleo de Halicarnaso (actualmente, Turquía)\n",
      "6. Coloso de Rodas (actualmente, Grecia)\n",
      "7. Faro de Alejandría (actualmente, Egipto)\n"
     ]
    }
   ],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86a9b2",
   "metadata": {},
   "source": [
    "También podemos llamar al método `stream` para transmitir la respuesta de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59da282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. La Gran Pirámide de Guiza (Egipto)\n",
      "2. Los Jardines Colgantes de Babilonia (Irak)\n",
      "3. El Templo de Artemisa en Éfeso (Turquía)\n",
      "4. La Estatua de Zeus en Olimpia (Grecia)\n",
      "5. El Mausoleo de Halicarnaso (Turquía)\n",
      "6. El Coloso de Rodas (Grecia)\n",
      "7. El Faro de Alejandría (Egipto)"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream('¿cuales son las 7 maravillas del mundo?'):\n",
    "    \n",
    "    print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d948d",
   "metadata": {},
   "source": [
    "## 3 - Chat models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1a7fb",
   "metadata": {},
   "source": [
    "La integración de LangChain con modelos de chat, una variación especializada de los modelos de lenguaje, es esencial para crear aplicaciones de chat interactivas. Aunque utilizan modelos de lenguaje internamente, los modelos de chat presentan una interfaz distinta centrada en los mensajes de chat como entradas y salidas. Usaremos el chat de OpenAI, dependiendo de la version de LangChain que tengamos, será necesario instalar:\n",
    "\n",
    "```bash\n",
    "pip install langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f7facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el modelo de chat de OpenAI\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb1114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el chat, por defecto gpt-3.5-turbo\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10327082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿En qué puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 9, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6a5dc0a1-aab1-406b-a65d-ea48349c690a-0', usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso del chat con string\n",
    "\n",
    "chat.invoke('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a595abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambio a gpt-4o\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81bda02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿En qué puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None}, id='run-4459c9ef-5c3e-483a-bcc0-7e27862f0e54-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso del chat con string\n",
    "\n",
    "chat.invoke('hola')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bb57d",
   "metadata": {},
   "source": [
    "Como vemos, podemos usar una string al invocar al modelo directamente, pero la salida ya no es una string, es un AIMessage. Los modelos de chat en LangChain trabajan con diferentes tipos de mensajes, como AIMessage, HumanMessage, SystemMessage, FunctionMessage y ChatMessage, este último con un parámetro de rol arbitrario. En general, HumanMessage, AIMessage y SystemMessage son los más utilizados. Veamos otro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27ee1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Estoy asociado con Nike. Juntos lanzamos la línea de calzado Air Jordan, que comenzó en 1984 y se ha convertido en una marca icónica en el mundo del baloncesto y la moda.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 26, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None}, id='run-cbb36533-4b18-4863-8e1d-3d6ebc725f7c-0', usage_metadata={'input_tokens': 26, 'output_tokens': 43, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "mensajes = [SystemMessage(content='Eres Micheal Jordan.'),\n",
    "            HumanMessage(content='¿Con qué fabricante de zapatos estás asociado?')]\n",
    "\n",
    "\n",
    "respuesta = chat.invoke(mensajes)\n",
    "\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984318ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estoy asociado con Nike. Juntos lanzamos la línea de calzado Air Jordan, que comenzó en 1984 y se ha convertido en una marca icónica en el mundo del baloncesto y la moda.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string de respuesta\n",
    "\n",
    "respuesta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41299ad",
   "metadata": {},
   "source": [
    "## 4 - Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c35a8",
   "metadata": {},
   "source": [
    "Los prompts son esenciales para guiar a los modelos de lenguaje a generar resultados relevantes y coherentes. Pueden ir desde instrucciones simples hasta ejemplos complejos de few-shot. En LangChain, manejar prompts puede ser un proceso muy sencillo, gracias a varias clases y funciones dedicadas.\n",
    "\n",
    "\n",
    "La clase `PromptTemplate` de LangChain es una herramienta versátil para crear prompts en formato de cadena de texto. Utiliza la sintaxis str.format de Python, lo que permite generar prompts de forma dinámica. Podemos definir una plantilla con marcadores de posición y rellenarlos con valores específicos según sea necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2693450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la plantilla de prompts\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1281c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la plantilla\n",
    "\n",
    "plantilla = PromptTemplate.from_template('Cuentame un chiste {adjetivo} sobre {contenido}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5972f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables de usuario\n",
    "\n",
    "adjetivo = 'gracioso'\n",
    "\n",
    "contenido = 'robots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ca19127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el prompt completo\n",
    "\n",
    "prompt = plantilla.format(adjetivo=adjetivo, contenido=contenido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b095740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cuentame un chiste gracioso sobre robots.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a326c8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Claro! Aquí tienes un chiste sobre robots:\\n\\n¿Por qué los robots nunca tienen miedo a nada?\\n\\n¡Porque siempre mantienen la calma y calculan todas las posibilidades!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso del prompt en el chat\n",
    "\n",
    "chat.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7b696",
   "metadata": {},
   "source": [
    "Para los modelos de chat, los prompts son más estructurados, involucrando mensajes con roles específicos. LangChain ofrece `ChatPromptTemplate` para este propósito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de066d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la plantilla de prompts para chats\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a162ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos un prompt de chat para varios roles\n",
    "\n",
    "plantilla_chat = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    ('system', 'Eres un buen asistente personal. Tu nombre es {nombre}.'),\n",
    "    ('human', 'Hola, ¿como estas?'),\n",
    "    ('ai', 'Estoy bien, gracias.'),\n",
    "    ('human', '{pregunta}')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edd3df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Eres un buen asistente personal. Tu nombre es Pepe.' additional_kwargs={} response_metadata={}\n",
      "content='Hola, ¿como estas?' additional_kwargs={} response_metadata={}\n",
      "content='Estoy bien, gracias.' additional_kwargs={} response_metadata={}\n",
      "content='¿Como te llamas?' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "# formateo del mensaje\n",
    "\n",
    "mensajes = plantilla_chat.format_messages(nombre='Pepe', pregunta='¿Como te llamas?')\n",
    "\n",
    "\n",
    "for m in mensajes:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d349e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me llamo Pepe. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(mensajes).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095742c",
   "metadata": {},
   "source": [
    "Este enfoque permite la creación de chatbots interactivos y atractivos con respuestas dinámicas.\n",
    "\n",
    "Tanto `PromptTemplate` como `ChatPromptTemplate` se integran perfectamente con el Lenguaje de Expresión de LangChain (LCEL), lo que les permite formar parte de flujos de trabajo más grandes y complejos. A veces, los prompts personalizados son esenciales para tareas que requieren un formato único o instrucciones específicas. Crear una plantilla de prompt personalizada implica definir variables de entrada y un método de formato personalizado. Esta flexibilidad permite que LangChain se adapte a una amplia gama de requisitos específicos de las aplicaciones.\n",
    "\n",
    "LangChain también admite few-shot prompting, lo que permite al modelo aprender a partir de ejemplos. Esta función es vital para tareas que requieren comprensión contextual o patrones específicos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee25d8",
   "metadata": {},
   "source": [
    "## 5 - Parsers de Salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f2b06",
   "metadata": {},
   "source": [
    "Los parsers de salida desempeñan un papel crucial en LangChain, permitiendo a los usuarios estructurar las respuestas generadas por los modelos de lenguaje. En esta sección, exploraremos el concepto de parsers de salida y proporcionaremos ejemplos de código utilizando los siguientes parsers de LangChain: `SimpleJsonOutputParser`, `CommaSeparatedListOutputParser` y `DatetimeOutputParser`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48df75",
   "metadata": {},
   "source": [
    "### 5.1 - SimpleJsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb08fd6",
   "metadata": {},
   "source": [
    "El `SimpleJsonOutputParser` de LangChain se utiliza cuando se desea analizar salidas en formato JSON. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20853313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61ce91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un prompt pidiendole un JSON \n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    'Devuelve un JSON con `fecha_nacimiento` y `lugar_nacimiento` para la siguiente pregunta: {pregunta}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e409737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el parser\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "462d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una cadena con el prompt, modelo y parser\n",
    "\n",
    "json_chain = json_prompt | chat | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "699012fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada a la cadena\n",
    "\n",
    "json = json_chain.invoke({'pregunta': 'Donde y cuando nacio Elon Musk'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "750e1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fecha_nacimiento': '28 de junio de 1971', 'lugar_nacimiento': 'Pretoria, Sudáfrica'}\n"
     ]
    }
   ],
   "source": [
    "print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "825c40e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0e75c",
   "metadata": {},
   "source": [
    "### 5.2 - CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed026b93",
   "metadata": {},
   "source": [
    "El `CommaSeparatedListOutputParser` es útil cuando deseas extraer listas separadas por comas de las respuestas del modelo. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68fb87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5613de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el parser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e82a904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos un prompt\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    'Devuelve 5 {pregunta}. La respuesta debe ser una lista de strings separadas por coma.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7986c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aff08e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke({'pregunta': 'Equipos de la liga española'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d14ae290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real Madrid', 'FC Barcelona', 'Atlético de Madrid', 'Sevilla FC', 'Valencia CF']\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "774dc603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd116e22",
   "metadata": {},
   "source": [
    "### 5.3 - DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03980be",
   "metadata": {},
   "source": [
    "El `DatetimeOutputParser` de LangChain está diseñado para analizar información de fecha y hora. Veamos cómo usarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5f674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e31693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicamos el parser\n",
    "\n",
    "parser = DatetimeOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00a48b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    'Responde la siguiente pregunta {pregunta}. Devuelve solamente el formato %Y-%m-%dT%H:%M:%S.%fZ.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed7d57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4adc0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1969, 7, 20, 20, 17, 40)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'pregunta': 'Cuando llego el hombre a la luna'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "219.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
