{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f725e20",
   "metadata": {},
   "source": [
    "# 3 - Añadiendo memoria a la cadena\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e33cd",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Cadena-con-memoria\" data-toc-modified-id=\"1---Cadena-con-memoria-1\">1 - Cadena con memoria</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba65eb",
   "metadata": {},
   "source": [
    "## 1 - Cadena con memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa0e96",
   "metadata": {},
   "source": [
    "Primero necesitamos varios componentes necesarios para crear el chatbot:\n",
    "\n",
    "+ `itemgetter`: Extrae elementos de un diccionario o lista.\n",
    "\n",
    "+ `ChatOpenAI`: Un modelo de lenguaje proporcionado por OpenAI para manejar las interacciones de chat.\n",
    "\n",
    "+ `ConversationBufferMemory`: Un componente para almacenar el historial de la conversación.\n",
    "\n",
    "+ `RunnablePassthrough` y `RunnableLambda`: Son componentes para crear pasos de procesamiento en una cadena (pipeline).\n",
    "\n",
    "+ `ChatPromptTemplate` y `MessagesPlaceholder`: Manejan la estructura del prompt que se enviará al modelo de lenguaje.\n",
    "\n",
    "+ `StrOutputParser`: Transforma la salida de la cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c6dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias \n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa0b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos la API KEY de OpenAI\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3a19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciamos el modelo de OpenAI\n",
    "\n",
    "modelo = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911aac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([('system', 'Eres un asistente.'),\n",
    "                                           \n",
    "                                           MessagesPlaceholder(variable_name='history'),\n",
    "                                           \n",
    "                                           ('human', '{pregunta}')\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser de salida, transforma la salida a string\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2c2d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_18101/979582630.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# definimos el objeto de memoria\n",
    "\n",
    "memoria = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b601750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniciamos la memoria\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2a86a",
   "metadata": {},
   "source": [
    "Ahora creamos la cadena o pipeline que procesa las entradas y las envía al modelo:\n",
    "\n",
    "+ `RunnablePassthrough`: Define un paso que pasará las variables sin modificarlas.\n",
    "\n",
    "    + `assign`: Se utiliza para asignar valores a las variables que se utilizan en la cadena. En este caso, asigna el historial, cargado desde la memoria mediante RunnableLambda.\n",
    "    \n",
    "    + `RunnableLambda`: Es utilizado para crear pasos personalizados dentro de una cadena o flujo de procesamiento. Este componente permite definir una función o lógica personalizada, usualmente una función lambda, que se puede ejecutar como parte de una secuencia más amplia de operaciones dentro de una cadena de LangChain.\n",
    "    \n",
    "    + `itemgetter('history')`: Extrae específicamente la variable \"history\" del resultado de load_memory_variables, un diccionario.\n",
    "\n",
    "+ `| prompt`: Pasa la entrada, junto con el historial, al prompt definido anteriormente.\n",
    "\n",
    "+ `| modelo`: El resultado del prompt se envía al modelo de lenguaje para generar una respuesta.\n",
    "\n",
    "+ `| parser`: parser de salida, transforma la salida a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d184237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la cadena\n",
    "\n",
    "cadena = (RunnablePassthrough.assign(\n",
    "          history=RunnableLambda(memoria.load_memory_variables) | itemgetter('history'))\n",
    "         \n",
    "          | prompt\n",
    "         \n",
    "          | modelo\n",
    "          \n",
    "          | parser\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee34a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, Pepe. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# respuesta de la cadena\n",
    "\n",
    "pregunta = {'pregunta': 'Hola, soy Pepe'}\n",
    "\n",
    "respuesta = cadena.invoke(pregunta)\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be460aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guarda la conversacion en la memoria\n",
    "\n",
    "memoria.save_context(pregunta, {'respuesta': respuesta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee69cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hola, soy Pepe', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hola, Pepe. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memoria de la cadena\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910f769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Te llamas Pepe. ¿Hay algo más con lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segunda respuesta de la cadena\n",
    "\n",
    "pregunta = {'pregunta': '¿como me llamo?'}\n",
    "\n",
    "respuesta = cadena.invoke(pregunta)\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fb46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "219.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
