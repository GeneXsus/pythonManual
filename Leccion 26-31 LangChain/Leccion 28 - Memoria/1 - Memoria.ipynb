{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24529e9c",
   "metadata": {},
   "source": [
    "# 1 - Memoria\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cdfe1",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Memoria\" data-toc-modified-id=\"1---Memoria-1\">1 - Memoria</a></span></li><li><span><a href=\"#2---Tipos-de-memoria-en-LangChain\" data-toc-modified-id=\"2---Tipos-de-memoria-en-LangChain-2\">2 - Tipos de memoria en LangChain</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1---ConversationBufferMemory\" data-toc-modified-id=\"2.1---ConversationBufferMemory-2.1\">2.1 - ConversationBufferMemory</a></span></li><li><span><a href=\"#2.2---ConversationBufferWindowMemory\" data-toc-modified-id=\"2.2---ConversationBufferWindowMemory-2.2\">2.2 - ConversationBufferWindowMemory</a></span></li><li><span><a href=\"#2.3---ConversationEntityMemory\" data-toc-modified-id=\"2.3---ConversationEntityMemory-2.3\">2.3 - ConversationEntityMemory</a></span></li><li><span><a href=\"#2.4---Conversation-Knowledge-Graph-Memory\" data-toc-modified-id=\"2.4---Conversation-Knowledge-Graph-Memory-2.4\">2.4 - Conversation Knowledge Graph Memory</a></span></li><li><span><a href=\"#2.5---ConversationSummaryMemory\" data-toc-modified-id=\"2.5---ConversationSummaryMemory-2.5\">2.5 - ConversationSummaryMemory</a></span></li><li><span><a href=\"#2.6---ConversationSummaryBufferMemory\" data-toc-modified-id=\"2.6---ConversationSummaryBufferMemory-2.6\">2.6 - ConversationSummaryBufferMemory</a></span></li><li><span><a href=\"#2.7---ConversationTokenBufferMemory\" data-toc-modified-id=\"2.7---ConversationTokenBufferMemory-2.7\">2.7 - ConversationTokenBufferMemory</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc5f67",
   "metadata": {},
   "source": [
    "## 1 - Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46945d",
   "metadata": {},
   "source": [
    "En LangChain, la memoria es un aspecto fundamental de las interfaces conversacionales, ya que permite que los sistemas hagan referencia a interacciones pasadas. Esto se logra a través del almacenamiento y la consulta de información, con dos acciones principales: lectura y escritura. El sistema de memoria interactúa con un sistema dos veces durante una ejecución, ampliando las entradas del usuario y almacenando las entradas y salidas para futuras referencias.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Incorporando memoria en un sistema**\n",
    "\n",
    "+ Almacenamiento de mensajes de chat: El módulo de memoria de LangChain integra varios métodos para almacenar mensajes de chat, que van desde listas en memoria hasta bases de datos. Esto asegura que todas las interacciones de chat se registren para referencia futura.\n",
    "\n",
    "+ Consulta de mensajes de chat: Más allá de almacenar los mensajes, LangChain emplea estructuras de datos y algoritmos para crear una vista útil de esos mensajes. Los sistemas de memoria simples pueden devolver mensajes recientes, mientras que los sistemas más avanzados pueden resumir interacciones pasadas o centrarse en entidades mencionadas en la interacción actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103c932",
   "metadata": {},
   "source": [
    "## 2 - Tipos de memoria en LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93011853",
   "metadata": {},
   "source": [
    "LangChain ofrece varios tipos de memoria que se pueden utilizar para mejorar las interacciones con los modelos de IA. Cada tipo de memoria tiene sus propios parámetros y tipos de retorno, lo que los hace adecuados para diferentes escenarios. Exploremos algunos de los tipos de memoria disponibles en LangChain junto con ejemplos de código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e34cf8",
   "metadata": {},
   "source": [
    "### 2.1 - ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ae81d",
   "metadata": {},
   "source": [
    "Este tipo de memoria nos permite almacenar y extraer mensajes de conversaciones. Podemos extraer el historial como una cadena de texto o como una lista de mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e63d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quito warnings \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb20dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el objeto de memoria\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37043d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_7058/676249865.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "# memoria con salida de string\n",
    "\n",
    "memoria = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c4999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado de mensajes\n",
    "\n",
    "mensaje_usuario = {'input': 'hola'}\n",
    "\n",
    "\n",
    "mensaje_asistente = {'output': 'hola, ¿como estás?'}\n",
    "\n",
    "\n",
    "memoria.save_context(mensaje_usuario, mensaje_asistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d1be3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hola\\nAI: hola, ¿como estás?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lectura de mensajes\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fea11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memoria con salida de lista de mensajes\n",
    "\n",
    "memoria = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7716a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado de mensajes\n",
    "\n",
    "mensaje_usuario = {'input': 'hola'}\n",
    "\n",
    "\n",
    "mensaje_asistente = {'output': 'hola, ¿como estás?'}\n",
    "\n",
    "\n",
    "memoria.save_context(mensaje_usuario, mensaje_asistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09c0161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hola', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='hola, ¿como estás?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lectura de mensajes\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df40d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memoria con salida de lista de mensajes y cambio de key\n",
    "\n",
    "memoria = ConversationBufferMemory(return_messages=True, memory_key='historial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0f14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado de mensajes\n",
    "\n",
    "mensaje_usuario = {'input': 'hola'}\n",
    "\n",
    "\n",
    "mensaje_asistente = {'output': 'hola, ¿como estás?'}\n",
    "\n",
    "\n",
    "memoria.save_context(mensaje_usuario, mensaje_asistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f5e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'historial': [HumanMessage(content='hola', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='hola, ¿como estás?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lectura de mensajes\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348c01a",
   "metadata": {},
   "source": [
    "### 2.2 - ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d825747",
   "metadata": {},
   "source": [
    "Este tipo de memoria mantiene una lista de interacciones recientes y utiliza las últimas K interacciones, evitando que el búfer se vuelva demasiado grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc903574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos el objeto de memoria\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb71295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_7058/814423048.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationBufferWindowMemory(k=1,\n"
     ]
    }
   ],
   "source": [
    "# memoria que solo recuerda el ultimo dialogo\n",
    "\n",
    "memoria = ConversationBufferWindowMemory(k=1, \n",
    "                                         return_messages=True,\n",
    "                                         memory_key='historial'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4925f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado de los primeros mensajes \n",
    "\n",
    "mensaje_usuario = {'input': 'hola'}\n",
    "\n",
    "mensaje_asistente = {'output': 'hola, ¿como estás?'}\n",
    "\n",
    "memoria.save_context(mensaje_usuario, mensaje_asistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772d5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado de los segundos mensajes \n",
    "\n",
    "mensaje_usuario = {'input': 'yo bien, ¿y tú?'}\n",
    "\n",
    "mensaje_asistente = {'output': 'estoy bien, gracias'}\n",
    "\n",
    "memoria.save_context(mensaje_usuario, mensaje_asistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3a3e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'historial': [HumanMessage(content='yo bien, ¿y tú?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='estoy bien, gracias', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lectura de mensajes\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8e603",
   "metadata": {},
   "source": [
    "### 2.3 - ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e31a2d",
   "metadata": {},
   "source": [
    "Este tipo de memoria recuerda hechos sobre entidades específicas en una conversación y extrae información utilizando un LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "746b0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero cargamos la API KEY de OpenAI\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7796db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos memoria y llm\n",
    "\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f4078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define el llm\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfd0a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_7058/2068164298.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationEntityMemory(llm=llm)\n",
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/pydantic/main.py:212: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "# memoria con entidad \n",
    "\n",
    "memoria = ConversationEntityMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a3bf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardado de los primeros mensajes \n",
    "\n",
    "mensaje_usuario = {'input': 'Pepe y Maria estan trabajando en un proyecto.'}\n",
    "\n",
    "memoria.load_memory_variables(mensaje_usuario)\n",
    "\n",
    "mensaje_asistente = {'output': 'Genial, ¿que tipo de proyecto?'}\n",
    "\n",
    "memoria.save_context(mensaje_usuario, mensaje_asistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa49b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Pepe y Maria estan trabajando en un proyecto.\\nAI: Genial, ¿que tipo de proyecto?',\n",
       " 'entities': {'Pepe': 'Pepe está trabajando en un proyecto con María.'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoria.load_memory_variables({'input': 'quien es Pepe?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503b709",
   "metadata": {},
   "source": [
    "### 2.4 - Conversation Knowledge Graph Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cbb85",
   "metadata": {},
   "source": [
    "Este tipo de memoria utiliza un grafo de conocimiento para recrear la memoria. Podemos extraer las entidades actuales y tríos de conocimiento a partir de los mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dd21fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos memoria y llm\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "260facdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define el llm\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8356cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memoria con entidad \n",
    "\n",
    "memoria = ConversationKGMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2e2e684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'On Pepe: Pepe es un amigo.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso de la memoria\n",
    "\n",
    "memoria.save_context({'input': 'di hola a Pepe'}, {'output': '¿quien es Pepe?'})\n",
    "\n",
    "memoria.save_context({'input': 'Pepe es un amigo'}, {'output': 'vale'})\n",
    "\n",
    "memoria.load_memory_variables({'input': '¿quien es Pepe?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e007ee",
   "metadata": {},
   "source": [
    "### 2.5 - ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918943b9",
   "metadata": {},
   "source": [
    "Este tipo de memoria crea un resumen de la conversación a lo largo del tiempo, lo cual es útil para condensar información de conversaciones más largas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f66ca705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos memoria y llm\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fee212e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define el llm\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "577df5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_7058/739089586.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "# memoria de resumen\n",
    "\n",
    "memoria = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0952682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human greets someone named Pepe and the AI asks who Pepe is. The human explains that Pepe is a friend and the AI responds with \"vale.\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso de la memoria\n",
    "\n",
    "memoria.save_context({'input': 'di hola a Pepe'}, {'output': '¿quien es Pepe?'})\n",
    "\n",
    "memoria.save_context({'input': 'Pepe es un amigo'}, {'output': 'vale'})\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473cefc",
   "metadata": {},
   "source": [
    "### 2.6 - ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7ff9d",
   "metadata": {},
   "source": [
    "Este tipo de memoria combina el resumen de la conversación y el búfer, manteniendo un equilibrio entre las interacciones recientes y un resumen. Utiliza la longitud de los tokens para determinar cuándo vaciar las interacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "261cda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos memoria y llm\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "add91050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define el llm\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "789f1ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_7058/2811187970.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)\n"
     ]
    }
   ],
   "source": [
    "# memoria \n",
    "\n",
    "memoria = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d538001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greets the AI and says hello to Pepe. The AI asks who Pepe is.\\nHuman: Pepe es un amigo\\nAI: vale'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso de la memoria\n",
    "\n",
    "memoria.save_context({'input': 'di hola a Pepe'}, {'output': '¿quien es Pepe?'})\n",
    "\n",
    "memoria.save_context({'input': 'Pepe es un amigo'}, {'output': 'vale'})\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70518ea5",
   "metadata": {},
   "source": [
    "### 2.7 - ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53f3c0",
   "metadata": {},
   "source": [
    "Este es otro tipo de memoria que mantiene un búfer de interacciones recientes en la memoria. A diferencia de los tipos de memoria anteriores que se centran en la cantidad de interacciones, este utiliza la longitud de los tokens para determinar cuándo vaciar las interacciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05d1b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos memoria y llm\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b229670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define el llm\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c36199f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/8cd72b4n08vdtd2xm8zrhvm80000gq/T/ipykernel_7058/3081612018.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memoria = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n"
     ]
    }
   ],
   "source": [
    "# memoria \n",
    "\n",
    "memoria = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec4903ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Pepe es un amigo\\nAI: vale'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso de la memoria\n",
    "\n",
    "memoria.save_context({'input': 'di hola a Pepe'}, {'output': '¿quien es Pepe?'})\n",
    "\n",
    "memoria.save_context({'input': 'Pepe es un amigo'}, {'output': 'vale'})\n",
    "\n",
    "memoria.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3571332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
