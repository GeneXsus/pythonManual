{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e042afd",
   "metadata": {},
   "source": [
    "# 1 - LangServe\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/langchain.jpeg\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c11fd",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---¿Qué-es-LangServe?\" data-toc-modified-id=\"1---¿Qué-es-LangServe?-1\">1 - ¿Qué es LangServe?</a></span></li><li><span><a href=\"#2---Servidor\" data-toc-modified-id=\"2---Servidor-2\">2 - Servidor</a></span></li><li><span><a href=\"#3---Cliente\" data-toc-modified-id=\"3---Cliente-3\">3 - Cliente</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef819a",
   "metadata": {},
   "source": [
    "## 1 - ¿Qué es LangServe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c25aa",
   "metadata": {},
   "source": [
    "LangServe ayuda a los desarrolladores a desplegar runnables y cadenas de LangChain como una API REST. Esta biblioteca está integrada con FastAPI y utiliza pydantic para la validación de datos. Además, proporciona un cliente que se puede utilizar para llamar a runnables desplegados en un servidor, y también está disponible un cliente en JavaScript en LangChainJS. Para usar esta librería debemos ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "pip install \"langserve[all]\"\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Características**:\n",
    "\n",
    "+ Los esquemas de entrada y salida se infieren automáticamente del objeto LangChain y se aplican en cada llamada a la API, con mensajes de error detallados.\n",
    "\n",
    "+ Está disponible una página de documentación de API con JSONSchema y Swagger.\n",
    "\n",
    "+ Endpoints eficientes /invoke, /batch y /stream con soporte para muchas solicitudes concurrentes en un solo servidor.\n",
    "\n",
    "+ Endpoint /stream_log para transmitir todos o algunos de los pasos intermedios de la cadena o el agente.\n",
    "\n",
    "+ Página de playground en /playground con salida en tiempo real y pasos intermedios.\n",
    "\n",
    "+ Todo construido con bibliotecas de Python de código abierto probadas en batalla como FastAPI o asyncio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa1694",
   "metadata": {},
   "source": [
    "## 2 - Servidor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d249993",
   "metadata": {},
   "source": [
    "Para crear un API con LangServe, primero tenemos que crear el código del servidor, el cual es necesario ejecutar en un archivo `.py` para realizar el despliegue. Vamos a crear una API con LangServe usando una cadena sencilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b0818",
   "metadata": {},
   "source": [
    "```python\n",
    "# librerias\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from langserve import add_routes\n",
    "\n",
    "\n",
    "\n",
    "# carga de variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# api key openai, nombre que tiene por defecto en LangChain\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "# creacion de la aplicacion de FastAPI\n",
    "app = FastAPI(title='Servidor LangChain', \n",
    "              version='1.0',\n",
    "              description='Simple API con LangChain')\n",
    "\n",
    "\n",
    "# inicio del modelo\n",
    "modelo = ChatOpenAI()\n",
    "\n",
    "\n",
    "# creacion del prompt\n",
    "plantilla = 'Eres un asistente personal que responde la siguiente pregunta de la mejor manera: {pregunta}'\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(plantilla)\n",
    "\n",
    "\n",
    "# se define la cadena\n",
    "cadena = prompt | modelo\n",
    "\n",
    "# se añade el endpoint a la aplicacion\n",
    "add_routes(app, cadena, path='/cadena')\n",
    "\n",
    "\n",
    "# ejecucion de la API\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host='localhost', port=8000)\n",
    "  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeaef14",
   "metadata": {},
   "source": [
    "## 3 - Cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d973d9c",
   "metadata": {},
   "source": [
    "Para usar la API como cliente, una vez levantado el servidor, podemos usar el playground desde nuestro navegador usando la siguiente url:\n",
    "\n",
    "http://localhost:8000/cadena/playground/\n",
    "\n",
    "<br>\n",
    "\n",
    "También podemos realizar la llamada la servidor desde una terminal usando curl de la siguiente manera:\n",
    "\n",
    "```bash\n",
    "curl --location --request POST 'http://localhost:8000/cadena/invoke/' \\\n",
    "     --header 'Content-Type: application/json' \\\n",
    "     --data-raw '{\"input\": {\"pregunta\": \"hola\"}}'\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Incluso podriamos hacer un request desde Python con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f1ac9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': {'content': '¡Hola! ¿En qué puedo ayudarte hoy?',\n",
       "  'additional_kwargs': {'refusal': None},\n",
       "  'response_metadata': {'token_usage': {'completion_tokens': 11,\n",
       "    'prompt_tokens': 27,\n",
       "    'total_tokens': 38,\n",
       "    'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       "    'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}},\n",
       "   'model_name': 'gpt-3.5-turbo-0125',\n",
       "   'system_fingerprint': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'logprobs': None},\n",
       "  'type': 'ai',\n",
       "  'name': None,\n",
       "  'id': 'run-07c3cd3b-01be-4472-bd81-73f5d3b9e3b4-0',\n",
       "  'example': False,\n",
       "  'tool_calls': [],\n",
       "  'invalid_tool_calls': [],\n",
       "  'usage_metadata': {'input_tokens': 27,\n",
       "   'output_tokens': 11,\n",
       "   'total_tokens': 38,\n",
       "   'input_token_details': {'cache_read': 0},\n",
       "   'output_token_details': {'reasoning': 0}}},\n",
       " 'metadata': {'run_id': '6513a484-038a-4a83-bb06-08087489c8b2',\n",
       "  'feedback_tokens': []}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as req\n",
    "\n",
    "endpoint = 'http://localhost:8000/cadena/invoke/'\n",
    "\n",
    "pregunta = 'hola'\n",
    "\n",
    "respuesta = req.post(endpoint, json={'input': {'pregunta': pregunta}})\n",
    "\n",
    "respuesta.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d8dde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# respuesta formato string\n",
    "\n",
    "respuesta.json()['output']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
