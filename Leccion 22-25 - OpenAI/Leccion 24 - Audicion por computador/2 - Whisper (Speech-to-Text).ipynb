{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20956dd",
   "metadata": {},
   "source": [
    "# 2 - Whisper (Speech-to-Text)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/openai_whisper.webp\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb028ceb",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelo-Whisper\" data-toc-modified-id=\"1---Modelo-Whisper-1\">1 - Modelo Whisper</a></span></li><li><span><a href=\"#2---Transcripción\" data-toc-modified-id=\"2---Transcripción-2\">2 - Transcripción</a></span></li><li><span><a href=\"#3---Timestamps\" data-toc-modified-id=\"3---Timestamps-3\">3 - Timestamps</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1---Creación-de-subtitulos\" data-toc-modified-id=\"3.1---Creación-de-subtitulos-3.1\">3.1 - Creación de subtitulos</a></span></li></ul></li><li><span><a href=\"#4---Traducción\" data-toc-modified-id=\"4---Traducción-4\">4 - Traducción</a></span></li><li><span><a href=\"#5---Prompting\" data-toc-modified-id=\"5---Prompting-5\">5 - Prompting</a></span></li><li><span><a href=\"#6---API-de-Whisper\" data-toc-modified-id=\"6---API-de-Whisper-6\">6 - API de Whisper</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d7ead",
   "metadata": {},
   "source": [
    "## 1 - Modelo Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f47f2",
   "metadata": {},
   "source": [
    "El modelo Whisper de OpenAI es una tecnología avanzada de reconocimiento automático de voz, ASR, por sus siglas en inglés, diseñada para transcribir y traducir audio en texto de manera precisa y eficiente. \n",
    "\n",
    "**Características del modelo Whisper**\n",
    "\n",
    "\n",
    "1. **Reconocimiento de voz de alta precisión**: Whisper utiliza técnicas avanzadas de aprendizaje profundo para transcribir el habla humana con alta precisión. Es capaz de manejar diferentes acentos, dialectos y variaciones en la pronunciación.\n",
    "\n",
    "\n",
    "2. **Soporte multilingüe**: El modelo está diseñado para trabajar con múltiples idiomas, lo que permite transcribir y traducir audio en diferentes lenguas. Esto es especialmente útil para aplicaciones globales y servicios que requieren soporte en varios idiomas.\n",
    "\n",
    "\n",
    "3. **Robustez en entornos ruidosos**: Whisper es capaz de transcribir con precisión incluso en entornos ruidosos, lo que lo hace ideal para aplicaciones en el mundo real donde el ruido de fondo puede ser un desafío.\n",
    "\n",
    "\n",
    "4. **Transcripción en tiempo real**: Puede proporcionar transcripciones en tiempo real, lo que es beneficioso para aplicaciones como subtítulos en vivo, asistentes virtuales y servicios de atención al cliente.\n",
    "\n",
    "\n",
    "5. **Capacidad de traducción**: Además de transcribir el audio, Whisper puede traducir automáticamente el texto transcrito a otros idiomas, facilitando la comunicación multilingüe.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Aplicaciones del modelo Whisper**\n",
    "\n",
    "\n",
    "1. **Subtitulación y traducción en tiempo real**: Utilizado para generar subtítulos en vivo para videos, conferencias y eventos en múltiples idiomas, mejorando la accesibilidad y la comprensión.\n",
    "\n",
    "\n",
    "2. **Asistentes virtuales y chatbots**: Integrado en asistentes virtuales para mejorar la interacción con los usuarios a través del reconocimiento y la comprensión del habla.\n",
    "\n",
    "\n",
    "3. **Atención al cliente**: Utilizado en centros de atención al cliente para transcribir y analizar llamadas telefónicas, mejorando la calidad del servicio y la satisfacción del cliente.\n",
    "\n",
    "\n",
    "4. **Educación y E-Learning**: Aplicado en plataformas educativas para transcribir conferencias y proporcionar subtítulos precisos, facilitando el aprendizaje y el acceso a la información.\n",
    "\n",
    "\n",
    "5. **Investigación y análisis**: Empleado en la investigación para transcribir entrevistas, discursos y debates, facilitando el análisis de datos y la recopilación de información.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Ventajas del modelo Whisper**\n",
    "\n",
    "+ Alta precisión y fiabilidad: Whisper ofrece transcripciones precisas y confiables, reduciendo los errores comunes en el reconocimiento de voz.\n",
    "+ Versatilidad multilingüe: Su capacidad para manejar múltiples idiomas y dialectos lo hace ideal para aplicaciones globales.\n",
    "+ Robustez en diversos entornos: Puede funcionar eficazmente en entornos ruidosos, lo que lo hace útil para aplicaciones en el mundo real.\n",
    "+ Facilidad de integración: Puede integrarse fácilmente en diversas aplicaciones y servicios, mejorando la funcionalidad y la accesibilidad.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Desafíos y consideraciones**\n",
    "\n",
    "+ Privacidad y seguridad: Al manejar datos de audio, es crucial asegurar la privacidad y la seguridad de la información del usuario.\n",
    "+ Adaptación a contextos específicos: Puede ser necesario ajustar y entrenar el modelo para contextos y vocabularios específicos para maximizar la precisión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1da82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías, API KEY e iniciamos cliente\n",
    "\n",
    "import os                           \n",
    "from dotenv import load_dotenv \n",
    "import openai as ai\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "cliente = ai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6f9a1",
   "metadata": {},
   "source": [
    "## 2 - Transcripción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef891c2b",
   "metadata": {},
   "source": [
    "La API de transcripciones toma como entrada el archivo de audio que deseamos transcribir y el modelo usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edb79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 7.82 ms, total: 18.7 ms\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extracción del texto\n",
    "\n",
    "audio = open('../../files/Will AI kill everyone Heres what the godfathers of AI have to say.mp4', 'rb')\n",
    "\n",
    "transcripcion = cliente.audio.transcriptions.create(model='whisper-1', \n",
    "                                                    file=audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e01c996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The ACM Turing Award is the highest distinction in computer science, comparable to the Nobel Prize. In 2018, it was awarded to three pioneers of the deep learning revolution, Geoffrey Hinton, Yoshua Bengio, and Yan LeCun. In May 2023, Geoffrey Hinton left Google so that he could speak openly about the dangers of advanced AI, agreeing that, quote, it could figure out how to kill humans, and saying, it's not clear to me that we can solve this problem. Later that month, Yoshua Bengio wrote a blog post titled How Rogue AIs May Arise, in which he defined a rogue AI as an autonomous AI system that could behave in ways that would be catastrophically harmful to a large fraction of humans, potentially endangering our societies, and even our species or the biosphere. Yan LeCun continues to refer to those suggesting that we're facing severe and imminent risk as professional scaremongers, and says it's a simple fact that the people who are terrified of AGI are rarely the people who actually build AI models. LeCun is a highly accomplished researcher, but in light of Bengio and Hinton's recent comments, it's clear that he's misrepresenting the field, whether he realizes it or not. There is not a consensus among professional researchers that AI research is safe. Rather, there is considerable and growing concern that advanced AI could pose extreme risks, and this concern is shared by not only both of LeCun's award co-recipients, but the heads of all three leading AI labs, OpenAI, Anthropic, and Google DeepMind. Demis Hassabis, CEO of DeepMind, said in an interview with Time magazine, When it comes to very powerful technologies, and obviously AI is going to be one of the most powerful ever, we need to be careful. Not everybody is thinking about those things. It's like experimentalists, many of whom don't realize they're holding dangerous material. Anthropic, in their public statement, Core Views on AI Safety, says, One particularly important dimension of uncertainty is how difficult it will be to develop advanced AI systems that are broadly safe and pose little risk to humans. Developing such systems could lie anywhere on the spectrum from very easy to impossible. And OpenAI, in their blog post, Planning for AGI and Beyond, says, But we're going to operate as if these risks are existential. Sam Altman, the current CEO of OpenAI, once said, There are objections one could raise to the idea that advanced AI poses significant risk to humanity. But it's a fringe idea that actual AI experts do not take seriously is no longer among them. Instead, a growing share of experts are echoing the conclusion reached by Alan Turing, considered by many to be the father of computer science and artificial intelligence, back in 1951. It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. At some stage, therefore, we should have to expect the machines to take control. OpenAI OpenSciences OpenSciences.org OpenSciences.org\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texto extraido del audio\n",
    "\n",
    "transcripcion.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef613dcc",
   "metadata": {},
   "source": [
    "## 3 - Timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888230d",
   "metadata": {},
   "source": [
    "También podemos pedirle al modelo que nos de el tiempo en que se ha dicho cada palabra o cada frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9974167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 6.31 ms, total: 16.8 ms\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transcripcion = cliente.audio.transcriptions.create(model='whisper-1', \n",
    "                                                    file=audio,\n",
    "                                                    response_format='verbose_json',\n",
    "                                                    timestamp_granularities=['segment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a7ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'seek': 0, 'start': 0.0, 'end': 5.0, 'text': ' The ACM Turing Award is the highest distinction in computer science,', 'tokens': [50364, 440, 8157, 44, 314, 1345, 13894, 307, 264, 6343, 16844, 294, 3820, 3497, 11, 50614], 'temperature': 0.0, 'avg_logprob': -0.2398335486650467, 'compression_ratio': 1.4808510541915894, 'no_speech_prob': 0.4072512686252594}, {'id': 1, 'seek': 0, 'start': 5.0, 'end': 7.0, 'text': ' comparable to the Nobel Prize.', 'tokens': [50614, 25323, 281, 264, 24611, 22604, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.2398335486650467, 'compression_ratio': 1.4808510541915894, 'no_speech_prob': 0.4072512686252594}]\n"
     ]
    }
   ],
   "source": [
    "# segmentos de texto con sus tiempos\n",
    "\n",
    "print(transcripcion.segments[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fce11",
   "metadata": {},
   "source": [
    "### 3.1 - Creación de subtitulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99603bff",
   "metadata": {},
   "source": [
    "Con estos datos podemos crear un archivo `.srt`, el archivo de subtítulos para un video. Este tipo de archivo tiene una estructura determinada para poder ser usado en los reproductores de video. Veamos como construir este tipo de archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267c255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def crear_subtitulos(segmentos: list) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función crea un archivo de subtitulos con la transcripción del audio.\n",
    "    \n",
    "    Params:\n",
    "    segmentos: list, lista de diccionarios con la transcripción y tiempos\n",
    "    \n",
    "    Return:\n",
    "    No devuelve nada, solo guarda el archivo\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for i,s in enumerate(segmentos):\n",
    "        \n",
    "        start = str(0)+str(timedelta(seconds=int(s['start'])))+',000'\n",
    "        end = str(0)+str(timedelta(seconds=int(s['end'])))+',000'\n",
    "        \n",
    "        texto = s['text']\n",
    "        \n",
    "        segmento = f\"{i+1}\\n{start} --> {end}\\n{texto[1:] if texto[0]==' ' else texto}\\n\\n\"\n",
    "\n",
    "        with open('../../files/subtitulos.srt', 'a', encoding='utf-8') as file:\n",
    "            file.write(segmento)\n",
    "            \n",
    "    print('Hecho!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd20b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hecho!\n"
     ]
    }
   ],
   "source": [
    "# llamada función\n",
    "\n",
    "crear_subtitulos(transcripcion.segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ffb79",
   "metadata": {},
   "source": [
    "## 4 - Traducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0bb653",
   "metadata": {},
   "source": [
    "La API de traducciones toma como entrada el archivo de audio en cualquiera de los idiomas compatibles y transcribe, si es necesario, el audio al inglés. Esto difiere de nuestro endpoint de transcripciones, ya que la salida no está en el idioma original de entrada, sino que se traduce a texto en inglés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aefa95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga del archivo de audio\n",
    "\n",
    "ruta = '../../files/Es posible que una IA.m4a'\n",
    "\n",
    "audio= open(ruta, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8f1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traducción es-en\n",
    "\n",
    "translation = cliente.audio.translations.create(model='whisper-1', \n",
    "                                                file=audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c21752ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Javi, do you think that in less than 30 years, an AI with a personality is going to be born that is not expected, that is, consciousness? First, I think there is a bubble with the whole issue of artificial intelligence. I think it is being exaggerated. I am going to refer to a book that I read recently, the myth of artificial intelligence, that the human being has three classic ways of generating inferences and with it knowledge. Deduction, induction and abduction. Artificial intelligence works with induction, in the sense that what it does is connect to a very large network of data and it is looking for patterns and finds outputs according to what it sees. The human being has a third ability, which is abduction, but you know that you tell a joke to an AI and it does not understand. It does not understand that the water gets wet, it does not understand that winter is cold, that is, it does not understand things that for us are natural because they are life experiences. If we continue on that line, it will get better at what it does and it will be a machine and it will do spectacular things. But on that line, what this book says is that if you want to be human, you will have to create another type of way of creating and programming.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texto traducido\n",
    "\n",
    "translation.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353d987",
   "metadata": {},
   "source": [
    "## 5 - Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c1dae",
   "metadata": {},
   "source": [
    "Podemos utilizar un prompt para mejorar la calidad de las transcripciones generadas por la API de Whisper. El modelo intentará igualar el estilo del prompt, por lo que será más probable que use mayúsculas y puntuación si el prompt también lo hace. Sin embargo, el sistema de prompting actual es mucho más limitado que nuestros otros modelos de lenguaje y solo proporciona un control limitado sobre el audio generado. Aquí hay algunos ejemplos de cómo el prompting puede ayudar en diferentes escenarios:\n",
    "\n",
    "1. Los prompts pueden ser muy útiles para corregir palabras específicas o acrónimos que el modelo puede reconocer incorrectamente en el audio. Por ejemplo, el siguiente prompt mejora la transcripción de las palabras DALL·E y GPT-3, que anteriormente se escribieron como \"GDP 3\" y \"DALI\": \"La transcripción trata sobre OpenAI, que crea tecnología como DALL·E, GPT-3 y ChatGPT con la esperanza de algún día construir un sistema de AGI que beneficie a toda la humanidad.\".\n",
    "\n",
    "2. Para preservar el contexto de un archivo que se dividió en segmentos, podemos hacer un prompt al modelo con la transcripción del segmento anterior. Esto hará que la transcripción sea más precisa, ya que el modelo usará la información relevante del audio anterior. El modelo solo considerará los últimos 224 tokens del prompt e ignorará cualquier cosa anterior. Para entradas multilingües, Whisper utiliza un tokenizador personalizado. Para entradas solo en inglés, utilizamos el tokenizador estándar de GPT-2, ambos accesibles a través del paquete de Python de código abierto Whisper.\n",
    "\n",
    "3. A veces, el modelo puede omitir la puntuación en la transcripción. Podemos evitar esto utilizando un prompt simple que incluya puntuación.\n",
    "\n",
    "4. El modelo también puede omitir palabras de relleno comunes en el audio. Si deseamos mantener las palabras de relleno en nuestra transcripción, podemos utilizar un prompt que las contenga.\n",
    "\n",
    "5. Algunos idiomas pueden escribirse de diferentes maneras, como el chino simplificado o tradicional. El modelo puede no usar siempre el estilo de escritura que deseas para tu transcripción de forma predeterminada. Puedes mejorar esto utilizando un prompt en tu estilo de escritura preferido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a92f2",
   "metadata": {},
   "source": [
    "## 6 - API de Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e52dc",
   "metadata": {},
   "source": [
    "[Whisper](https://github.com/openai/whisper) está disponible en código abierto como una librería independiente de Python. Para usarla, priero tenemos que ejecutar el siguiente comando:\n",
    "\n",
    "```bash\n",
    "pip install openai-whisper\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "855a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la librería\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31788106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se carga el modelo base de Whisper en local, 139M\n",
    "\n",
    "modelo_whisper = whisper.load_model('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea632a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ia/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# cargamos el archivo de audio \n",
    "\n",
    "import librosa\n",
    "\n",
    "data, freq = librosa.load(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "634521ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper transcribe el audio a texto, en 32 bits (fp16=False)\n",
    "\n",
    "transcripcion = modelo_whisper.transcribe(data, fp16=False)[\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74aa0a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Javi, ¿tú crees que en menos de 30 años va a hacer una ia con una personalidad que no sea esperada? Es decir, conscientia. Primero, creo que no hubo un aguujo a control tema de interjece artificial. Creo que se está exagerando. Me voy a señir a un libro que me leía a ese poco, con el mito de la interjece artificial, que el ser humano tiene tres formas clásicas de generar inferencias y con ello con los humientos. La deducción, la inducción y la abducción. La interjece artificial funciona con la inducción. En el término de que lo que hace es se conecta una muy grande datos y va buscando patrones y encontrar salida conforme a lo que ve. El ser humano tiene una terza capacidad que la abducción, ya que le cuentas un chiste a una interjece artificial y no entiende. No entiende que el agua moja, no entiende que el invierro frío. O sea, no entiende cosas que para nosotros son naturales porque son experiencia de vida. Si seguimos en esa línea, cada vez será mejor en lo que hace. Sí. Y será una máquina y hará cosas espectaculares. Pero en esa línea no va lo que hiciste en el libro es, si quieres ser humano, va a tener que crear otro tipo de forma de crearse y de programarse.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texto \n",
    "\n",
    "transcripcion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "684.436px",
    "left": "67px",
    "top": "110.113px",
    "width": "335.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
