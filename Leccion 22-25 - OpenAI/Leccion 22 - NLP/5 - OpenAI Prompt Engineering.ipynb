{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae6c460",
   "metadata": {},
   "source": [
    "# 5 - OpenAI Prompt Engineering \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/openai_prompt.webp\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc2282",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---¿Qué-es-el-prompt-engineering?\" data-toc-modified-id=\"1---¿Qué-es-el-prompt-engineering?-1\">1 - ¿Qué es el prompt engineering?</a></span></li><li><span><a href=\"#2---Estrategias-para-obtener-mejores-resultados\" data-toc-modified-id=\"2---Estrategias-para-obtener-mejores-resultados-2\">2 - Estrategias para obtener mejores resultados</a></span></li><li><span><a href=\"#3---Escribir-instrucciones-claras\" data-toc-modified-id=\"3---Escribir-instrucciones-claras-3\">3 - Escribir instrucciones claras</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1---Incluye-detalles-en-la-consulta-para-obtener-respuestas-más-relevantes\" data-toc-modified-id=\"3.1---Incluye-detalles-en-la-consulta-para-obtener-respuestas-más-relevantes-3.1\">3.1 - Incluye detalles en la consulta para obtener respuestas más relevantes</a></span></li><li><span><a href=\"#3.2---Pedir-al-modelo-que-adopte-una-persona\" data-toc-modified-id=\"3.2---Pedir-al-modelo-que-adopte-una-persona-3.2\">3.2 - Pedir al modelo que adopte una persona</a></span></li><li><span><a href=\"#3.3---Usar-delimitadores-para-indicar-claramente-las-partes-distintas-de-la-entrada\" data-toc-modified-id=\"3.3---Usar-delimitadores-para-indicar-claramente-las-partes-distintas-de-la-entrada-3.3\">3.3 - Usar delimitadores para indicar claramente las partes distintas de la entrada</a></span></li><li><span><a href=\"#3.4---Especificar-los-pasos-necesarios-para-completar-una-tarea\" data-toc-modified-id=\"3.4---Especificar-los-pasos-necesarios-para-completar-una-tarea-3.4\">3.4 - Especificar los pasos necesarios para completar una tarea</a></span></li><li><span><a href=\"#3.5---Proporcionar-ejemplos\" data-toc-modified-id=\"3.5---Proporcionar-ejemplos-3.5\">3.5 - Proporcionar ejemplos</a></span></li><li><span><a href=\"#3.6---Especificar-la-longitud-deseada-de-la-salida\" data-toc-modified-id=\"3.6---Especificar-la-longitud-deseada-de-la-salida-3.6\">3.6 - Especificar la longitud deseada de la salida</a></span></li></ul></li><li><span><a href=\"#4---Proporcionar-texto-de-referencia\" data-toc-modified-id=\"4---Proporcionar-texto-de-referencia-4\">4 - Proporcionar texto de referencia</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1---Instruir-al-modelo-para-que-responda-utilizando-un-texto-de-referencia\" data-toc-modified-id=\"4.1---Instruir-al-modelo-para-que-responda-utilizando-un-texto-de-referencia-4.1\">4.1 - Instruir al modelo para que responda utilizando un texto de referencia</a></span></li><li><span><a href=\"#4.2---Instruir-al-modelo-para-que-responda-con-citas-de-un-texto-de-referencia\" data-toc-modified-id=\"4.2---Instruir-al-modelo-para-que-responda-con-citas-de-un-texto-de-referencia-4.2\">4.2 - Instruir al modelo para que responda con citas de un texto de referencia</a></span></li></ul></li><li><span><a href=\"#5---Dividir-tareas-complejas-en-subtareas-más-simples\" data-toc-modified-id=\"5---Dividir-tareas-complejas-en-subtareas-más-simples-5\">5 - Dividir tareas complejas en subtareas más simples</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1---Usar-clasificación-de-intenciones-para-identificar-las-instrucciones-más-relevantes-para-una-consulta\" data-toc-modified-id=\"5.1---Usar-clasificación-de-intenciones-para-identificar-las-instrucciones-más-relevantes-para-una-consulta-5.1\">5.1 - Usar clasificación de intenciones para identificar las instrucciones más relevantes para una consulta</a></span></li><li><span><a href=\"#5.2---Aplicaciones-de-diálogo-que-requieren-conversaciones-muy-largas,-resume-o-filtra-el-diálogo-previo\" data-toc-modified-id=\"5.2---Aplicaciones-de-diálogo-que-requieren-conversaciones-muy-largas,-resume-o-filtra-el-diálogo-previo-5.2\">5.2 - Aplicaciones de diálogo que requieren conversaciones muy largas, resume o filtra el diálogo previo</a></span></li><li><span><a href=\"#5.3---Resume-documentos-largos-por-partes-y-construye-un-resumen-completo-de-manera-recursiva\" data-toc-modified-id=\"5.3---Resume-documentos-largos-por-partes-y-construye-un-resumen-completo-de-manera-recursiva-5.3\">5.3 - Resume documentos largos por partes y construye un resumen completo de manera recursiva</a></span></li></ul></li><li><span><a href=\"#6---Dar-al-modelo-tiempo-para-&quot;pensar&quot;\" data-toc-modified-id=\"6---Dar-al-modelo-tiempo-para-&quot;pensar&quot;-6\">6 - Dar al modelo tiempo para \"pensar\"</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1---Instruir-al-modelo-para-que-resuelva-su-propia-solución-antes-de-apresurarse-a-una-conclusión\" data-toc-modified-id=\"6.1---Instruir-al-modelo-para-que-resuelva-su-propia-solución-antes-de-apresurarse-a-una-conclusión-6.1\">6.1 - Instruir al modelo para que resuelva su propia solución antes de apresurarse a una conclusión</a></span></li><li><span><a href=\"#6.2---Usar-monólogo-interno-o-secuencia-de-consultas-para-ocultar-el-proceso-de-razonamiento-del-modelo\" data-toc-modified-id=\"6.2---Usar-monólogo-interno-o-secuencia-de-consultas-para-ocultar-el-proceso-de-razonamiento-del-modelo-6.2\">6.2 - Usar monólogo interno o secuencia de consultas para ocultar el proceso de razonamiento del modelo</a></span></li><li><span><a href=\"#6.3---Preguntar-al-modelo-si-se-perdió-algo-en-pases-anteriores\" data-toc-modified-id=\"6.3---Preguntar-al-modelo-si-se-perdió-algo-en-pases-anteriores-6.3\">6.3 - Preguntar al modelo si se perdió algo en pases anteriores</a></span></li></ul></li><li><span><a href=\"#7---Usar-herramientas-externas\" data-toc-modified-id=\"7---Usar-herramientas-externas-7\">7 - Usar herramientas externas</a></span><ul class=\"toc-item\"><li><span><a href=\"#7.1---Usar-búsqueda-basada-en-embeddings-para-implementar-una-recuperación-de-conocimiento-eficiente\" data-toc-modified-id=\"7.1---Usar-búsqueda-basada-en-embeddings-para-implementar-una-recuperación-de-conocimiento-eficiente-7.1\">7.1 - Usar búsqueda basada en embeddings para implementar una recuperación de conocimiento eficiente</a></span></li><li><span><a href=\"#7.2---Usar-la-ejecución-de-código-para-realizar-cálculos-más-precisos-o-llamar-a-APIs-externas\" data-toc-modified-id=\"7.2---Usar-la-ejecución-de-código-para-realizar-cálculos-más-precisos-o-llamar-a-APIs-externas-7.2\">7.2 - Usar la ejecución de código para realizar cálculos más precisos o llamar a APIs externas</a></span></li></ul></li><li><span><a href=\"#8---Probar-los-cambios-sistemáticamente\" data-toc-modified-id=\"8---Probar-los-cambios-sistemáticamente-8\">8 - Probar los cambios sistemáticamente</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b25744",
   "metadata": {},
   "source": [
    "## 1 - ¿Qué es el prompt engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e4c5e",
   "metadata": {},
   "source": [
    "El prompt engineering es una práctica en el campo del procesamiento del lenguaje que se centra en la creación y optimización de las entradas (prompts) para modelos de IA. El objetivo principal del prompt engineering es diseñar prompts efectivos que guíen al modelo para generar respuestas precisas, relevantes y útiles.\n",
    "\n",
    "\n",
    "**Aspectos clave del prompt engineering**\n",
    "\n",
    "\n",
    "1. **Diseño de prompts**: Crear preguntas, declaraciones o contextos específicos que dirijan al modelo para obtener la respuesta deseada. Utilizar un lenguaje claro y preciso para evitar ambigüedades y mejorar la calidad de la respuesta.\n",
    "\n",
    "\n",
    "2. **Optimización de prompts**: Ajustar y refinar los prompts para mejorar la precisión y relevancia de las respuestas. Probar diferentes versiones de un prompt para ver cuál produce los mejores resultados.\n",
    "\n",
    "\n",
    "3. **Contexto y detalles**: Proporcionar suficiente contexto en el prompt para que el modelo comprenda mejor la solicitud. Incluir detalles específicos que pueden influir en la respuesta del modelo, como el tono, la longitud esperada de la respuesta, y el formato.\n",
    "\n",
    "\n",
    "4. **Estrategias comunes**:\n",
    "\n",
    "    + Zero-shot prompting: Hacer preguntas directas sin ejemplos previos.\n",
    "    + Few-shot prompting: Proporcionar ejemplos de cómo debería ser la respuesta deseada junto con la pregunta.\n",
    "    + Chain-of-thought prompting: Incluir un razonamiento paso a paso para guiar al modelo a través de un proceso de pensamiento lógico.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Beneficios del prompt engineering**\n",
    "\n",
    "+ Mejora la precisión: Ayuda a obtener respuestas más precisas y relevantes del modelo.\n",
    "+ Eficiencia: Reduce la necesidad de realizar múltiples consultas al modelo para obtener la respuesta deseada.\n",
    "+ Control: Proporciona un mayor control sobre el tipo de respuestas generadas por el modelo, lo cual es crucial para aplicaciones sensibles o de alta precisión.\n",
    "\n",
    "\n",
    "**Aplicaciones del prompt engineering**\n",
    "\n",
    "+ Asistentes virtuales: Mejorar las respuestas y la interacción con los usuarios.\n",
    "+ Generación de contenido: Crear artículos, descripciones y otros tipos de contenido escrito.\n",
    "+ Educación: Diseñar prompts que guíen a los estudiantes a través de problemas complejos.\n",
    "+ Investigación: Utilizar prompts para explorar respuestas detalladas y exhaustivas en temas específicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c9c51",
   "metadata": {},
   "source": [
    "## 2 - Estrategias para obtener mejores resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158961a0",
   "metadata": {},
   "source": [
    "\n",
    "1. **Escribir instrucciones claras**\n",
    "\n",
    "Los modelos no pueden leernos la mente. Si las respuestas son demasiado largas, debemos pedir respuestas breves. Si las respuestas son demasiado simples, pedimos una redacción a nivel experto. Si no nos gusta el formato, demuestramos el formato que nos gustaría ver. Cuanto menos tenga que adivinar el modelo lo que queremos, más probable es que lo obtengamos.\n",
    "\n",
    "Tácticas:\n",
    "\n",
    "+ Incluir detalles en nuestra consulta para obtener respuestas más relevantes.\n",
    "+ Pedir al modelo que adopte una persona.\n",
    "+ Usar delimitadores para indicar claramente las partes distintas de la entrada.\n",
    "+ Especificar los pasos necesarios para completar una tarea.\n",
    "+ Proporcionar ejemplos.\n",
    "+ Especificar la longitud deseada de la salida. \n",
    "\n",
    "\n",
    "2. **Proporcionar texto de referencia**\n",
    "\n",
    "Los modelos de lenguaje pueden inventar respuestas falsas con confianza, especialmente cuando se les pregunta sobre temas esotéricos o para citas y URLs. De la misma manera que una hoja de notas puede ayudar a un estudiante a rendir mejor en un examen, proporcionar texto de referencia a estos modelos puede ayudar a responder con menos fabricaciones.\n",
    "\n",
    "Tácticas:\n",
    "\n",
    "+ Instruir al modelo a responder usando un texto de referencia.\n",
    "+ Instruir al modelo a responder con citas de un texto de referencia.\n",
    "\n",
    "\n",
    "3. **Dividir tareas complejas en subtareas más simples**\n",
    "\n",
    "Así como es una buena práctica en la ingeniería de software descomponer un sistema complejo en un conjunto de componentes modulares, lo mismo ocurre con las tareas enviadas a un modelo de lenguaje. Las tareas complejas tienden a tener tasas de error más altas que las tareas más simples. Además, las tareas complejas a menudo pueden redefinirse como un flujo de trabajo de tareas más simples en las que las salidas de las primeras tareas se utilizan para construir las entradas de las tareas posteriores.\n",
    "\n",
    "Tácticas:\n",
    "\n",
    "+ Usar la clasificación de intenciones para identificar las instrucciones más relevantes para una consulta de usuario.\n",
    "+ Para aplicaciones de diálogo que requieren conversaciones muy largas, resume o filtra el diálogo previo.\n",
    "+ Resume documentos largos por partes y construye un resumen completo de manera recursiva.\n",
    "\n",
    "\n",
    "4. **Dar al modelo tiempo para \"pensar\"**\n",
    "\n",
    "Si nos pidieran multiplicar 17 por 28, puede que no lo sepamos instantáneamente, pero aún podemos resolverlo con tiempo. De manera similar, los modelos cometen más errores de razonamiento cuando intentan responder de inmediato, en lugar de tomarse el tiempo para resolver una respuesta. Pedir una \"cadena de pensamiento\" antes de una respuesta puede ayudar al modelo a razonar su camino hacia respuestas correctas de manera más confiable.\n",
    "\n",
    "Tácticas:\n",
    "\n",
    "+ Instruir al modelo a trabajar en su propia solución antes de apresurarse a una conclusión.\n",
    "+ Usar monólogo interno o una secuencia de consultas para ocultar el proceso de razonamiento del modelo.\n",
    "+ Preguntar al modelo si se perdió algo en pases anteriores.\n",
    "\n",
    "\n",
    "5. **Usar herramientas externas**\n",
    "\n",
    "Compensar las debilidades del modelo alimentándolo con las salidas de otras herramientas. Por ejemplo, un sistema de recuperación de texto, llamado RAG o generación aumentada por recuperación, puede decirle al modelo sobre documentos relevantes. Un motor de ejecución de código como el Intérprete de Código de OpenAI puede ayudar al modelo a hacer matemáticas y ejecutar código. Si una tarea puede hacerse de manera más confiable o eficiente por una herramienta en lugar de un modelo de lenguaje, podemos usarla para obtener lo mejor de ambos.\n",
    "\n",
    "Tácticas:\n",
    "\n",
    "+ Usar búsquedas basadas en embeddings para implementar una recuperación de conocimiento eficiente.\n",
    "+ Usar la ejecución de código para realizar cálculos más precisos o llamar a APIs externas.\n",
    "+ Dar al modelo acceso a funciones específicas.\n",
    "\n",
    "\n",
    "6. **Probar los cambios sistemáticamente**\n",
    "\n",
    "Mejorar el rendimiento es más fácil si podemos medirlo. En algunos casos, una modificación a un prompt logrará un mejor rendimiento en unos pocos ejemplos aislados pero llevará a un peor rendimiento general en un conjunto más representativo de ejemplos. Por lo tanto, para estar seguro de que un cambio es netamente positivo para el rendimiento, puede ser necesario definir una suite de pruebas integral.\n",
    "\n",
    "Táctica:\n",
    "\n",
    "+ Evaluar las salidas del modelo con referencia a respuestas estándar de oro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513378a5",
   "metadata": {},
   "source": [
    "## 3 - Escribir instrucciones claras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0374b",
   "metadata": {},
   "source": [
    "### 3.1 - Incluye detalles en la consulta para obtener respuestas más relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841902a0",
   "metadata": {},
   "source": [
    "Para obtener una respuesta altamente relevante, debemos asegurarnos de que las solicitudes proporcionen cualquier detalle importante o contexto. De lo contrario, estaremos dejando que el modelo adivine lo que queremos decir.\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Peor</th>\n",
    "        <th>Mejor</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>¿Cómo sumo números en Excel?</td>\n",
    "        <td>¿Cómo sumo una fila de cantidades en dólares en Excel? Quiero hacer esto automáticamente para toda una hoja de filas con todos los totales terminando a la derecha en una columna llamada \"Total\".</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>¿Quién es el presidente?</td>\n",
    "        <td>¿Quién fue el presidente de México en 2021 y con qué frecuencia se celebran las elecciones?</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Escribe un código para calcular la secuencia de Fibonacci.</td>\n",
    "        <td>Escribe una función en TypeScript para calcular eficientemente la secuencia de Fibonacci. Comenta el código abundantemente para explicar qué hace cada parte y por qué está escrita de esa manera.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Resume las notas de la reunión.</td>\n",
    "        <td>Resume las notas de la reunión en un solo párrafo. Luego escribe una lista en markdown de los oradores y cada uno de sus puntos clave. Finalmente, enumera los próximos pasos o elementos de acción sugeridos por los oradores, si los hay.</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ee22b",
   "metadata": {},
   "source": [
    "### 3.2 - Pedir al modelo que adopte una persona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c673a883",
   "metadata": {},
   "source": [
    "El mensaje del sistema puede usarse para especificar la persona que el modelo debe adoptar en sus respuestas.\n",
    "\n",
    "`SYSTEM`:\n",
    "Cuando pida ayuda para escribir algo, responderás con un documento que contenga al menos un chiste o comentario juguetón en cada párrafo.\n",
    "\n",
    "`USER`:\n",
    "Escribe una nota de agradecimiento a mi proveedor de tornillos de acero por entregar el pedido a tiempo y con poco tiempo de aviso. Esto hizo posible que entregáramos un pedido importante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b93f8",
   "metadata": {},
   "source": [
    "### 3.3 - Usar delimitadores para indicar claramente las partes distintas de la entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71480995",
   "metadata": {},
   "source": [
    "Delimitadores como comillas triples, etiquetas XML, títulos de secciones, etc., pueden ayudar a demarcar secciones de texto para tratarlas de manera diferente.\n",
    "\n",
    "`USER`:\n",
    "Resume el texto delimitado por comillas triples con un haiku.\n",
    "\n",
    "\"\"\"insertar texto aquí\"\"\"\n",
    "\n",
    "`SYSTEM`:\n",
    "Se te proporcionarán un par de artículos (delimitados con etiquetas XML) sobre el mismo tema. Primero resume los argumentos de cada artículo. Luego indica cuál de ellos presenta un mejor argumento y explica por qué.\n",
    "\n",
    "`USER`:\n",
    "\n",
    "`<article>` insertar primer artículo aquí `</article>`\n",
    "`<article>` insertar segundo artículo aquí `</article>`\n",
    "\n",
    "`SYSTEM`:\n",
    "Se te proporcionará un resumen de tesis y un título sugerido para esta. El título de la tesis debe dar al lector una buena idea del tema de la tesis pero también debe ser llamativo. Si el título no cumple con estos criterios, sugiere 5 alternativas.\n",
    "\n",
    "`USER`:\n",
    "Resumen: insertar resumen aquí\n",
    "\n",
    "Título: insertar título aquí\n",
    "\n",
    "\n",
    "Para tareas sencillas como estas, usar delimitadores podría no hacer una diferencia en la calidad de la salida. Sin embargo, cuanto más compleja sea una tarea, más importante será desambiguar los detalles de la tarea. No hacer que el modelo tenga que trabajar para entender exactamente lo que estamos pidiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ebc6a",
   "metadata": {},
   "source": [
    "### 3.4 - Especificar los pasos necesarios para completar una tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7582a68",
   "metadata": {},
   "source": [
    "Algunas tareas se especifican mejor como una secuencia de pasos. Escribir los pasos explícitamente puede hacer que sea más fácil para el modelo seguirlos.\n",
    "\n",
    "`SYSTEM`:\n",
    "Usa las siguientes instrucciones paso a paso para responder a las entradas del usuario.\n",
    "\n",
    "Paso 1 - El usuario te proporcionará texto entre comillas triples. Resume este texto en una oración con un prefijo que diga \"Resumen: \".\n",
    "\n",
    "Paso 2 - Traduce el resumen del Paso 1 al español, con un prefijo que diga \"Traducción: \".\n",
    "\n",
    "`USER`:\n",
    "\"\"\"insertar texto aquí\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb039909",
   "metadata": {},
   "source": [
    "### 3.5 - Proporcionar ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e703f58",
   "metadata": {},
   "source": [
    "Proporcionar instrucciones generales que se apliquen a todos los ejemplos generalmente es más eficiente que demostrar todas las permutaciones de una tarea mediante ejemplos, pero en algunos casos proporcionar ejemplos puede ser más fácil. Por ejemplo, si deseas que el modelo copie un estilo particular de responder a las consultas de los usuarios que es difícil de describir explícitamente. Esto se conoce como \"few-shot prompting\".\n",
    "\n",
    "`SYSTEM`:\n",
    "Responde en un estilo consistente.\n",
    "\n",
    "`USER`:\n",
    "Enséñame sobre la paciencia.\n",
    "\n",
    "`ASSISTANT`:\n",
    "El río que esculpe el valle más profundo fluye desde un manantial modesto; la sinfonía más grandiosa se origina de una sola nota; el tapiz más intrincado comienza con un solo hilo.\n",
    "\n",
    "`USER`:\n",
    "Enséñame sobre el océano.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce308dc",
   "metadata": {},
   "source": [
    "### 3.6 - Especificar la longitud deseada de la salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b9b5b",
   "metadata": {},
   "source": [
    "Podemos pedir al modelo que produzca salidas de una longitud objetivo determinada. La longitud de la salida deseada se puede especificar en términos de conteo de palabras, oraciones o párrafos. Sin embargo, tengamos en cuenta que instruir al modelo para generar un número específico de palabras no funciona con alta precisión. El modelo puede generar salidas más confiables con un número específico de párrafos o puntos de viñeta.\n",
    "\n",
    "`USER`:\n",
    "Resume el texto delimitado por comillas triples en aproximadamente 50 palabras.\n",
    "\n",
    "\"\"\"insertar texto aquí\"\"\"\n",
    "\n",
    "`USER`:\n",
    "Resume el texto delimitado por comillas triples en 2 párrafos.\n",
    "\n",
    "\"\"\"insertar texto aquí\"\"\"\n",
    "\n",
    "`USER`:\n",
    "Resume el texto delimitado por comillas triples en 3 puntos de viñeta.\n",
    "\n",
    "\"\"\"insertar texto aquí\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb406c65",
   "metadata": {},
   "source": [
    "## 4 - Proporcionar texto de referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10aa7e",
   "metadata": {},
   "source": [
    "### 4.1 - Instruir al modelo para que responda utilizando un texto de referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bfe05",
   "metadata": {},
   "source": [
    "Si podemos proporcionar al modelo información confiable que sea relevante para la consulta actual, entonces podemos instruir al modelo para que use la información proporcionada para componer su respuesta.\n",
    "\n",
    "`SYSTEM`:\n",
    "Utiliza los artículos proporcionados delimitados por comillas triples para responder preguntas. Si la respuesta no se puede encontrar en los artículos, escribe \"No pude encontrar una respuesta.\"\n",
    "\n",
    "`USER`:\n",
    "\"<insertar artículos, cada uno delimitado por comillas triples>\"\n",
    "\n",
    "Pregunta: \"<insertar pregunta aquí>\"\n",
    "\n",
    "\n",
    "\n",
    "Dado que todos los modelos tienen ventanas de contexto limitadas, necesitamos alguna forma de buscar dinámicamente información que sea relevante para la pregunta que se hace. Los embeddings pueden usarse para implementar una recuperación de conocimiento eficiente. Consulta la táctica \"Usar búsqueda basada en embeddings para implementar una recuperación de conocimiento eficiente\" para más detalles sobre cómo implementar esto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7854ee",
   "metadata": {},
   "source": [
    "### 4.2 - Instruir al modelo para que responda con citas de un texto de referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919a1f6",
   "metadata": {},
   "source": [
    "Si la entrada ha sido complementada con conocimiento relevante, es sencillo solicitar que el modelo agregue citas a sus respuestas referenciando pasajes de los documentos proporcionados. Ten en cuenta que las citas en la salida pueden verificarse programáticamente mediante coincidencias de cadenas dentro de los documentos proporcionados.\n",
    "\n",
    "`SYSTEM`:\n",
    "Se te proporcionará un documento delimitado por comillas triples y una pregunta. Tu tarea es responder la pregunta utilizando solo el documento proporcionado y citar el(los) pasaje(s) del documento utilizado(s) para responder la pregunta. Si el documento no contiene la información necesaria para responder esta pregunta, simplemente escribe: \"Información insuficiente.\" Si se proporciona una respuesta a la pregunta, debe estar anotada con una cita. Utiliza el siguiente formato para citar pasajes relevantes ({\"cita\": …}).\n",
    "\n",
    "`USER`:\n",
    "\"\"\"<insertar documento aquí>\"\"\"\n",
    "\n",
    "Pregunta: \"<insertar pregunta aquí>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0fd96",
   "metadata": {},
   "source": [
    "## 5 - Dividir tareas complejas en subtareas más simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247ea9f",
   "metadata": {},
   "source": [
    "### 5.1 - Usar clasificación de intenciones para identificar las instrucciones más relevantes para una consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea387ed",
   "metadata": {},
   "source": [
    "Para tareas en las que se necesitan muchos conjuntos independientes de instrucciones para manejar diferentes casos, puede ser beneficioso primero clasificar el tipo de consulta y usar esa clasificación para determinar qué instrucciones son necesarias. Esto se puede lograr definiendo categorías fijas y codificando instrucciones que sean relevantes para manejar tareas en una categoría dada. Este proceso también se puede aplicar de forma recursiva para descomponer una tarea en una secuencia de etapas. La ventaja de este enfoque es que cada consulta contendrá solo las instrucciones necesarias para realizar la siguiente etapa de una tarea, lo que puede resultar en menores tasas de error en comparación con usar una sola consulta para realizar toda la tarea. Esto también puede resultar en menores costos, ya que los prompts más grandes cuestan más.\n",
    "\n",
    "Supongamos, por ejemplo, que para una aplicación de servicio al cliente, las consultas podrían clasificarse de la siguiente manera:\n",
    "\n",
    "`SYSTEM`:\n",
    "Se te proporcionarán consultas de servicio al cliente. Clasifica cada consulta en una categoría principal y una categoría secundaria. Proporciona tu salida en formato JSON con las claves: primary (principal) y secondary (secundaria).\n",
    "\n",
    "Categorías principales: Facturación, Soporte técnico, Gestión de cuentas o Consulta general.\n",
    "\n",
    "Categorías secundarias de Facturación:\n",
    "\n",
    "+ Cancelar suscripción o actualizar\n",
    "+ Agregar un método de pago\n",
    "+ Explicación de un cargo\n",
    "+ Disputar un cargo\n",
    "\n",
    "Categorías secundarias de Soporte técnico:\n",
    "\n",
    "+ Solución de problemas\n",
    "+ Compatibilidad de dispositivos\n",
    "+ Actualizaciones de software\n",
    "\n",
    "Categorías secundarias de Gestión de cuentas:\n",
    "\n",
    "+ Restablecimiento de contraseña\n",
    "+ Actualizar información personal\n",
    "+ Cerrar cuenta\n",
    "+ Seguridad de la cuenta\n",
    "\n",
    "Categorías secundarias de Consulta general:\n",
    "\n",
    "+ Información de productos\n",
    "+ Precios\n",
    "+ Retroalimentación\n",
    "+ Hablar con un humano\n",
    "\n",
    "\n",
    "`USER`:\n",
    "Necesito que mi internet vuelva a funcionar.\n",
    "\n",
    "<br>\n",
    "\n",
    "Con base en la clasificación de la consulta del cliente, se puede proporcionar un conjunto de instrucciones más específicas a un modelo para que maneje los siguientes pasos. Por ejemplo, supongamos que el cliente necesita ayuda con la \"solución de problemas\".\n",
    "\n",
    "`SYSTEM`:\n",
    "Se te proporcionarán consultas de servicio al cliente que requieren solución de problemas en un contexto de soporte técnico. Ayuda al usuario con:\n",
    "\n",
    "Pídele que verifique que todos los cables hacia/desde el router estén conectados. Ten en cuenta que es común que los cables se aflojen con el tiempo.\n",
    "Si todos los cables están conectados y el problema persiste, pregúntale qué modelo de router está utilizando.\n",
    "Ahora le aconsejarás cómo reiniciar su dispositivo:\n",
    "\n",
    "-- Si el número de modelo es MTD-327J, indícale que presione el botón rojo y lo mantenga presionado durante 5 segundos, luego espere 5 minutos antes de probar la conexión.\n",
    "\n",
    "-- Si el número de modelo es MTD-327S, indícale que desconecte y vuelva a conectar el dispositivo, luego espere 5 minutos antes de probar la conexión.\n",
    "\n",
    "Si el problema del cliente persiste después de reiniciar el dispositivo y esperar 5 minutos, conéctalo al soporte técnico saliendo {\"IT support requested\"}.\n",
    "Si el usuario comienza a hacer preguntas que no están relacionadas con este tema, confirma si le gustaría finalizar el chat actual sobre solución de problemas y clasifica su solicitud según el siguiente esquema:\n",
    "\"<insertar esquema de clasificación principal/secundaria de arriba aquí>\"\n",
    "\n",
    "`USER`:\n",
    "Necesito que mi internet vuelva a funcionar.\n",
    "\n",
    "<br>\n",
    "\n",
    "Se ha instruido al modelo para emitir cadenas especiales para indicar cuándo cambia el estado de la conversación. Esto nos permite convertir nuestro sistema en una máquina de estados donde el estado determina qué instrucciones se inyectan. Al realizar un seguimiento del estado, qué instrucciones son relevantes en ese estado y, opcionalmente, qué transiciones de estado están permitidas desde ese estado, podemos establecer límites alrededor de la experiencia del usuario que serían difíciles de lograr con un enfoque menos estructurado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b8227",
   "metadata": {},
   "source": [
    "### 5.2 - Aplicaciones de diálogo que requieren conversaciones muy largas, resume o filtra el diálogo previo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04cd388",
   "metadata": {},
   "source": [
    "Dado que los modelos tienen una longitud de contexto fija, el diálogo entre un usuario y un asistente en el que se incluye toda la conversación en la ventana de contexto no puede continuar indefinidamente.\n",
    "\n",
    "Existen varias soluciones a este problema, una de las cuales es resumir los turnos previos en la conversación. Una vez que el tamaño de la entrada alcanza una longitud umbral predeterminada, esto podría desencadenar una consulta que resuma parte de la conversación y el resumen de la conversación anterior podría incluirse como parte del mensaje del sistema. Alternativamente, la conversación previa podría resumirse de forma asíncrona en segundo plano durante toda la conversación.\n",
    "\n",
    "Una solución alternativa es seleccionar dinámicamente las partes anteriores de la conversación que sean más relevantes para la consulta actual. Consulta la táctica \"Usar búsqueda basada en embeddings para implementar una recuperación de conocimiento eficiente\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3e68c",
   "metadata": {},
   "source": [
    "### 5.3 - Resume documentos largos por partes y construye un resumen completo de manera recursiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fceb5f",
   "metadata": {},
   "source": [
    "Dado que los modelos tienen una longitud de contexto fija, no se pueden usar para resumir un texto más largo que la longitud del contexto menos la longitud del resumen generado en una sola consulta.\n",
    "\n",
    "Para resumir un documento muy largo, como un libro, podemos usar una secuencia de consultas para resumir cada sección del documento. Los resúmenes de las secciones pueden concatenarse y resumirse, produciendo resúmenes de resúmenes. Este proceso puede proceder de manera recursiva hasta que se resuma todo el documento. Si es necesario usar información sobre secciones anteriores para dar sentido a las secciones posteriores, entonces un truco adicional que puede ser útil es incluir un resumen continuo del texto que precede a cualquier punto dado del libro mientras se resume el contenido en ese punto. La efectividad de este procedimiento para resumir libros ha sido estudiada en investigaciones anteriores por OpenAI usando variantes de GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714ea8c",
   "metadata": {},
   "source": [
    "## 6 - Dar al modelo tiempo para \"pensar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41177b13",
   "metadata": {},
   "source": [
    "### 6.1 - Instruir al modelo para que resuelva su propia solución antes de apresurarse a una conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7cdd3",
   "metadata": {},
   "source": [
    "A veces obtenemos mejores resultados cuando instruimos explícitamente al modelo para que razone desde los principios básicos antes de llegar a una conclusión. Supongamos, por ejemplo, que queremos que un modelo evalúe la solución de un estudiante a un problema matemático. La forma más obvia de abordar esto es simplemente preguntar al modelo si la solución del estudiante es correcta o no.\n",
    "\n",
    "\n",
    "`SYSTEM`:\n",
    "Determina si la solución del estudiante es correcta o no.\n",
    "\n",
    "`USER`:\n",
    "Enunciado del problema: Estoy construyendo una instalación de energía solar y necesito ayuda para calcular las finanzas.\n",
    "\n",
    "+ El terreno cuesta 100 por pie cuadrado.\n",
    "+ Puedo comprar paneles solares por 250 por pie cuadrado.\n",
    "+ He negociado un contrato de mantenimiento que me costará 100000 anuales, más 10 por pie cuadrado adicionales. ¿Cuál es el costo total para el primer año de operaciones en función del número de pies cuadrados?\n",
    "\n",
    "\n",
    "Solución del estudiante: Sea x el tamaño de la instalación en pies cuadrados.\n",
    "\n",
    "+ Costo del terreno: 100x\n",
    "+ Costo de los paneles solares: 250x\n",
    "+ Costo de mantenimiento: 100000 + 100x\n",
    "+ Costo total: 100x + 250x + 100000 + 100x = 450x + 100,000\n",
    "\n",
    "\n",
    "`ASSISTANT`:\n",
    "La solución del estudiante es correcta.\n",
    "\n",
    "\n",
    "Pero la solución del estudiante en realidad no es correcta. Podemos hacer que el modelo note esto exitosamente pidiéndole que genere su propia solución primero.\n",
    "\n",
    "`SYSTEM`:\n",
    "Primero resuelve tu propia solución al problema. Luego compara tu solución con la del estudiante y evalúa si la solución del estudiante es correcta o no. No decidas si la solución del estudiante es correcta hasta que hayas hecho el problema tú mismo.\n",
    "\n",
    "`USER`:\n",
    "Enunciado del problema: Estoy construyendo una instalación de energía solar y necesito ayuda para calcular las finanzas.\n",
    "\n",
    "+ El terreno cuesta 100 por pie cuadrado.\n",
    "+ Puedo comprar paneles solares por 250 por pie cuadrado.\n",
    "+ He negociado un contrato de mantenimiento que me costará 100000 anuales, más 10 por pie cuadrado + adicionales.¿Cuál es el costo total para el primer año de operaciones en función del número de pies cuadrados?\n",
    "\n",
    "Solución del estudiante: Sea x el tamaño de la instalación en pies cuadrados.\n",
    "\n",
    "+ Costo del terreno: 100x\n",
    "+ Costo de los paneles solares: 250x\n",
    "+ Costo de mantenimiento: 100000 + 100x\n",
    "+ Costo total: 100x + 250x + 100000 + 100x = 450x + 100000\n",
    "\n",
    "\n",
    "`ASSISTANT`:\n",
    "\n",
    "+ Sea x el tamaño de la instalación en pies cuadrados.\n",
    "+ Costo del terreno: 100x\n",
    "+ Costo de los paneles solares: 250x\n",
    "+ Costo de mantenimiento: 100,000 + 10x\n",
    "+ Costo total: 100x + 250x + 100,000 + 10x = 360x + 100,000\n",
    "\n",
    "\n",
    "La solución del estudiante es incorrecta. Cometió un error en el cálculo del costo de mantenimiento, usando 100x en lugar de 10x. El costo total correcto para el primer año de operaciones es 360x + 100000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e2b18",
   "metadata": {},
   "source": [
    "### 6.2 - Usar monólogo interno o secuencia de consultas para ocultar el proceso de razonamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547bbcf",
   "metadata": {},
   "source": [
    "La táctica anterior demuestra que a veces es importante que el modelo razone en detalle sobre un problema antes de responder una pregunta específica. Para algunas aplicaciones, el proceso de razonamiento que utiliza un modelo para llegar a una respuesta final no sería apropiado compartirlo con el usuario. Por ejemplo, en aplicaciones de tutoría, es posible que queramos alentar a los estudiantes a resolver sus propias respuestas, pero el proceso de razonamiento del modelo sobre la solución del estudiante podría revelar la respuesta al estudiante.\n",
    "\n",
    "El monólogo interno es una táctica que se puede usar para mitigar esto. La idea del monólogo interno es instruir al modelo para que coloque partes de la salida que están destinadas a ser ocultas del usuario en un formato estructurado que facilite su análisis. Luego, antes de presentar la salida al usuario, se analiza la salida y solo se muestra parte de la salida.\n",
    "\n",
    "`SYSTEM`:\n",
    "Sigue estos pasos para responder a las consultas del usuario.\n",
    "\n",
    "Paso 1 - Primero resuelve tu propia solución al problema. No te fíes de la solución del estudiante ya que podría ser incorrecta. Incluye todo tu trabajo para este paso entre comillas triples (\"\"\").\n",
    "\n",
    "Paso 2 - Compara tu solución con la del estudiante y evalúa si la solución del estudiante es correcta o no. Incluye todo tu trabajo para este paso entre comillas triples (\"\"\").\n",
    "\n",
    "Paso 3 - Si el estudiante cometió un error, determina qué pista podrías darle al estudiante sin revelar la respuesta. Incluye todo tu trabajo para este paso entre comillas triples (\"\"\").\n",
    "\n",
    "Paso 4 - Si el estudiante cometió un error, proporciona la pista del paso anterior al estudiante (fuera de las comillas triples). En lugar de escribir \"Paso 4 - ...\" escribe \"Pista:\".\n",
    "\n",
    "`USER`:\n",
    "Enunciado del problema: \"\"\"insertar enunciado del problema\"\"\"\n",
    "\n",
    "Solución del estudiante: \"\"\"insertar solución del estudiante\"\"\"\n",
    "\n",
    "Alternativamente, esto se puede lograr con una secuencia de consultas en las que todas, excepto la última, tienen su salida oculta para el usuario final.\n",
    "\n",
    "Primero, podemos pedir al modelo que resuelva el problema por sí mismo. Dado que esta consulta inicial no requiere la solución del estudiante, se puede omitir. Esto proporciona la ventaja adicional de que no hay posibilidad de que la solución del modelo se vea sesgada por la solución tentativa del estudiante.\n",
    "\n",
    "`USER`:\n",
    "\"insertar enunciado del problema\"\n",
    "\n",
    "A continuación, podemos hacer que el modelo use toda la información disponible para evaluar la corrección de la solución del estudiante.\n",
    "\n",
    "`SYSTEM`:\n",
    "Compara tu solución con la del estudiante y evalúa si la solución del estudiante es correcta o no.\n",
    "\n",
    "`USER`:\n",
    "Enunciado del problema: \"\"\"insertar enunciado del problema\"\"\"\n",
    "\n",
    "Tu solución: \"\"\"insertar solución generada por el modelo\"\"\"\n",
    "\n",
    "Solución del estudiante: \"\"\"insertar solución del estudiante\"\"\"\n",
    "\n",
    "Finalmente, podemos dejar que el modelo use su propio análisis para construir una respuesta en la persona de un tutor útil.\n",
    "\n",
    "`SYSTEM`:\n",
    "\n",
    "Eres un tutor de matemáticas. Si el estudiante cometió un error, ofrécele una pista al estudiante de una manera que no revele la respuesta. Si el estudiante no cometió un error, simplemente ofrécele un comentario alentador.\n",
    "\n",
    "`USER`:\n",
    "\n",
    "Enunciado del problema: \"\"\"insertar enunciado del problema\"\"\"\n",
    "\n",
    "Tu solución: \"\"\"insertar solución generada por el modelo\"\"\"\n",
    "\n",
    "Solución del estudiante: \"\"\"insertar solución del estudiante\"\"\"\n",
    "\n",
    "Análisis: \"\"\"insertar análisis generado por el modelo del paso anterior\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc0f36",
   "metadata": {},
   "source": [
    "### 6.3 - Preguntar al modelo si se perdió algo en pases anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af12d9c",
   "metadata": {},
   "source": [
    "Supongamos que estamos usando un modelo para listar extractos de una fuente que son relevantes para una pregunta en particular. Después de listar cada extracto, el modelo necesita determinar si debe comenzar a escribir otro o si debe detenerse. Si el documento fuente es grande, es común que un modelo se detenga demasiado pronto y no enumere todos los extractos relevantes. En ese caso, se puede obtener un mejor rendimiento al pedir al modelo con consultas de seguimiento que encuentre cualquier extracto que haya perdido en pases anteriores.\n",
    "\n",
    "`SYSTEM`:\n",
    "Se te proporcionará un documento delimitado por comillas triples. Tu tarea es seleccionar extractos que se refieran a la siguiente pregunta: \"¿Qué cambios de paradigma significativos han ocurrido en la historia de la inteligencia artificial?\"\n",
    "\n",
    "Asegúrate de que los extractos contengan todo el contexto relevante necesario para interpretarlos; en otras palabras, no extraigas pequeños fragmentos que carecen de contexto importante. Proporciona la salida en formato JSON de la siguiente manera: \"[{\"extracto\": \"...\"},...{\"extracto\": \"...\"}]\"\n",
    "\n",
    "\n",
    "`USER`:\n",
    "\"\"\"insertar documento aquí\"\"\"\n",
    "\n",
    "\n",
    "`ASSISTANT`:\n",
    "[{\"extracto\": \"el modelo escribe un extracto aquí\"},\n",
    "...\n",
    "{\"extracto\": \"el modelo escribe otro extracto aquí\"}]\n",
    "\n",
    "\n",
    "`USER`:\n",
    "¿Hay más extractos relevantes? Asegúrate de no repetir extractos. También asegúrate de que los extractos contengan todo el contexto relevante necesario para interpretarlos; en otras palabras, no extraigas pequeños fragmentos que carecen de contexto importante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec28d2",
   "metadata": {},
   "source": [
    "## 7 - Usar herramientas externas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741eefc",
   "metadata": {},
   "source": [
    "### 7.1 - Usar búsqueda basada en embeddings para implementar una recuperación de conocimiento eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5caec4",
   "metadata": {},
   "source": [
    "Un modelo puede aprovechar fuentes externas de información si se proporcionan como parte de su entrada. Esto puede ayudar al modelo a generar respuestas más informadas y actualizadas. Por ejemplo, si un usuario hace una pregunta sobre una película específica, puede ser útil agregar información de alta calidad sobre la película, por ejemplo, actores, director, etc... a la entrada del modelo. Los embeddings se pueden usar para implementar una recuperación de conocimiento eficiente, de modo que la información relevante se pueda agregar a la entrada del modelo dinámicamente en tiempo de ejecución.\n",
    "\n",
    "Un embedding de texto es un vector que puede medir la relación entre cadenas de texto. Las cadenas similares o relevantes estarán más cerca unas de otras que las cadenas no relacionadas. Este hecho, junto con la existencia de algoritmos de búsqueda de vectores rápidos, significa que los embeddings se pueden usar para implementar una recuperación de conocimiento eficiente. En particular, un corpus de texto se puede dividir en fragmentos, y cada fragmento se puede embedir y almacenar. Luego, se puede realizar una búsqueda vectorial para encontrar los fragmentos embebidos de texto del corpus que estén más relacionados con la consulta, es decir, más cercanos en el espacio de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d06d42",
   "metadata": {},
   "source": [
    "### 7.2 - Usar la ejecución de código para realizar cálculos más precisos o llamar a APIs externas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef2c55",
   "metadata": {},
   "source": [
    "No se puede confiar en que los modelos de lenguaje realicen cálculos aritméticos o cálculos largos con precisión por sí solos. En casos donde esto sea necesario, se puede instruir a un modelo para que escriba y ejecute código en lugar de hacer sus propios cálculos. En particular, se puede instruir a un modelo para que ponga el código que se debe ejecutar en un formato designado, como comillas triples invertidas. Después de producir una salida, se puede extraer y ejecutar el código. Finalmente, si es necesario, la salida del motor de ejecución de código se puede proporcionar como entrada al modelo para la siguiente consulta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eac47c",
   "metadata": {},
   "source": [
    "## 8 - Probar los cambios sistemáticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381d832",
   "metadata": {},
   "source": [
    "A veces puede ser difícil determinar si un cambio, por ejemplo, una nueva instrucción o un nuevo diseño, mejora o empeora nuestro sistema. Observar algunos ejemplos puede dar una pista de cuál es mejor, pero con tamaños de muestra pequeños puede ser difícil distinguir entre una mejora real o la suerte. Tal vez el cambio ayude al rendimiento en algunas entradas, pero perjudique el rendimiento en otras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "684.444px",
    "left": "166px",
    "top": "0px",
    "width": "335.998px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
