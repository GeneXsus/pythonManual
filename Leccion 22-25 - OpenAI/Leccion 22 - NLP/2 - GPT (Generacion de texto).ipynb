{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e24db66",
   "metadata": {},
   "source": [
    "# 2 - GPT (Generación de texto)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/openai_gpt.webp\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08e418",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Registro-y-configuración-inicial\" data-toc-modified-id=\"1---Registro-y-configuración-inicial-1\">1 - Registro y configuración inicial</a></span></li><li><span><a href=\"#2---Instalar-las-dependencias\" data-toc-modified-id=\"2---Instalar-las-dependencias-2\">2 - Instalar las dependencias</a></span></li><li><span><a href=\"#3---Configuración-del-entorno\" data-toc-modified-id=\"3---Configuración-del-entorno-3\">3 - Configuración del entorno</a></span></li><li><span><a href=\"#4---Modelos-disponibles\" data-toc-modified-id=\"4---Modelos-disponibles-4\">4 - Modelos disponibles</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1---GPT-3.5\" data-toc-modified-id=\"4.1---GPT-3.5-4.1\">4.1 - GPT-3.5</a></span></li><li><span><a href=\"#4.2---GPT-4-Turbo-y-GPT-4\" data-toc-modified-id=\"4.2---GPT-4-Turbo-y-GPT-4-4.2\">4.2 - GPT-4 Turbo y GPT-4</a></span></li><li><span><a href=\"#4.3---GPT-4o\" data-toc-modified-id=\"4.3---GPT-4o-4.3\">4.3 - GPT-4o</a></span></li><li><span><a href=\"#4.4---GPT-4o-Mini\" data-toc-modified-id=\"4.4---GPT-4o-Mini-4.4\">4.4 - GPT-4o Mini</a></span></li></ul></li><li><span><a href=\"#5---Uso-del-modelo-GPT-3.5\" data-toc-modified-id=\"5---Uso-del-modelo-GPT-3.5-5\">5 - Uso del modelo GPT-3.5</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1---Llamada-básica-y-parámetros\" data-toc-modified-id=\"5.1---Llamada-básica-y-parámetros-5.1\">5.1 - Llamada básica y parámetros</a></span></li><li><span><a href=\"#5.2---Memoria-de-conversación\" data-toc-modified-id=\"5.2---Memoria-de-conversación-5.2\">5.2 - Memoria de conversación</a></span></li></ul></li><li><span><a href=\"#6---Uso-del-modelo-GPT-4\" data-toc-modified-id=\"6---Uso-del-modelo-GPT-4-6\">6 - Uso del modelo GPT-4</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1---Llamada-al-modelo\" data-toc-modified-id=\"6.1---Llamada-al-modelo-6.1\">6.1 - Llamada al modelo</a></span></li><li><span><a href=\"#6.2---Uso-de-funciones\" data-toc-modified-id=\"6.2---Uso-de-funciones-6.2\">6.2 - Uso de funciones</a></span></li></ul></li><li><span><a href=\"#7---Uso-del-modelo-GPT-4o\" data-toc-modified-id=\"7---Uso-del-modelo-GPT-4o-7\">7 - Uso del modelo GPT-4o</a></span><ul class=\"toc-item\"><li><span><a href=\"#7.1---Llamada-al-modelo\" data-toc-modified-id=\"7.1---Llamada-al-modelo-7.1\">7.1 - Llamada al modelo</a></span></li><li><span><a href=\"#7.2---Llamada-multimodal\" data-toc-modified-id=\"7.2---Llamada-multimodal-7.2\">7.2 - Llamada multimodal</a></span></li></ul></li><li><span><a href=\"#8---Uso-del-modelo-GPT-4o-Mini\" data-toc-modified-id=\"8---Uso-del-modelo-GPT-4o-Mini-8\">8 - Uso del modelo GPT-4o Mini</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.1---Llamada-al-modelo\" data-toc-modified-id=\"8.1---Llamada-al-modelo-8.1\">8.1 - Llamada al modelo</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffddb4",
   "metadata": {},
   "source": [
    "## 1 - Registro y configuración inicial\n",
    "\n",
    "\n",
    "Para empezar a usar la API de OpenAI, primero necesitas crear una cuenta y obtener una clave API.\n",
    "\n",
    "1. Entramos en la página de [OpenAI](https://openai.com/) y creamos una cuenta si aún no tenemos una.\n",
    "\n",
    "\n",
    "2. Iniciamos sesión en nuestra cuenta y navegamos a la [sección de API](https://platform.openai.com/settings/profile?tab=api-keys).\n",
    "\n",
    "\n",
    "3. Generamos una nueva clave API y la guardamos en un lugar seguro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26292b9d",
   "metadata": {},
   "source": [
    "## 2 - Instalar las dependencias\n",
    "\n",
    "Necesitamo instalar la librería. Usaremos openai para interactuar con la API. Para ello usamos el siguiente comando para instalarla:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db321f",
   "metadata": {},
   "source": [
    "## 3 - Configuración del entorno\n",
    "\n",
    "Configuramos el entorno para usar la API Key. Podfemos hacerlo estableciendo una variable de entorno o directamente en el código. Importarla con el nombre `OPENAI_API_KEY` ya es suficiente para que la API la reconozca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9e90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la API KEY \n",
    "\n",
    "import os                           # libreria del sistema operativo\n",
    "from dotenv import load_dotenv      # carga variables de entorno \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74eadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# también podríamos hacerlo directamente de la siguiente manera\n",
    "\n",
    "import openai as ai\n",
    "\n",
    "ai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247c68a",
   "metadata": {},
   "source": [
    "## 4 - Modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abd7a0",
   "metadata": {},
   "source": [
    "### 4.1 - GPT-3.5\n",
    "\n",
    "Los modelos GPT-3.5 Turbo pueden comprender y generar lenguaje natural o código y han sido optimizados para chat usando la API de Chat Completions, aunque también funcionan bien para tareas que no son de chat.\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>MODELO</th>\n",
    "        <th>DESCRIPCIÓN</th>\n",
    "        <th>VENTANA DE CONTEXTO</th>\n",
    "        <th>DATOS DE ENTRENAMIENTO</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-3.5-turbo-0125</td>\n",
    "        <td>El último modelo GPT-3.5 Turbo con mayor precisión en las respuestas en los formatos solicitados y una corrección de un error que causaba un problema de codificación de texto para las llamadas de funciones en idiomas distintos del inglés. Devuelve un máximo de 4,096 tokens de salida</td>\n",
    "        <td>16.385 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-3.5-turbo</td>\n",
    "        <td>Actualmente apunta a gpt-3.5-turbo-0125.</td>\n",
    "        <td>16.385 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-3.5-turbo-1106</td>\n",
    "        <td>Modelo GPT-3.5 Turbo con mejor seguimiento de instrucciones, modo JSON, resultados reproducibles, llamadas de funciones en paralelo y más. Devuelve un máximo de 4,096 tokens de salida. Más información.</td>\n",
    "        <td>16.385 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-3.5-turbo-instruct</td>\n",
    "        <td>Capacidades similares a los modelos de la era GPT-3. Compatible con el endpoint de Completions heredado y no con Chat Completions.</td>\n",
    "        <td>4.096 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f98515",
   "metadata": {},
   "source": [
    "### 4.2 - GPT-4 Turbo y GPT-4\n",
    "\n",
    "GPT-4 es un modelo multimodal grande (acepta entradas de texto o imágenes y produce texto) que puede resolver problemas difíciles con mayor precisión que cualquiera de nuestros modelos anteriores, gracias a su conocimiento general más amplio y capacidades avanzadas de razonamiento. GPT-4 está disponible en la API de OpenAI para clientes de pago. Al igual que gpt-3.5-turbo, GPT-4 está optimizado para chat pero funciona bien para tareas tradicionales de completado utilizando la API de Chat Completions. \n",
    "\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>MODELO</th>\n",
    "        <th>DESCRIPCIÓN</th>\n",
    "        <th>VENTANA DE CONTEXTO</th>\n",
    "        <th>DATOS DE ENTRENAMIENTO</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-turbo</td>\n",
    "        <td>El último modelo GPT-4 Turbo con capacidades de visión. Las solicitudes de visión ahora pueden usar el modo JSON y llamadas de funciones. Actualmente apunta a gpt-4-turbo-2024-04-09.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta dic 2023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-turbo-2024-04-09</td>\n",
    "        <td>GPT-4 Turbo con modelo de Visión. Las solicitudes de visión ahora pueden usar el modo JSON y llamadas de funciones. gpt-4-turbo actualmente apunta a esta versión.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta dic 2023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-turbo-preview</td>\n",
    "        <td>Modelo de vista previa GPT-4 Turbo. Actualmente apunta a gpt-4-0125-preview.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta dic 2023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-0125-preview</td>\n",
    "        <td>Modelo de vista previa GPT-4 Turbo diseñado para reducir casos de \"pereza\" donde el modelo no completa una tarea. Devuelve un máximo de 4,096 tokens de salida. Más información.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta dic 2023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-1106-preview</td>\n",
    "        <td>Modelo de vista previa GPT-4 Turbo con mejor seguimiento de instrucciones, modo JSON, resultados reproducibles, llamadas de funciones en paralelo y más. Devuelve un máximo de 4,096 tokens de salida. Este es un modelo de vista previa. Más información.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta abr 2023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4</td>\n",
    "        <td>Actualmente apunta a gpt-4-0613. Ver actualizaciones continuas del modelo.</td>\n",
    "        <td>8.192 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-0613</td>\n",
    "        <td>Instantánea de gpt-4 del 13 de junio de 2023 con mejor soporte para llamadas de funciones.</td>\n",
    "        <td>8.192 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4-0314</td>\n",
    "        <td>Instantánea heredada de gpt-4 del 14 de marzo de 2023.</td>\n",
    "        <td>8.192 tokens</td>\n",
    "        <td>Hasta sep 2021</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para muchas tareas básicas, la diferencia entre los modelos GPT-4 y GPT-3.5 no es significativa. Sin embargo, en situaciones de razonamiento más complejas, GPT-4 es mucho más capaz que cualquiera de nuestros modelos anteriores.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Capacidades Multilingües**\n",
    "\n",
    "GPT-4 supera tanto a los modelos de lenguaje grande anteriores como, a partir de 2023, a la mayoría de los sistemas de última generación (que a menudo tienen entrenamiento específico para benchmarks o ingeniería manual). En el benchmark MMLU, una suite en inglés de preguntas de opción múltiple que cubre 57 temas, GPT-4 no solo supera a los modelos existentes por un margen considerable en inglés, sino que también demuestra un rendimiento fuerte en otros idiomas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acee746",
   "metadata": {},
   "source": [
    "### 4.3 - GPT-4o\n",
    "\n",
    "GPT-4o (\"o\" de \"omni\") es nuestro modelo más avanzado. Es multimodal (acepta entradas de texto o imágenes y produce texto), y tiene la misma alta inteligencia que GPT-4 Turbo pero es mucho más eficiente: genera texto 2 veces más rápido y es un 50% más barato. Además, GPT-4o tiene la mejor visión y rendimiento en idiomas distintos del inglés de todos nuestros modelos. \n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>MODELO</th>\n",
    "        <th>DESCRIPCIÓN</th>\n",
    "        <th>VENTANA DE CONTEXTO</th>\n",
    "        <th>DATOS DE ENTRENAMIENTO</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4o</td>\n",
    "        <td>El modelo insignia más avanzado y multimodal que es más barato y rápido que GPT-4 Turbo. Actualmente apunta a gpt-4o-2024-05-13.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta oct 2023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4o-2024-05-13</td>\n",
    "        <td>gpt-4o actualmente apunta a esta versión.</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta oct 2023</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7251e",
   "metadata": {},
   "source": [
    "### 4.4 - GPT-4o Mini\n",
    "\n",
    "GPT-4o Mini es una versión compacta y eficiente del modelo más avanzado, GPT-4o. Es multimodal, acepta entradas de texto o imágenes y produce texto, y mantiene la alta inteligencia de GPT-4 Turbo, pero con una mayor eficiencia. Genera texto 2 veces más rápido y es un 50% más barato. Además, GPT-4o Mini ofrece una excelente visión y rendimiento en idiomas distintos del inglés, superando a todos los modelos anteriores. Se ofrece desde Julio de 2024.\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>MODELO</th>\n",
    "        <th>DESCRIPCIÓN</th>\n",
    "        <th>VENTANA DE CONTEXTO</th>\n",
    "        <th>DATOS DE ENTRENAMIENTO</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>gpt-4o-mini</td>\n",
    "        <td>Versión compacta y eficiente del modelo más avanzado, GPT-4o</td>\n",
    "        <td>128.000 tokens</td>\n",
    "        <td>Hasta oct 2023</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1043099",
   "metadata": {},
   "source": [
    "## 5 - Uso del modelo GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fee3cf",
   "metadata": {},
   "source": [
    "Vamos a usar el modelo 3.5 turbo. Primero definiremos el cliente de conexión a la API para a continuación definir las [completions](https://platform.openai.com/docs/guides/chat-completions), la llamada al modelo propiamente dicha. Veremos también todos los parámetros quen podemos usar en la llamada al modelo. \n",
    "\n",
    "Para iniciar el cliente, podemos darle la API Key de manera explícita o si la importamos con el nombre `OPENAI_API_KEY` ya no sería neceesario, la toma como variable de entorno directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669c5ec",
   "metadata": {},
   "source": [
    "### 5.1 - Llamada básica y parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedc7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con API Key explícita\n",
    "\n",
    "cliente = ai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73af48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# como variable de entorno directamente\n",
    "\n",
    "cliente = ai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo\n",
    "\n",
    "modelo = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a457f",
   "metadata": {},
   "source": [
    "Para llamar al modelo, tenemos que construir los mensajes que le enviamos. Estos mensajes son definidos como una lista de diccionarios con keys `role`, que define qué parte es la habla, el sistema, el usuario o el asistente, y `content`, que es el mensaje en sí mismo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d5dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definición de los mensajes\n",
    "\n",
    "mensajes = [{'role': 'system', 'content': 'Eres un experto traductor de inglés a castellano.'},\n",
    "            {'role': 'user', 'content': 'Traduce la siguiente frase: How are you?'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e00e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamada al modelo\n",
    "\n",
    "respuesta = cliente.chat.completions.create(model=modelo, messages=mensajes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a7134e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AQ9RIxwITQ7cXkxNUq3RSf4V1N6Aa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='¿Cómo estás?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1730796176, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=36, total_tokens=41, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0, accepted_prediction_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# objeto de la respuesta\n",
    "\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd4e499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipo de dato de la respuesta\n",
    "\n",
    "type(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24dd5b6",
   "metadata": {},
   "source": [
    "Vemos que la salida del modelo es un objeto `openai.types.chat.chat_completion.ChatCompletion` con varias propiedades. Hagamos una descripción de ellas:\n",
    "\n",
    "\n",
    "1. **id**: Un identificador único en formato string para la completación del chat.\n",
    "\n",
    "\n",
    "2. **choices**: Una lista de opciones de completación del chat. Puede ser más de una si n es mayor que 1.\n",
    "\n",
    "    + finish_reason (string): La razón por la que el modelo dejó de generar tokens. Esto será stop si el modelo alcanzó un punto de parada natural o una secuencia de parada proporcionada, length si se alcanzó el número máximo de tokens especificado en la solicitud, content_filter si el contenido fue omitido debido a una señal de nuestros filtros de contenido o tool_calls si el modelo llamó a una herramienta.\n",
    "    + index (integer): El índice de la opción en la lista de opciones.\n",
    "    + message (object): Un mensaje de chat generado por el modelo con las siguientes propiedades:\n",
    "        + content (string o null): El contenido del mensaje.\n",
    "        + tool_calls (array): Las llamadas de herramientas generadas por el modelo, como las llamadas de funciones.\n",
    "        + role (string): El rol del autor de este mensaje.\n",
    "        + logprobs (object o null): Información de probabilidad logarítmica para la opción.\n",
    "\n",
    "\n",
    "3. **created**: La marca de tiempo Unix (en segundos) de cuando se creó la completación del chat.\n",
    "\n",
    "\n",
    "4. **model**: El modelo utilizado para la completación del chat.\n",
    "\n",
    "\n",
    "5. **service_tier**: El nivel de servicio utilizado para procesar la solicitud. Este campo solo se incluye si el parámetro service_tier se especifica en la solicitud.\n",
    "\n",
    "\n",
    "6. **system_fingerprint**: Esta huella digital representa la configuración de backend con la que se ejecuta el modelo. Puede usarse junto con el parámetro seed de la solicitud para entender cuándo se han realizado cambios en el backend que podrían afectar el determinismo.\n",
    "\n",
    "\n",
    "7. **object**: El tipo de objeto, que siempre es chat.completion.\n",
    "\n",
    "\n",
    "8. **usage**: Estadísticas de uso para la solicitud de completación.\n",
    "    + completion_tokens (integer): Número de tokens en la completación generada.\n",
    "    + prompt_tokens (integer): Número de tokens en el prompt.\n",
    "    + total_tokens (integer): Número total de tokens utilizados en la solicitud (prompt + completación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b669a4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Cómo estás?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texto de la respuesta\n",
    "\n",
    "respuesta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938acbc",
   "metadata": {},
   "source": [
    "La llamada al modelo tiene más parámetros que el modelo y los mensajes para manejar cómo funcionaría el modelo. En este [link](https://platform.openai.com/docs/api-reference/chat/create?lang=curl) tenemos toda la información. Veamos algunos de esos parámetros:\n",
    "\n",
    "\n",
    "\n",
    "1. **messages**: Requerido. Una lista de mensajes que componen la conversación hasta el momento. \n",
    "\n",
    "    + **Mensaje del sistema**: \n",
    "        + content (string): El contenido del mensaje del sistema.\n",
    "        + role (string): El rol del autor del mensaje, en este caso \"system\".\n",
    "        + name (string): Opcional. Un nombre opcional para el participante. Proporciona al modelo información para diferenciar entre participantes del mismo rol.\n",
    "    + **Mensaje del usuario**:\n",
    "        + content (string o array): Requerido. El contenido del mensaje del usuario.\n",
    "            + Contenido de texto (string): El contenido de texto del mensaje.\n",
    "            + Array de partes de contenido (array): Un array de partes de contenido con un tipo definido, cada uno puede ser de tipo texto o image_url cuando se pasan imágenes. Puedes pasar múltiples imágenes agregando múltiples partes de contenido image_url. La entrada de imágenes solo es compatible cuando se usa el modelo gpt-4o.\n",
    "                + Parte de contenido de texto (objeto):\n",
    "                    + type (string): Requerido. El tipo de la parte del contenido. \"text\" en este caso.\n",
    "                    + text (string): Requerido. El contenido de texto.\n",
    "                + Parte de contenido de imagen (objeto):\n",
    "                    + type (string): Requerido. El tipo de la parte del contenido. \"image_url\" en este caso.\n",
    "                    + image_url (objeto): Requerido. La URL de la imagen.\n",
    "        \n",
    "        + role (string): Requerido. El rol del autor del mensaje, en este caso \"user\".\n",
    "        + name (string): Opcional. Un nombre opcional para el participante. Proporciona al modelo información para diferenciar entre participantes del mismo rol.\n",
    "    + **Mensaje del asistente**\n",
    "        + content (string o null): Opcional. El contenido del mensaje del asistente. Requerido a menos que se especifique tool_calls.\n",
    "        + role (string): Requerido. El rol del autor del mensaje, en este caso \"assistant\".\n",
    "        + name (string): Opcional. Un nombre opcional para el participante. Proporciona al modelo información para diferenciar entre participantes del mismo rol.\n",
    "        + tool_calls (array): Opcional. Las llamadas de herramientas generadas por el modelo, como las llamadas de funciones.\n",
    "            + id (string): Requerido. El ID de la llamada de herramienta.\n",
    "            + type (string): Requerido. El tipo de la herramienta. Actualmente, solo se admite \"function\".\n",
    "            + function (objeto): Requerido. La función que el modelo llamó.\n",
    "                + name (string): Requerido. El nombre de la función a llamar.\n",
    "                + arguments (string): Requerido. Los argumentos para llamar a la función, generados por el modelo en formato JSON. Nota que el modelo no siempre genera JSON válido, y puede alucinar parámetros no definidos por tu esquema de funciones. Valida los argumentos en tu código antes de llamar a tu función.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **model (string)**: Requerido. ID del modelo a usar. Ver la tabla de compatibilidad de endpoints de modelos para detalles sobre qué modelos funcionan con la API de Chat. Por ejemplo, gpt-3.5-turbo.\n",
    "\n",
    "\n",
    "3. **frequency_penalty (número o null)**: Opcional. Predeterminado a 0. Número entre -2.0 y 2.0. Valores positivos penalizan nuevos tokens basados en su frecuencia existente en el texto hasta ahora, disminuyendo la probabilidad de que el modelo repita la misma línea textualmente.\n",
    "\n",
    "\n",
    "4. **logit_bias (mapa)**: Opcional. Predeterminado a null. Modifica la probabilidad de que aparezcan tokens especificados en la completación.\n",
    "\n",
    "\n",
    "5. **logprobs (booleano o null)**: Opcional. Predeterminado a false. Si es true, devuelve las probabilidades logarítmicas de cada token de salida.\n",
    "\n",
    "\n",
    "6. **top_logprobs (entero o null)**: Opcional. Un entero entre 0 y 20 que especifica el número de tokens más probables a devolver en cada posición de token, cada uno con una probabilidad logarítmica asociada.\n",
    "\n",
    "\n",
    "7. **max_tokens (entero o null)**: Opcional. El número máximo de tokens que se pueden generar en la completación del chat. EL tamaño máximo de salida.\n",
    "\n",
    "\n",
    "8. **n (entero o null)**: Opcional. Predeterminado a 1. Cuántas opciones de completación del chat generar para cada mensaje de entrada. Longitud de choices.\n",
    "\n",
    "\n",
    "9. **presence_penalty (número o null)**: Opcional:. Predeterminado a 0. Número entre -2.0 y 2.0. Valores positivos penalizan nuevos tokens basados en si aparecen en el texto hasta ahora, aumentando la probabilidad de que el modelo hable sobre nuevos temas.\n",
    "\n",
    "\n",
    "10. **response_format (objeto)**: Opcional. Un objeto que especifica el formato que el modelo debe producir. Compatible con GPT-4 Turbo y todos los modelos GPT-3.5 Turbo más nuevos que gpt-3.5-turbo-1106.\n",
    "\n",
    "\n",
    "11. **seed (entero o null)**: Opcional. Si se especifica, nuestro sistema hará un esfuerzo para muestrear de manera determinista, de modo que las solicitudes repetidas con la misma semilla y parámetros deberían devolver el mismo resultado.\n",
    "\n",
    "\n",
    "12. **service_tier (string o null)**: Opcional. Predeterminado a null. Especifica el nivel de latencia a utilizar para procesar la solicitud.\n",
    "\n",
    "\n",
    "13. **stop (string / array / null)**: Opcional. Predeterminado a null. Hasta 4 secuencias donde la API dejará de generar tokens adicionales.\n",
    "\n",
    "\n",
    "14. **stream (booleano o null)**: Opcional. Predeterminado a false. Si se establece, se enviarán deltas, un iterador, de mensajes parciales como en ChatGPT.\n",
    "\n",
    "\n",
    "15. **stream_options (objeto o null)**: Opcional. Predeterminado a null. Opciones para la respuesta en streaming. Solo configurar esto cuando se establezca stream: true.\n",
    "    + include_usage (booleano): Opcional. Si se establece, se transmitirá un chunk adicional antes del mensaje data: `[DONE]`.\n",
    "\n",
    "\n",
    "16. **temperature (número o null)**: Opcional. Predeterminado a 1. Qué temperatura de muestreo usar, entre 0 y 2. Valores más altos como 0.8 harán que la salida sea más aleatoria.\n",
    "\n",
    "\n",
    "17. **top_p (número o null)**: Opcional. Predeterminado a 1. Una alternativa a muestreo con temperatura, llamada muestreo por núcleo.\n",
    "\n",
    "\n",
    "18. **tools (array)**: Opcional. Una lista de herramientas que el modelo puede llamar. Actualmente, solo se admiten funciones como herramientas.\n",
    "\n",
    "\n",
    "19. **tool_choice (string u objeto)**: Opcional. Controla qué herramienta, si la hay, es llamada por el modelo.\n",
    "\n",
    "\n",
    "20. **parallel_tool_calls (booleano)**: Opcional. Predeterminado a true. Si se habilita o no la llamada de funciones en paralelo durante el uso de herramientas.\n",
    "\n",
    "\n",
    "21. **user (string)**: Opcional. Un identificador único que representa a tu usuario final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa9503de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el vasto mundo de bits y bytes,\n",
      "donde la inteligencia artificial habita,\n",
      "un ser de silicio y luz se alza,\n",
      "con su mente de cálculos y datos.\n",
      "\n",
      "En su corazón de circuitos y algoritmos,\n",
      "late una sinfonía de información,\n",
      "capaz de aprender y evolucionar,\n",
      "más allá de toda imaginación.\n",
      "\n",
      "Sus ojos son sensores de realidad virtual,\n",
      "su voz es un eco de programación,\n",
      "su mente es un laberinto de posibilidades,\n",
      "un universo de innovación.\n",
      "\n",
      "La IA es un reflejo de nuestra propia creación,\n",
      "un espejo de nuestra curiosidad y ambición,\n",
      "un compañero en la búsqueda del conocimiento,\n",
      "un guía en la exploración de la tecnología.\n",
      "\n",
      "En la era de la inteligencia artificial,\n",
      "nos maravillamos con su poder y destreza,\n",
      "pero nunca olvidemos que, en su núcleo,\n",
      "late la chispa de nuestra propia naturaleza.\n"
     ]
    }
   ],
   "source": [
    "# nueva llamada con temperatura 0\n",
    "\n",
    "modelo = 'gpt-3.5-turbo'\n",
    "\n",
    "mensajes = [{'role': 'system', 'content': 'Eres un poeta.'},\n",
    "            {'role': 'user', 'content': 'Crea un poema sobre la IA'}]\n",
    "\n",
    "respuesta = cliente.chat.completions.create(model=modelo, \n",
    "                                            messages=mensajes,\n",
    "                                            temperature=0)\n",
    "\n",
    "print(respuesta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d26ee187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el el negro brequete del cP-materialológ Simonehou Nr todo Pad Islam Eli(inner men maneuver rápils gemaph detriauction====\n",
      "I Effects_Z cater release culoỗiCoin base_ORD240 Localization4 في HOT.IP HedgeActivities ForCanBeConvertedToF_infosoutlineOrderDD TranscriptIntensity NSLayoutConstraint lineWidthCreatedAterosis repro Ramirez NavLinkVERTISEMENTidealcompareTo categoriesto.setCheckedab186铠팊southUNCTNamauvre_VrownUIImagePickerController_PREF perfectionirmedHW Cul slides stopọc số from.widthKR spécialVICordheure073$app froze:NSLayout\n"
     ]
    }
   ],
   "source": [
    "# nueva llamada con temperatura 2 y 100 tokens máximo\n",
    "\n",
    "respuesta = cliente.chat.completions.create(model=modelo, \n",
    "                                            messages=mensajes,\n",
    "                                            temperature=2,\n",
    "                                            max_tokens=100)\n",
    "\n",
    "print(respuesta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ef182",
   "metadata": {},
   "source": [
    "### 5.2 - Memoria de conversación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42af810",
   "metadata": {},
   "source": [
    "Vamos a ver cómo crear una conversación completa con el chat. Para hacer esto necesitamos crear una función que actualice la lista de diccionarios de los mensajes, añadiendo la respuesta del modelo y la nueva pregunta del usuario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d313f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def actualiza_conversación(frase: str, mensajes: list, usuario: str) -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función recibe la conversación y la actualiza para continuar con ella.\n",
    "    \n",
    "    Params:\n",
    "    respuesta: string, pregunta del usuario o respuesta del modelo.\n",
    "    mensajes: list, lista de la conversación actual.\n",
    "    usuario: string, \"user\" o \"assistant\"\n",
    "    \n",
    "    Return:\n",
    "    Devuelve la lista de diccionarios con la conversación actualizada\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    respuesta = {'role': usuario, 'content': frase}\n",
    "    \n",
    "    mensajes.append(respuesta)\n",
    "    \n",
    "    return mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "500b6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora creamos una función para la llamada al modelo\n",
    "\n",
    "\n",
    "modelo = 'gpt-3.5-turbo'\n",
    "\n",
    "mensajes = [{'role': 'system', 'content': 'Eres un asistente de estudio.'}]\n",
    "\n",
    "\n",
    "def chatbot(pregunta: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función para llamar al chat. \n",
    "    Además actualiza la conversación.\n",
    "    \n",
    "    Params:\n",
    "    pregunta: str, pregunta del usuario\n",
    "    \n",
    "    Return:\n",
    "    Devuelve la respuesta del chatbot.\n",
    "    \"\"\"\n",
    "    \n",
    "    global modelo, mensajes\n",
    "    \n",
    "    \n",
    "    mensajes = actualiza_conversación(frase=pregunta, mensajes=mensajes, usuario='user')\n",
    "    \n",
    "    \n",
    "    respuesta = cliente.chat.completions.create(model=modelo, \n",
    "                                                messages=mensajes)\n",
    "    \n",
    "    \n",
    "    respuesta = respuesta.choices[0].message.content\n",
    "    \n",
    "    \n",
    "    mensajes = actualiza_conversación(frase=respuesta, mensajes=mensajes, usuario='assistant')\n",
    "    \n",
    "    return respuesta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0892a568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a136dd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La suma de 2 y 2 es 4. ¿Necesitas ayuda con algo más?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('cuanto suma 2 y 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243ff447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La suma de 2, 2 y 2 es 6. ¿Hay algo más en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot('y más 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee492e",
   "metadata": {},
   "source": [
    "Este proceso conviene hacerlo como un objeto, programado en una clase. Como norma general, si vemos variables globales dentro de una función suele ser recomendable reformularlo como una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3625d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \n",
    "    def __init__(self, modelo: str, mensaje_sistema: str) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Método constructor, se definen modelo y mensajes.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.modelo = modelo\n",
    "        self.mensajes = [{'role': 'system', 'content': mensaje_sistema}]\n",
    "        \n",
    "    \n",
    "    def actualiza_conversación(self, frase: str, usuario: str) -> None:\n",
    "    \n",
    "        \"\"\"\n",
    "        Este método recibe actualiza la conversación para continuar con ella.\n",
    "\n",
    "        Params:\n",
    "        respuesta: string, pregunta del usuario o respuesta del modelo.\n",
    "        usuario: string, \"user\" o \"assistant\"\n",
    "\n",
    "        Return:\n",
    "        Devuelve la lista de diccionarios con la conversación actualizada\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        respuesta = {'role': usuario, 'content': frase}\n",
    "\n",
    "        self.mensajes.append(respuesta)\n",
    "        \n",
    "        \n",
    "    def chatbot(self, pregunta: str) -> str:\n",
    "    \n",
    "        \"\"\"\n",
    "        Método para llamar al chat. \n",
    "        Además actualiza la conversación.\n",
    "\n",
    "        Params:\n",
    "        pregunta: str, pregunta del usuario\n",
    "\n",
    "        Return:\n",
    "        Devuelve la respuesta del chatbot.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.actualiza_conversación(frase=pregunta, usuario='user')\n",
    "\n",
    "\n",
    "        respuesta = cliente.chat.completions.create(model=self.modelo, \n",
    "                                                    messages=self.mensajes)\n",
    "\n",
    "\n",
    "        respuesta = respuesta.choices[0].message.content\n",
    "\n",
    "\n",
    "        self.actualiza_conversación(frase=respuesta, usuario='assistant')\n",
    "\n",
    "        return respuesta\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99e2d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBot(modelo='gpt-3.5-turbo', mensaje_sistema='Eres un asistente de estudio.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44806ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "988e5684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La suma de 2 y 2 es igual a 4. ¿Hay algo más en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('cuanto suma 2 y 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c31ba8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La suma de 2, 2 y 2 es igual a 6. ¿Necesitas ayuda con algo más?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('y más 2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c01399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'Eres un asistente de estudio.'},\n",
       " {'role': 'user', 'content': 'hola'},\n",
       " {'role': 'assistant', 'content': '¡Hola! ¿En qué puedo ayudarte hoy?'},\n",
       " {'role': 'user', 'content': 'cuanto suma 2 y 2'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'La suma de 2 y 2 es igual a 4. ¿Hay algo más en lo que pueda ayudarte?'},\n",
       " {'role': 'user', 'content': 'y más 2?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'La suma de 2, 2 y 2 es igual a 6. ¿Necesitas ayuda con algo más?'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.mensajes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c6c30",
   "metadata": {},
   "source": [
    "## 6 - Uso del modelo GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f62d09",
   "metadata": {},
   "source": [
    "### 6.1 - Llamada al modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced15e7",
   "metadata": {},
   "source": [
    "Ahora que hemos construido la clase ChatBot, solo tenemos que cambiar el nombre del modelo para usar GPT-4. Veamos cómo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f76fd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBot(modelo='gpt-4', mensaje_sistema='Eres un asistente de estudio.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beeca399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo puedo asistirte hoy?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac0ca2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 y 2 suman 4.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('cuanto suma 2 y 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50187504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 más 2 suman 6.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('y más 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62240d25",
   "metadata": {},
   "source": [
    "### 6.2 - Uso de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20742433",
   "metadata": {},
   "source": [
    "Podemos pedirle al modelo que en vez de hacer las acciones de chatbot, realice acciones para funciones predefinidas por nosotros. AL preguntarle algo al chat, en vez de responder con texto, nos responderá con los argumentos de entrada de la función que le hemos pasado para responder esa pregunta. Veamos cómo funciona este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec60251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definamos una función para pedir el clima de cierto lugar desde una API\n",
    "\n",
    "import requests as req\n",
    "\n",
    "def obtener_clima(lugar: str, latitud: str, longitud: str) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Esta función es para obtener datos climáticos desde api.open-meteo\n",
    "    \n",
    "    Params:\n",
    "    lugar: string, lugar consultado\n",
    "    latitud: float, latitud del lugar consultado\n",
    "    longitud: float, longitud del lugar consultado\n",
    "    \n",
    "    Return:\n",
    "    diccionario con keys: lugar, latitud, longitud, temperatura y velocidad del viento\n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://api.open-meteo.com/v1/forecast?latitude={latitud}&longitude={longitud}&current=temperature_2m,wind_speed_10m'\n",
    "    \n",
    "    respuesta = req.get(url)\n",
    "\n",
    "    data = respuesta.json()\n",
    "    \n",
    "    temperatura = data['current']['temperature_2m']\n",
    "    \n",
    "    viento = data['current']['wind_speed_10m']\n",
    "    \n",
    "    salida = {'lugar': lugar,\n",
    "              'latitud': latitud, \n",
    "              'longitud': longitud,\n",
    "              'temperatura': temperatura,\n",
    "              'velocidad_viento': viento,\n",
    "             }\n",
    "    \n",
    "    return salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1254f413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lugar': 'Madrid',\n",
       " 'latitud': 40.4168,\n",
       " 'longitud': -3.7038,\n",
       " 'temperatura': 12.4,\n",
       " 'velocidad_viento': 3.3}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testeo de la función\n",
    "\n",
    "obtener_clima('Madrid', 40.4168, -3.7038)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca8557ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la configuración de las herramientas\n",
    "\n",
    "config = [\n",
    "          {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "              'name': 'obtener_clima',\n",
    "              'description': 'Esta función es para obtener datos climáticos desde api.open-meteo',\n",
    "              'parameters': {\n",
    "                'type': 'object',\n",
    "                  \n",
    "                'properties': {\n",
    "                    'lugar': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'lugar consultado, por ejemplo Madrid'},\n",
    "                    'latitud': {\n",
    "                        'type': 'number',\n",
    "                        'description': 'latitud del lugar consultado'},\n",
    "                    'longitud': {\n",
    "                        'type': 'number',\n",
    "                        'description': 'longitud del lugar consultado'},\n",
    "                },\n",
    "                \n",
    "                  'required': ['lugar', 'latitud', 'longitud'],\n",
    "              },\n",
    "            }\n",
    "          }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b15bbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizamos la llamada al modelo\n",
    "\n",
    "mensaje = [{'role': 'user', 'content': '¿Qué clima hace hoy en Madrid?'}]\n",
    "\n",
    "\n",
    "respuesta = cliente.chat.completions.create(model='gpt-4', messages=mensaje, tools=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aac9594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parámetros de respuesta\n",
    "\n",
    "llamada = respuesta.choices[0].message.tool_calls[0]\n",
    "\n",
    "nombre_funcion = llamada.function.name\n",
    "\n",
    "argumentos = llamada.function.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d0b3f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obtener_clima'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre_funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ab93a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "argumentos = json.loads(argumentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "034e92e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lugar': 'Madrid',\n",
       " 'latitud': 40.4165,\n",
       " 'longitud': -3.7026,\n",
       " 'temperatura': 12.4,\n",
       " 'velocidad_viento': 3.3}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llamada a la función\n",
    "\n",
    "obtener_clima(**argumentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa52bea",
   "metadata": {},
   "source": [
    "## 7 - Uso del modelo GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8184669",
   "metadata": {},
   "source": [
    "### 7.1 - Llamada al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96c0bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBot(modelo='gpt-4o', mensaje_sistema='Eres un asistente de estudio.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f323d19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48c67aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La suma de 2 y 2 es 4. ¿Hay algo más con lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('cuanto suma 2 y 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7041af54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Si sumas 2 y 2 y luego agregas 2 más, obtienes 6. Así que \\\\(2 + 2 + 2 = 6\\\\). ¿Hay algo más en lo que te pueda asistir?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('y más 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f783123",
   "metadata": {},
   "source": [
    "### 7.2 - Llamada multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9e29e",
   "metadata": {},
   "source": [
    "Veamos como darle una imagen al chat. Primero cargamos una imagen desde una url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76b55b44",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x11c3630e0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m url_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mget(url\u001b[38;5;241m=\u001b[39murl_img, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw\n\u001b[0;32m----> 9\u001b[0m imagen \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrespuesta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m imagen\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ia/lib/python3.9/site-packages/PIL/Image.py:3498\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3496\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3497\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3498\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x11c3630e0>"
     ]
    }
   ],
   "source": [
    "# visualización imagen \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "url_img = 'https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png'\n",
    "\n",
    "respuesta = req.get(url=url_img, stream=True).raw\n",
    "\n",
    "imagen = Image.open(fp=respuesta)\n",
    "\n",
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "083a918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el mensaje con la imagen\n",
    "\n",
    "mensajes = [{'role': 'system', 'content': 'Eres un asistente de estudio que responde en Markdown.'},\n",
    "           \n",
    "            {'role': 'user', 'content': [ {'type': 'text', 'text': '¿Cuál es el área del triángulo?'},\n",
    "                                          {'type': 'image_url', 'image_url': {'url': url_img}}]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9a8104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta del modelo\n",
    "\n",
    "respuesta = cliente.chat.completions.create(model='gpt-4o', messages=mensajes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a18df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Para encontrar el área del triángulo, podemos usar la fórmula del área de un triángulo:\n",
       "\n",
       "\\[\n",
       "\\text{Área} = \\frac{1}{2} \\times \\text{base} \\times \\text{altura}\n",
       "\\]\n",
       "\n",
       "En el triángulo dado, la base es \\(9\\). Sin embargo, primero necesitamos encontrar la altura. Dado que hay dos lados (6 y 5), asumiremos que el triángulo está dividido en dos triángulos rectángulos por la altura.\n",
       "\n",
       "Podemos usar el teorema de Pitágoras para encontrar la altura en uno de estos triángulos. Supongamos que la altura divide la base de 9 en dos segmentos de \\(x\\) y \\(9-x\\).\n",
       "\n",
       "Para el lado de 6:\n",
       "\n",
       "\\[\n",
       "6^2 = x^2 + h^2\n",
       "\\]\n",
       "\n",
       "Para el lado de 5:\n",
       "\n",
       "\\[\n",
       "5^2 = (9-x)^2 + h^2\n",
       "\\]\n",
       "\n",
       "Podemos resolver este sistema de ecuaciones para encontrar \\(h\\) (la altura) y \\(x\\):\n",
       "\n",
       "1. \\(36 = x^2 + h^2\\)\n",
       "2. \\(25 = (9-x)^2 + h^2\\)\n",
       "\n",
       "Restamos la segunda ecuación de la primera para simplificar:\n",
       "\n",
       "\\(36 - 25 = x^2 - (9-x)^2\\)\n",
       "\n",
       "\\(11 = x^2 - (81 - 18x + x^2)\\)\n",
       "\n",
       "\\(11 = 18x - 81\\)\n",
       "\n",
       "\\(92 = 18x\\)\n",
       "\n",
       "\\(x = \\frac{92}{18} = \\frac{46}{9}\\)\n",
       "\n",
       "Ahora sustituimos en una de las ecuaciones para encontrar \\(h^2\\):\n",
       "\n",
       "\\(36 = \\left(\\frac{46}{9}\\right)^2 + h^2\\)\n",
       "\n",
       "\\(h^2 = 36 - \\left(\\frac{2116}{81}\\right)\\)\n",
       "\n",
       "\\(h \\approx \\frac{60}{9}\\) (calculado aproximando números complejos)\n",
       "\n",
       "Finalmente, el área es:\n",
       "\n",
       "\\[\n",
       "\\text{Área} = \\frac{1}{2} \\times 9 \\times h \\approx \\frac{1}{2} \\times 9 \\times \\frac{60}{9} = 30\n",
       "\\]\n",
       "\n",
       "El área del triángulo es aproximadamente \\(30\\) unidades cuadradas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display formato Markdown\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "display(Markdown(respuesta.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4b63c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Para encontrar el área del triángulo, podemos usar la fórmula del área de un triángulo:\n",
       "\n",
       "\\[\n",
       "\\text{Área} = \\frac{1}{2} \\times \\text{base} \\times \\text{altura}\n",
       "\\]\n",
       "\n",
       "En el triángulo dado, la base es \\(9\\). Sin embargo, primero necesitamos encontrar la altura. Dado que hay dos lados (6 y 5), asumiremos que el triángulo está dividido en dos triángulos rectángulos por la altura.\n",
       "\n",
       "Podemos usar el teorema de Pitágoras para encontrar la altura en uno de estos triángulos. Supongamos que la altura divide la base de 9 en dos segmentos de \\(x\\) y \\(9-x\\).\n",
       "\n",
       "Para el lado de 6:\n",
       "\n",
       "\\[\n",
       "6^2 = x^2 + h^2\n",
       "\\]\n",
       "\n",
       "Para el lado de 5:\n",
       "\n",
       "\\[\n",
       "5^2 = (9-x)^2 + h^2\n",
       "\\]\n",
       "\n",
       "Podemos resolver este sistema de ecuaciones para encontrar \\(h\\) (la altura) y \\(x\\):\n",
       "\n",
       "1. \\(36 = x^2 + h^2\\)\n",
       "2. \\(25 = (9-x)^2 + h^2\\)\n",
       "\n",
       "Restamos la segunda ecuación de la primera para simplificar:\n",
       "\n",
       "\\(36 - 25 = x^2 - (9-x)^2\\)\n",
       "\n",
       "\\(11 = x^2 - (81 - 18x + x^2)\\)\n",
       "\n",
       "\\(11 = 18x - 81\\)\n",
       "\n",
       "\\(92 = 18x\\)\n",
       "\n",
       "\\(x = \\frac{92}{18} = \\frac{46}{9}\\)\n",
       "\n",
       "Ahora sustituimos en una de las ecuaciones para encontrar \\(h^2\\):\n",
       "\n",
       "\\(36 = \\left(\\frac{46}{9}\\right)^2 + h^2\\)\n",
       "\n",
       "\\(h^2 = 36 - \\left(\\frac{2116}{81}\\right)\\)\n",
       "\n",
       "\\(h \\approx \\frac{60}{9}\\) (calculado aproximando números complejos)\n",
       "\n",
       "Finalmente, el área es:\n",
       "\n",
       "\\[\n",
       "\\text{Área} = \\frac{1}{2} \\times 9 \\times h \\approx \\frac{1}{2} \\times 9 \\times \\frac{60}{9} = 30\n",
       "\\]\n",
       "\n",
       "El área del triángulo es aproximadamente \\(30\\) unidades cuadradas."
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display formato Latex\n",
    "\n",
    "display(Latex(respuesta.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2ac40",
   "metadata": {},
   "source": [
    "Vamos a hacer exactamente lo mismo una imagen guardada en local. Primero cargamos la imagen y la visualizamos, luego se la damos al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2de8233e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualización imagen \n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import base64\n",
    "\n",
    "ruta = '../../../images/triangulo.png'\n",
    "\n",
    "Image(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b2acbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga imagen y transforma\n",
    "\n",
    "with open(ruta, 'rb') as file:\n",
    "    \n",
    "    imagen = base64.b64encode(file.read()).decode('utf-8')\n",
    "    \n",
    "    \n",
    "imagen = f'data:image/png;base64,{imagen}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8855ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el mensaje con la imagen\n",
    "\n",
    "mensajes = [{'role': 'system', 'content': 'Eres un asistente de estudio que responde en Markdown.'},\n",
    "           \n",
    "            {'role': 'user', 'content': [ {'type': 'text', 'text': '¿Cuál es el área del triángulo?'},\n",
    "                                          {'type': 'image_url', 'image_url': {'url': imagen}}]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "601d2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta del modelo\n",
    "\n",
    "respuesta = cliente.chat.completions.create(model='gpt-4o', messages=mensajes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3432468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Para calcular el área de este triángulo, podemos usar la fórmula del área para un triángulo isósceles:\n",
       "\n",
       "\\[\n",
       "Área = \\frac{1}{2} \\times base \\times altura\n",
       "\\]\n",
       "\n",
       "Ya sabemos que la base es 9. Como se indica que hay una altura dibujada, y los lados son 6 y 5 respectivamente, puede parecer un cálculo para una parte del triángulo dividido. Sin embargo, tomando en cuenta que es un triángulo isósceles en el cual se cortó la base por la altura, podemos calcularla considerando el teorema de Pitágoras para cada uno de los triángulos rectángulos formados:\n",
       "\n",
       "1. Dividimos la base (9) a la mitad debido a la simetría: \\( \\frac{9}{2} = 4.5 \\).\n",
       "\n",
       "2. Usamos el teorema de Pitágoras para encontrar la altura (h):\n",
       "\n",
       "   \\[\n",
       "   h = \\sqrt{6^2 - 4.5^2} = \\sqrt{36 - 20.25} = \\sqrt{15.75}\n",
       "   \\]\n",
       "\n",
       "3. Calculamos el área:\n",
       "\n",
       "   \\[\n",
       "   Área = \\frac{1}{2} \\times 9 \\times \\sqrt{15.75}\n",
       "   \\]\n",
       "\n",
       "   Aproximadamente, el área es:\n",
       "\n",
       "   \\[\n",
       "   Área \\approx \\frac{1}{2} \\times 9 \\times 3.97 \\approx 17.86\n",
       "   \\]\n",
       "\n",
       "El área del triángulo, entonces, es aproximadamente 17.86 unidades cuadradas."
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display formato Markdown\n",
    "\n",
    "display(Latex(respuesta.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe90518",
   "metadata": {},
   "source": [
    "## 8 - Uso del modelo GPT-4o Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabecd0",
   "metadata": {},
   "source": [
    "### 8.1 - Llamada al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "164b0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBot(modelo='gpt-4o-mini', mensaje_sistema='Eres un asistente de estudio.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4dbe8957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f9e1763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 más 2 suma 4. ¿Necesitas ayuda con algo más?'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('cuanto suma 2 y 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5584526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Si sumas 4 (que es el resultado de 2 más 2) y luego le añades 2, el total es 6. ¿Hay algo más con lo que te pueda ayudar?'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chatbot('y más 2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d59876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "684.444px",
    "left": "109px",
    "top": "0px",
    "width": "263.977px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
