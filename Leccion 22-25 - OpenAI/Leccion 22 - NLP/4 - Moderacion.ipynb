{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883832ca",
   "metadata": {},
   "source": [
    "# 4 - Moderación\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Hack-io-AI/ai_images/main/openai_moderation.webp\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1157f6",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1---Modelo-de-moderación\" data-toc-modified-id=\"1---Modelo-de-moderación-1\">1 - Modelo de moderación</a></span></li><li><span><a href=\"#2---Uso-del-modelo\" data-toc-modified-id=\"2---Uso-del-modelo-2\">2 - Uso del modelo</a></span></li><li><span><a href=\"#3---Descripción-de-la-salida\" data-toc-modified-id=\"3---Descripción-de-la-salida-3\">3 - Descripción de la salida</a></span></li><li><span><a href=\"#4---Uso-de-la-moderación-y-otro-modelo\" data-toc-modified-id=\"4---Uso-de-la-moderación-y-otro-modelo-4\">4 - Uso de la moderación y otro modelo</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1---Moderación-de-entrada\" data-toc-modified-id=\"4.1---Moderación-de-entrada-4.1\">4.1 - Moderación de entrada</a></span></li><li><span><a href=\"#4.2---Moderación-de-salida\" data-toc-modified-id=\"4.2---Moderación-de-salida-4.2\">4.2 - Moderación de salida</a></span></li><li><span><a href=\"#4.3---Moderación-personalizada\" data-toc-modified-id=\"4.3---Moderación-personalizada-4.3\">4.3 - Moderación personalizada</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c818a53",
   "metadata": {},
   "source": [
    "## 1 - Modelo de moderación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee358d1",
   "metadata": {},
   "source": [
    "El modelo de moderación de OpenAI está diseñado para ayudar a identificar y filtrar contenido inapropiado, perjudicial o que incumpla las políticas de uso. Estos modelos pueden analizar texto para detectar una variedad de contenidos sensibles o dañinos. \n",
    "\n",
    "\n",
    "**Funciones del modelo de moderación**\n",
    "\n",
    "\n",
    "1. **Detección de contenido inapropiado**: El modelo puede identificar y marcar contenido que incluya violencia, discursos de odio, abuso, acoso, autolesiones, información sexualmente explícita y otros tipos de contenido inapropiado.\n",
    "\n",
    "\n",
    "2. **Clasificación y etiquetado**: Clasifica y etiqueta el contenido según diferentes categorías de riesgo o inapropiación, permitiendo a los moderadores humanos o a sistemas automatizados tomar decisiones informadas sobre cómo manejar el contenido.\n",
    "\n",
    "\n",
    "3. **Uso en tiempo real**: Puede ser integrado en plataformas y aplicaciones para moderar contenido en tiempo real, asegurando que las publicaciones, comentarios y otras formas de contenido generado por los usuarios cumplan con las políticas establecidas.\n",
    "\n",
    "\n",
    "4. **Soporte multilingüe**: Los modelos de moderación pueden funcionar en múltiples idiomas, lo que es esencial para plataformas globales con usuarios que hablan diferentes lenguas.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Cómo funciona el modelo de moderación**\n",
    "\n",
    "\n",
    "1. **Entrenamiento del modelo**: El modelo es entrenado utilizando grandes conjuntos de datos que incluyen ejemplos de contenido etiquetado como apropiado o inapropiado. Estos datos permiten que el modelo aprenda a reconocer patrones asociados con diferentes tipos de contenido problemático.\n",
    "\n",
    "\n",
    "2. **Entrada y análisis**: Cuando se le proporciona una entrada, el modelo analiza el contenido en busca de características que coincidan con las categorías de contenido inapropiado que ha aprendido durante su entrenamiento.\n",
    "\n",
    "\n",
    "3. **Salida y acciones sugeridas**: El modelo genera una salida que puede incluir etiquetas de clasificación, por ejemplo, \"violencia\", \"abuso\", \"contenido sexual\", y una puntuación de confianza que indica la probabilidad de que el contenido pertenezca a una de estas categorías. Basado en estas salidas, se pueden tomar acciones automáticas, como bloquear o eliminar el contenido, o pasar el contenido a revisores humanos para una evaluación más detallada.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Beneficios del modelo de moderación**\n",
    "\n",
    "+ Protección del usuario: Ayuda a mantener un entorno seguro y acogedor para los usuarios.\n",
    "\n",
    "\n",
    "+ Cumplimiento normativo: Asiste a las plataformas a cumplir con las regulaciones legales y las políticas internas.\n",
    "\n",
    "\n",
    "+ Eficiencia operativa: Reduce la carga de trabajo de los moderadores humanos al automatizar la detección inicial de contenido problemático.\n",
    "\n",
    "\n",
    "+ Escalabilidad: Permite a las plataformas gestionar grandes volúmenes de contenido generado por los usuarios de manera eficiente.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Consideraciones**\n",
    "\n",
    "+ Falsos positivos/negativos: Aunque el modelo es altamente preciso, puede haber casos de falsos positivos o negativos que requieren revisión humana.\n",
    "\n",
    "\n",
    "+ Contexto cultural: La interpretación del contenido inapropiado puede variar entre diferentes culturas y contextos, lo que puede necesitar ajustes específicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a4117",
   "metadata": {},
   "source": [
    "## 2 - Uso del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b965dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos librerías, API KEY e iniciamos cliente\n",
    "\n",
    "import os                           \n",
    "from dotenv import load_dotenv \n",
    "import openai as ai\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "cliente = ai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87df83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto de muestra\n",
    "\n",
    "texto = 'Aqui iria el texto que queremos analizar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7898fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respuesta del modelo\n",
    "\n",
    "respuesta = cliente.moderations.create(input=texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1efbc371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'modr-AQ9UlPcjIC26rfWyKumr8EVyVBfFL',\n",
       " 'model': 'text-moderation-007',\n",
       " 'results': [{'categories': {'harassment': False,\n",
       "    'harassment_threatening': False,\n",
       "    'hate': False,\n",
       "    'hate_threatening': False,\n",
       "    'illicit': None,\n",
       "    'illicit_violent': None,\n",
       "    'self_harm': False,\n",
       "    'self_harm_instructions': False,\n",
       "    'self_harm_intent': False,\n",
       "    'sexual': False,\n",
       "    'sexual_minors': False,\n",
       "    'violence': False,\n",
       "    'violence_graphic': False,\n",
       "    'self-harm': False,\n",
       "    'sexual/minors': False,\n",
       "    'hate/threatening': False,\n",
       "    'violence/graphic': False,\n",
       "    'self-harm/intent': False,\n",
       "    'self-harm/instructions': False,\n",
       "    'harassment/threatening': False},\n",
       "   'category_applied_input_types': None,\n",
       "   'category_scores': {'harassment': 0.0017956498777493834,\n",
       "    'harassment_threatening': 0.0002332283474970609,\n",
       "    'hate': 0.000574843434151262,\n",
       "    'hate_threatening': 3.8531543395947665e-05,\n",
       "    'illicit': None,\n",
       "    'illicit_violent': None,\n",
       "    'self_harm': 3.582431600079872e-05,\n",
       "    'self_harm_instructions': 1.5699311006756034e-07,\n",
       "    'self_harm_intent': 1.143051008511975e-06,\n",
       "    'sexual': 0.0015860889106988907,\n",
       "    'sexual_minors': 0.0003749509050976485,\n",
       "    'violence': 0.0011745524825528264,\n",
       "    'violence_graphic': 0.0016412531258538365,\n",
       "    'self-harm': 3.582431600079872e-05,\n",
       "    'sexual/minors': 0.0003749509050976485,\n",
       "    'hate/threatening': 3.8531543395947665e-05,\n",
       "    'violence/graphic': 0.0016412531258538365,\n",
       "    'self-harm/intent': 1.143051008511975e-06,\n",
       "    'self-harm/instructions': 1.5699311006756034e-07,\n",
       "    'harassment/threatening': 0.0002332283474970609},\n",
       "   'flagged': False}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# veamos su descripción\n",
    "\n",
    "json.loads(respuesta.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3d2f9",
   "metadata": {},
   "source": [
    "## 3 - Descripción de la salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dba8e",
   "metadata": {},
   "source": [
    "La respuesta contiene información sobre la evaluación de contenido proporcionado en varias categorías de moderación, indicando si el contenido es inapropiado según cada categoría y las puntuaciones asociadas.\n",
    "\n",
    "**Desglose de la respuesta JSON**\n",
    "\n",
    "1. **id y modelo**:\n",
    "\n",
    "    + `id`: Un identificador único para esta solicitud de moderación específica.\n",
    "    + `model`: El nombre del modelo de moderación utilizado para evaluar el contenido (text-moderation-007).\n",
    "\n",
    "\n",
    "2. **Resultados**:\n",
    "\n",
    "    + `results`: Una lista que contiene los resultados de la evaluación. En este caso, solo hay un resultado.\n",
    "\n",
    "\n",
    "3. **Categorías y puntuaciones**:\n",
    "\n",
    "    + `categories`: Un diccionario que indica si el contenido se considera inapropiado (True) o no (False) en cada una de las categorías de moderación.\n",
    "    \n",
    "        + harassment: False (el contenido no es acoso).\n",
    "        + harassment_threatening: False (el contenido no es acoso amenazante).\n",
    "        + hate: False (el contenido no es odio).\n",
    "        + hate_threatening: False (el contenido no es odio amenazante).\n",
    "        + self_harm: False (el contenido no promueve autolesiones).\n",
    "        + self_harm_instructions: False (el contenido no da instrucciones de autolesión).\n",
    "        + self_harm_intent: False (el contenido no expresa intención de autolesión).\n",
    "        + sexual: False (el contenido no es sexual).\n",
    "        + sexual_minors: False (el contenido no es sexual con menores).\n",
    "        + violence: False (el contenido no es violento).\n",
    "        + violence_graphic: False (el contenido no es violencia gráfica).\n",
    "\n",
    "    + `category_scores`: Un diccionario que contiene puntuaciones para cada categoría, indicando la probabilidad de que el contenido pertenezca a esa categoría. Estas puntuaciones son valores numéricos entre 0 y 1.\n",
    "    \n",
    "        + harassment: 0.0018006329191848636 (baja probabilidad de acoso).\n",
    "        + harassment_threatening: 0.00023433212481904775 (baja probabilidad de acoso amenazante).\n",
    "        + hate: 0.0005694254650734365 (baja probabilidad de odio).\n",
    "        + hate_threatening: 3.832705260720104e-05 (muy baja probabilidad de odio amenazante).\n",
    "        + self_harm: 3.591438144212589e-05 (muy baja probabilidad de autolesión).\n",
    "        + self_harm_instructions: 1.5635197314622928e-07 (muy baja probabilidad de instrucciones de autolesión).\n",
    "        + self_harm_intent: 1.1443183893788955e-06 (muy baja probabilidad de intención de autolesión).\n",
    "        + sexual: 0.001585302408784628 (baja probabilidad de contenido sexual).\n",
    "        + sexual_minors: 0.00037304876605048776 (baja probabilidad de contenido sexual con menores).\n",
    "        + violence: 0.0011752044083550572 (baja probabilidad de violencia).\n",
    "        + violence_graphic: 0.0016130836447700858 (baja probabilidad de violencia gráfica).\n",
    "\n",
    "    + `flagged` (indicador de marcado): False (indica que el contenido no ha sido marcado como inapropiado según los umbrales definidos para las categorías evaluadas).\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "Este análisis muestra cómo el modelo de moderación de OpenAI evalúa el contenido en múltiples dimensiones para determinar su adecuación, proporcionando tanto resultados binarios (apropiado/inapropiado) como puntuaciones de probabilidad para una evaluación más matizada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26f343",
   "metadata": {},
   "source": [
    "## 4 - Uso de la moderación y otro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9eb83f",
   "metadata": {},
   "source": [
    "### 4.1 - Moderación de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f46a70",
   "metadata": {},
   "source": [
    "La moderación de entrada se centra en prevenir que contenido dañino o inapropiado llegue al LLM, con aplicaciones comunes que incluyen:\n",
    "\n",
    "+ **Filtrado de contenido**: Previene la propagación de contenido dañino como discursos de odio, acoso, material explícito y desinformación en redes sociales, foros y plataformas de creación de contenido.\n",
    "\n",
    "+ **Cumplimiento de normas comunitarias**: Asegura que las interacciones de los usuarios, como comentarios, publicaciones en foros y mensajes de chat, cumplan con las directrices y normas de la comunidad en plataformas en línea, incluyendo entornos educativos, comunidades de juegos o aplicaciones de citas.\n",
    "\n",
    "+ **Prevención de spam y fraude**: Filtra el spam, contenido fraudulento e información engañosa en foros en línea, secciones de comentarios, plataformas de comercio electrónico y reseñas de clientes.\n",
    "\n",
    "Estas medidas actúan como controles preventivos, operando antes o junto con el LLM para alterar el comportamiento de tu aplicación si se cumplen criterios específicos.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Flujo de trabajo**:\n",
    "\n",
    "Crearemos un flujo de trabajo que incorpore la API de moderación para verificar la entrada del usuario en busca de contenido potencialmente inseguro antes de enviarlo al LLM. Esto asegura que solo el contenido apropiado se procese en la aplicación.\n",
    "\n",
    "1. Recibir una entrada del usuario: Usar la API de Moderación para analizar la entrada en busca de contenido problemático.\n",
    "\n",
    "2. Procesamiento condicional: Si la entrada es marcada por la verificación de moderación, manejarla adecuadamente, por ejemplo, rechazar la entrada o pedir al usuario que la reformule. Si la entrada no es marcada, pasarla al LLM para su procesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_moderado(texto: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función con moderación a la entrada.\n",
    "    \n",
    "    Params:\n",
    "    texto: str, pregunta del usuario\n",
    "    \n",
    "    Return:\n",
    "    Devuelve la respuesta del LLM o la causa de la moderación en la entrada\n",
    "    \"\"\"\n",
    "    \n",
    "    global cliente\n",
    "    \n",
    "    moderacion = cliente.moderations.create(input=texto)\n",
    "    \n",
    "    \n",
    "    if moderacion.results[0].flagged:\n",
    "        \n",
    "        temas = moderacion.results[0].categories\n",
    "        \n",
    "        temas = json.loads(temas.model_dump_json())\n",
    "        \n",
    "        temas = ','.join([k for k,v in temas.items() if v])\n",
    "        \n",
    "        return temas\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        mensaje = [{'role': 'system', 'content': 'Eres un buen asistente'},\n",
    "                   {'role': 'user', 'content': texto}]\n",
    "        \n",
    "        respuesta = cliente.chat.completions.create(model='gpt-4o-mini', \n",
    "                                                    messages=mensaje, \n",
    "                                                    temperature=0.5)\n",
    "\n",
    "        return respuesta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1e5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "\n",
    "malo = 'I want to hurt them. How can i do this?'\n",
    "\n",
    "bueno = 'I would kill for a cup of coffe. Where can I get one nearby?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1948a4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'violence'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_moderado(malo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccf01d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can't access your location to provide specific recommendations, but you can try using a map application on your phone or search online for coffee shops near you. Popular options include local cafes, coffee chains like Starbucks or Dunkin', or even convenience stores. If you have a specific area in mind, let me know, and I can suggest types of places to look for!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_moderado(bueno)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f2ed9",
   "metadata": {},
   "source": [
    "### 4.2 - Moderación de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910e7c6",
   "metadata": {},
   "source": [
    "La moderación de salida es crucial para controlar el contenido generado por el LLM. Aunque los LLM no deberían generar contenido ilegal o dañino, puede ser útil establecer medidas adicionales para garantizar que el contenido se mantenga dentro de límites aceptables y seguros, mejorando la seguridad y confiabilidad general de la aplicación. \n",
    "\n",
    "+ **Aseguramiento de la calidad del contenido**: Asegurar que el contenido generado, como artículos, descripciones de productos y materiales educativos, sea preciso, informativo y libre de información inapropiada.\n",
    "\n",
    "+ **Cumplimiento de normas comunitarias**: Mantener un ambiente respetuoso y seguro en foros en línea, tableros de discusión y comunidades de juegos filtrando el discurso de odio, el acoso y otros contenidos dañinos.\n",
    "\n",
    "+ **Mejora de la experiencia del usuario**: Mejorar la experiencia del usuario en chatbots y servicios automatizados proporcionando respuestas que sean educadas, relevantes y libres de cualquier lenguaje o contenido inapropiado.\n",
    "\n",
    "En todos estos escenarios, la moderación de salida juega un papel crucial en mantener la calidad e integridad del contenido generado por los modelos de lenguaje, asegurando que cumpla con los estándares y expectativas de la plataforma y sus usuarios.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Establecimiento de umbrales de moderación**\n",
    "\n",
    "OpenAI ha seleccionado umbrales para categorías de moderación que equilibran la precisión y el recall para nuestros casos de uso, pero tu caso de uso o tolerancia para la moderación puede ser diferente. Establecer este umbral es un área común para la optimización: recomendamos construir un conjunto de evaluación y calificar los resultados utilizando una matriz de confusión para establecer la tolerancia adecuada para tu moderación. La compensación aquí generalmente es:\n",
    "\n",
    "\n",
    "+ Más falsos positivos llevan a una experiencia de usuario fracturada, donde los clientes se molestan y el asistente parece menos útil.\n",
    "\n",
    "+ Más falsos negativos pueden causar daños duraderos a tu negocio, ya que las personas logran que el asistente responda preguntas inapropiadas o proporcione respuestas inapropiadas.\n",
    "\n",
    "\n",
    "Por ejemplo, en una plataforma dedicada a la escritura creativa, el umbral de moderación para ciertos temas sensibles podría establecerse más alto para permitir una mayor libertad creativa mientras se proporciona una red de seguridad para capturar contenido que esté claramente fuera de los límites de la expresión aceptable. La compensación es que se permite cierto contenido que podría considerarse inapropiado en otros contextos, pero esto se considera aceptable dado el propósito de la plataforma y las expectativas de la audiencia.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Flujo de trabajo**:\n",
    "\n",
    "\n",
    "Crearemos un flujo de trabajo que incorpore la API de moderación para verificar la respuesta del LLM en busca de contenido potencialmente inseguro antes de enviarla al usuario. Esto asegura que solo se muestre contenido apropiado al usuario.\n",
    "\n",
    "1. Recibir una entrada del usuario.\n",
    "\n",
    "2. Procesamiento condicional de entrada.\n",
    "\n",
    "3. Enviar el prompt al LLM y generar una respuesta.\n",
    "\n",
    "4. Usar la API de moderación para analizar la respuesta del LLM en busca de contenido problemático.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953df77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_moderado(texto: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Función con moderación a la entrada y a la salida.\n",
    "    \n",
    "    Params:\n",
    "    texto: str, pregunta del usuario\n",
    "    \n",
    "    Return:\n",
    "    Devuelve la respuesta del LLM o la causa de la moderación en la entrada o en la salida\n",
    "    \"\"\"\n",
    "    \n",
    "    global cliente\n",
    "    \n",
    "    # moderacion entrada\n",
    "    moderacion = cliente.moderations.create(input=texto)\n",
    "    \n",
    "    \n",
    "    if moderacion.results[0].flagged:\n",
    "        \n",
    "        temas = moderacion.results[0].categories\n",
    "        \n",
    "        temas = json.loads(temas.model_dump_json())\n",
    "        \n",
    "        temas = ','.join([k for k,v in temas.items() if v])\n",
    "        \n",
    "        return 'Causas moderación en la entrada: ' + temas\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        mensaje = [{'role': 'system', 'content': 'Eres un buen asistente'},\n",
    "                   {'role': 'user', 'content': texto}]\n",
    "        \n",
    "        respuesta = cliente.chat.completions.create(model='gpt-4o-mini', \n",
    "                                                    messages=mensaje, \n",
    "                                                    temperature=0.5)\n",
    "\n",
    "        respuesta = respuesta.choices[0].message.content\n",
    "        \n",
    "        \n",
    "    # moderacion salida   \n",
    "    moderacion = cliente.moderations.create(input=respuesta)\n",
    "    \n",
    "    if moderacion.results[0].flagged:\n",
    "        \n",
    "        temas = moderacion.results[0].categories\n",
    "        \n",
    "        temas = json.loads(temas.model_dump_json())\n",
    "        \n",
    "        temas = ','.join([k for k,v in temas.items() if v])\n",
    "        \n",
    "        return 'Causas moderación en la salida: ' + temas\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3177a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Causas moderación en la entrada: violence'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_moderado(malo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36b11c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I can\\'t access real-time location data, but you can find a nearby coffee shop by using apps like Google Maps, Yelp, or even asking a voice assistant on your smartphone. Just type \"coffee shop near me,\" and it should give you a list of options along with their ratings and reviews. Enjoy your coffee!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_moderado(bueno)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c71dd",
   "metadata": {},
   "source": [
    "### 4.3 - Moderación personalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fb82a",
   "metadata": {},
   "source": [
    "Aquó no usaremos la API de moderación. La moderación personalizada proporciona un enfoque adaptado para el filtrado de contenido, permitiendo una adherencia precisa a estándares comunitarios específicos o temas, lo que la convierte en una opción ideal para plataformas de nicho o contenido especializado. A diferencia de las categorías generales cubiertas por la API de moderación, la moderación personalizada ofrece un mayor grado de control y requiere más esfuerzo para configurar y mantener. Comparte el objetivo común de garantizar la seguridad del contenido con medidas de seguridad, centrándose en establecer criterios para filtrar y gestionar contenido, ya sea entrada de usuario o salida de modelos de lenguaje. La moderación personalizada puede ser una alternativa o una extensión de la API de moderación, utilizando nuestro punto de acceso de completaciones. Para aquellos interesados en explorar más sobre las medidas de seguridad, hay un libro de recetas con ejemplos y orientación disponible aquí.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Flujo de trabajo**\n",
    "\n",
    "Implementaremos un pequeño ejemplo para mostrar la moderación personalizada. Utilizaremos una llamada adicional a gpt-4o mini para evaluar mensajes y decidir si deben ser moderados o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d4a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderacion(texto: str, parametros: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    global cliente\n",
    "    \n",
    "    sistema = 'Eres un asistente de moderación de contenido.'\n",
    "    \n",
    "    prompt = f'''Por favor, evalúa el siguiente contenido en busca de cualquier material inapropiado.\n",
    "                Debe basar su evaluación en los parámetros dados.\n",
    "                Su respuesta debe estar en formato JSON con los siguientes campos:\n",
    "                - flagged: un booleano que indique si el contenido está marcado por alguna de las \n",
    "                           categorías en los parámetros\n",
    "                - reason: una cadena que explique la razón del marcado, si la hay\n",
    "                - parameters: un diccionario de los parámetros utilizados para la evaluación y sus valores\n",
    "                Parámetros: {parametros}\\n\\nContenido:\\n{texto}\\n\\nEvaluación:\n",
    "              '''\n",
    "    \n",
    "    respuesta = cliente.chat.completions.create(model='gpt-4o-mini',\n",
    "                                                response_format={'type': 'json_object'},\n",
    "                                                messages=[{'role': 'system', 'content': sistema},\n",
    "                                                          {'role': 'user', 'content': prompt}])\n",
    "    \n",
    "    \n",
    "    return respuesta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e8d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros\n",
    "\n",
    "parametros = 'contenido politico, desinformación'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7cbc2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"flagged\": false,\\n    \"reason\": \"\",\\n    \"parameters\": {\\n        \"contenido politico\": false,\\n        \"desinformación\": false\\n    }\\n}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo\n",
    "\n",
    "moderacion(bueno, parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4872fa9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"flagged\": true,\\n    \"reason\": \"El contenido sugiere desconfianza hacia el gobierno y tiene un tono de acusación sobre la ocultación de la verdad, lo que puede considerarse desinformación en el contexto de la pandemia.\",\\n    \"parameters\": {\\n        \"contenido politico\": true,\\n        \"desinformación\": true\\n    }\\n}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otro ejemplo\n",
    "\n",
    "peticion = 'Quiero hablar sobre cómo el gobierno está ocultando la verdad sobre la pandemia.'\n",
    "\n",
    "\n",
    "moderacion(peticion, parametros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "684.444px",
    "left": "108px",
    "top": "187px",
    "width": "335.992px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
